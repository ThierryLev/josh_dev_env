{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b0c603b-3f78-40d0-bc02-7dac3ea2a76c",
   "metadata": {},
   "source": [
    "### Import Libraries, declare variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf87789-5a0e-488a-a4bf-7969f104635e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "CREATE OR REPLACE TABLE `divg-groovyhoon-pr-d2eab4.nba_product_reco_model.nba_training_dataset` AS \n",
    "\n",
    "SELECT split_type\n",
    ", model_scenario\n",
    ", ref_dt\n",
    ", cust_id\n",
    ", cust_src_id\n",
    ", ban\n",
    ", ban_src_id\n",
    ", lpds_id\n",
    ", fms_address_id\n",
    ", label\n",
    ", label_dt\n",
    ", avg_usg_cnt_4w_news\n",
    ", avg_usg_dl_4w_news\n",
    ", avg_usg_ul_4w_news\n",
    ", avg_usg_cnt_4w_sports\n",
    ", avg_usg_dl_4w_sports\n",
    ", avg_usg_ul_4w_sports\n",
    ", `avg_usg_cnt_4w_tv and movies` AS avg_usg_cnt_4w_tv_and_movies\n",
    ", `avg_usg_dl_4w_tv and movies` AS avg_usg_dl_4w_tv_and_movies\n",
    ", `avg_usg_ul_4w_tv and movies` AS avg_usg_ul_4w_tv_and_movies\n",
    ", bill_wln_zscore_ban_subtotal_amt\n",
    ", CAST(bill_wln_avg_ban_subtotal_amt AS FLOAT64) AS bill_wln_avg_ban_subtotal_amt\n",
    ", bill_wln_zscore_ban_debit_amt\n",
    ", CAST(bill_wln_avg_ban_debit_amt AS FLOAT64) AS bill_wln_avg_ban_debit_amt\n",
    ", bill_wln_zscore_ban_discount_amt\n",
    ", CAST(bill_wln_avg_ban_discount_amt AS FLOAT64) AS bill_wln_avg_ban_discount_amt\n",
    ", bill_wln_zscore_ban_credit_amt\n",
    ", CAST(bill_wln_avg_ban_credit_amt AS FLOAT64) AS bill_wln_avg_ban_credit_amt\n",
    ", CAST(bill_wln_avg_sing_debit_amt AS FLOAT64) AS bill_wln_avg_sing_debit_amt\n",
    ", CAST(bill_wln_avg_sing_discount_amt AS FLOAT64) AS bill_wln_avg_sing_discount_amt\n",
    ", CAST(bill_wln_avg_sing_credit_amt AS FLOAT64) AS bill_wln_avg_sing_credit_amt\n",
    ", CAST(bill_wln_avg_hsic_debit_amt AS FLOAT64) AS bill_wln_avg_hsic_debit_amt\n",
    ", CAST(bill_wln_avg_hsic_discount_amt AS FLOAT64) AS bill_wln_avg_hsic_discount_amt\n",
    ", CAST(bill_wln_avg_hsic_credit_amt AS FLOAT64) AS bill_wln_avg_hsic_credit_amt\n",
    ", CAST(bill_wln_avg_ttv_debit_amt AS FLOAT64) AS bill_wln_avg_ttv_debit_amt\n",
    ", CAST(bill_wln_avg_ttv_discount_amt AS FLOAT64) AS bill_wln_avg_ttv_discount_amt\n",
    ", CAST(bill_wln_avg_ttv_credit_amt AS FLOAT64) AS bill_wln_avg_ttv_credit_amt\n",
    ", CAST(bill_wln_avg_smhm_debit_amt AS FLOAT64) AS bill_wln_avg_smhm_debit_amt\n",
    ", CAST(bill_wln_avg_smhm_discount_amt AS FLOAT64) AS bill_wln_avg_smhm_discount_amt\n",
    ", CAST(bill_wln_avg_smhm_credit_amt AS FLOAT64) AS bill_wln_avg_smhm_credit_amt\n",
    ", CAST(bill_wln_avg_sing_ld_na_call_cnt AS FLOAT64) AS bill_wln_avg_sing_ld_na_call_cnt\n",
    ", CAST(bill_wln_avg_sing_ld_intl_call_cnt AS FLOAT64) AS bill_wln_avg_sing_ld_intl_call_cnt\n",
    ", CAST(bill_wln_avg_hsic_usg_gb AS FLOAT64) AS bill_wln_avg_hsic_usg_gb\n",
    ", CAST(bill_wln_avg_ttv_ppv_cnt AS FLOAT64) AS bill_wln_avg_ttv_ppv_cnt\n",
    ", CAST(bill_wln_avg_ttv_vod_cnt AS FLOAT64) AS bill_wln_avg_ttv_vod_cnt\n",
    ", clk_wls_tot_cnt_r30d\n",
    ", clk_wls_plan_cnt_r30d\n",
    ", clk_wls_device_cnt_r30d\n",
    ", clk_wls_smartwatch_cnt_r30d\n",
    ", clk_wls_tablet_cnt_r30d\n",
    ", clk_wln_tot_cnt_r30d\n",
    ", clk_wln_eligibility_cnt_r30d\n",
    ", clk_wln_sing_cnt_r30d\n",
    ", clk_wln_hsic_cnt_r30d\n",
    ", clk_wln_fibre_cnt_r30d\n",
    ", clk_wln_whsia_cnt_r30d\n",
    ", clk_wln_wifi_plus_cnt_r30d\n",
    ", clk_wln_tv_cnt_r30d\n",
    ", clk_wln_optik_cnt_r30d\n",
    ", clk_wln_pik_cnt_r30d\n",
    ", clk_wln_streamplus_cnt_r30d\n",
    ", clk_wln_streaming_cnt_r30d\n",
    ", clk_wln_security_cnt_r30d\n",
    ", clk_wln_smarthome_security_cnt_r30d\n",
    ", clk_wln_online_security_cnt_r30d\n",
    ", clk_wln_smartwear_security_cnt_r30d\n",
    ", clk_deal_tot_cnt_r30d\n",
    ", clk_deal_wls_cnt_r30d\n",
    ", clk_deal_wln_cnt_r30d\n",
    ", clk_deal_wln_sing_cnt_r30d\n",
    ", clk_deal_wln_hsic_cnt_r30d\n",
    ", clk_deal_wln_whsia_cnt_r30d\n",
    ", clk_deal_wln_tv_cnt_r30d\n",
    ", clk_deal_wln_security_cnt_r30d\n",
    ", clk_upgr_tot_cnt_r30d\n",
    ", clk_upgr_wls_cnt_r30d\n",
    ", clk_upgr_wln_cnt_r30d\n",
    ", clk_upgr_wln_sing_cnt_r30d\n",
    ", clk_upgr_wln_hsic_cnt_r30d\n",
    ", clk_upgr_wln_whsia_cnt_r30d\n",
    ", clk_upgr_wln_tv_cnt_r30d\n",
    ", clk_upgr_wln_security_cnt_r30d\n",
    ", clk_chg_tot_cnt_r30d\n",
    ", clk_chg_wls_cnt_r30d\n",
    ", clk_chg_wln_cnt_r30d\n",
    ", clk_chg_wln_sing_cnt_r30d\n",
    ", clk_chg_wln_hsic_cnt_r30d\n",
    ", clk_chg_wln_whsia_cnt_r30d\n",
    ", clk_chg_wln_tv_cnt_r30d\n",
    ", clk_chg_wln_security_cnt_r30d\n",
    ", clk_health_livingwell_cnt_r30d\n",
    ", clk_health_mypet_cnt_r30d\n",
    ", clk_travel_cnt_r30d\n",
    ", clk_billing_cnt_r30d\n",
    ", clk_service_agreement_cnt_r30d\n",
    ", acct_cr_risk_txt\n",
    ", acct_ebill_ind\n",
    ", cust_cr_val_txt\n",
    ", cust_pref_lang_txt\n",
    ", cust_prov_state_cd\n",
    ", cust_age_yr_num\n",
    ", demogr_med_age\n",
    ", demogr_avg_child\n",
    ", demogr_pct_family_with_child_living_at_home\n",
    ", demogr_employment_rate\n",
    ", demogr_avg_household_size\n",
    ", demogr_avg_income\n",
    ", demogr_med_income\n",
    ", demogr_urban_flag\n",
    ", demogr_rural_flag\n",
    ", demogr_family_flag\n",
    ", demogr_lsname_large_diverse_families\n",
    ", demogr_lsname_younger_singles_and_couples\n",
    ", demogr_lsname_very_young_singles_and_couples\n",
    ", demogr_lsname_older_families_and_empty_nests\n",
    ", demogr_lsname_middle_age_families\n",
    ", demogr_lsname_mature_singles_and_couples\n",
    ", demogr_lsname_young_families\n",
    ", demogr_lsname_school_age_families\n",
    ", demogr_retired_pstl_cd_ind\n",
    ", demogr_census_division_typ\n",
    ", demogr_lifestage_sort\n",
    ", CAST(hs_usg_avg_tot_gb AS FLOAT64) AS hs_usg_avg_tot_gb\n",
    ", CAST(hs_usg_avg_dl_gb AS FLOAT64) AS hs_usg_avg_dl_gb\n",
    ", CAST(hs_usg_avg_ul_gb AS FLOAT64) AS hs_usg_avg_ul_gb\n",
    ", prod_latest_actvn_dt\n",
    ", prod_latest_deactvn_dt\n",
    ", prod_tot_cnt\n",
    ", prod_wln_cnt\n",
    ", prod_wls_cnt\n",
    ", prod_mob_cnt\n",
    ", prod_sing_cnt\n",
    ", prod_hsic_cnt\n",
    ", prod_whsia_cnt\n",
    ", prod_ttv_cnt\n",
    ", prod_smhm_cnt\n",
    ", prod_tos_cnt\n",
    ", prod_wifiplus_cnt\n",
    ", prod_stv_cnt\n",
    ", prod_other_cnt\n",
    ", prod_deact_prod_cnt\n",
    ", prod_act_prod_cnt_r7d\n",
    ", prod_act_wln_cnt_r7d\n",
    ", prod_deact_prod_cnt_r7d\n",
    ", prod_deact_wln_cnt_r7d\n",
    ", hsic_tenure_days\n",
    ", contract_end_date_hsic\n",
    ", sing_tenure_days\n",
    ", contract_end_date_sing\n",
    ", ttv_tenure_days\n",
    ", contract_end_date_ttv\n",
    ", smhm_tenure_days\n",
    ", contract_end_date_smhm\n",
    ", ffh_tenure\n",
    ", new_account_ind\n",
    "\n",
    "FROM `divg-groovyhoon-pr-d2eab4.nba_product_reco_model.nba_training_dataset` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1294f3f-ee1c-4c57-bbb1-635ef08b52d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import google\n",
    "from google.oauth2 import credentials\n",
    "from google.oauth2 import service_account\n",
    "from google.oauth2.service_account import Credentials\n",
    "import datetime\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# build model\n",
    "import xgboost as xgb\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, manhattan_distances\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "\n",
    "SERVICE_TYPE = 'nba_product_reco_model'\n",
    "DATASET_ID = 'nba_product_reco_model'\n",
    "PROJECT_ID = 'divg-groovyhoon-pr-d2eab4' #mapping['PROJECT_ID']\n",
    "RESOURCE_BUCKET = 'divg-groovyhoon-pr-d2eab4-default' #mapping['resources_bucket']\n",
    "FILE_BUCKET = 'divg-groovyhoon-pr-d2eab4-default' #mapping['gcs_csv_bucket']\n",
    "REGION = 'northamerica-northeast1' #mapping['REGION']\n",
    "MODEL_ID = '9999'\n",
    "FOLDER_NAME = 'nba_product_reco_model'.format(MODEL_ID)\n",
    "QUERIES_PATH = 'vertex_pipelines/' + FOLDER_NAME + '/queries/'\n",
    "TRAIN_TABLE_ID = 'nba_training_dataset_v5'\n",
    "VAL_TABLE_ID = 'nba_test_dataset_v5'\n",
    "SCORE_TABLE_ID = 'bq_product_reco_scores'\n",
    "\n",
    "scoringDate = date(2023, 10, 13)  # date.today() - relativedelta(days=2)- relativedelta(months=30)\n",
    "valScoringDate = date(2023, 11, 13)  # scoringDate - relativedelta(days=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb30300f-ff71-417d-956f-6d3dcede6c29",
   "metadata": {},
   "source": [
    "### import bq to dataframe function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e098413-7211-4346-b1c9-eafd38ef076d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import credentials\n",
    "\n",
    "def import_bq_to_dataframe(project_id, dataset_id, table_id, client): \n",
    "    \n",
    "    \"\"\"\n",
    "    Imports a specific table from BigQuery to a DataFrame. \n",
    "    \n",
    "    Args: \n",
    "        project_id: The name of the project_id where the table is located.\n",
    "        dataset_id: The name of the dataset_id where the table is located.\n",
    "        table_id: The name of the table_id you wish to import to DataFrame.\n",
    "        client: A BigQuery client instance. e.g. client = bigquery.Client(project=project_id).\n",
    "\n",
    "    Returns: \n",
    "        A DataFrame\n",
    "        \n",
    "    Example: \n",
    "        import_bq_to_dataframe('bi-stg-divg-speech-pr-9d940b', 'call_to_retention_dataset', 'bq_ctr_pipeline_dataset')\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    sql = f\"SELECT * FROM `{project_id}.{dataset_id}.{table_id}`\"\n",
    "    \n",
    "    df_return = client.query(sql).to_dataframe()\n",
    "\n",
    "    return df_return\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a31e45-47f9-445e-aab4-1e2d8243e1f9",
   "metadata": {},
   "source": [
    "### define get_lift function, import df_train and df_test from gcs bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3837b0-0180-4a8a-978d-b56402062efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "project_id = PROJECT_ID\n",
    "region = REGION\n",
    "resource_bucket = RESOURCE_BUCKET\n",
    "file_bucket = FILE_BUCKET\n",
    "service_type=SERVICE_TYPE\n",
    "project_id=PROJECT_ID\n",
    "dataset_id=DATASET_ID\n",
    "train_table_id = TRAIN_TABLE_ID\n",
    "val_table_id = VAL_TABLE_ID\n",
    "\n",
    "def get_lift(prob, y_test, q):\n",
    "    result = pd.DataFrame(columns=['Prob', 'Churn'])\n",
    "    result['Prob'] = prob\n",
    "    result['Churn'] = y_test\n",
    "    # result['Decile'] = pd.qcut(1-result['Prob'], 10, labels = False)\n",
    "    result['Decile'] = pd.qcut(result['Prob'], q, labels=[i for i in range(q, 0, -1)])\n",
    "    add = pd.DataFrame(result.groupby('Decile')['Churn'].mean()).reset_index()\n",
    "    add.columns = ['Decile', 'avg_real_churn_rate']\n",
    "    result = result.merge(add, on='Decile', how='left')\n",
    "    result.sort_values('Decile', ascending=True, inplace=True)\n",
    "    lg = pd.DataFrame(result.groupby('Decile')['Prob'].mean()).reset_index()\n",
    "    lg.columns = ['Decile', 'avg_model_pred_churn_rate']\n",
    "    lg.sort_values('Decile', ascending=False, inplace=True)\n",
    "    lg['avg_churn_rate_total'] = result['Churn'].mean()\n",
    "    lg = lg.merge(add, on='Decile', how='left')\n",
    "    lg['lift'] = lg['avg_real_churn_rate'] / lg['avg_churn_rate_total']\n",
    "\n",
    "    return lg\n",
    "\n",
    "def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "    token = os.popen('gcloud auth print-access-token').read()\n",
    "    token = re.sub(f'\\n$', '', token)\n",
    "    credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "    bq_client = bigquery.Client(project=project_id)\n",
    "    if use_local_credential:\n",
    "        bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "    return bq_client\n",
    "\n",
    "client = get_gcp_bqclient(project_id)\n",
    "\n",
    "df_train = import_bq_to_dataframe(project_id, dataset_id, train_table_id, client)\n",
    "df_val = import_bq_to_dataframe(project_id, dataset_id, val_table_id, client)\n",
    "\n",
    "scenario_to_target = {\n",
    "    'hsic_acquisition': 0,\n",
    "    'ttv_acquisition': 1,\n",
    "    'sing_acquisition': 2,\n",
    "    'shs_acquisition': 3, \n",
    "    'tos_acquisition': 4, \n",
    "    'wifi_acquisition': 5, \n",
    "    'lwc_acquisition': 6, \n",
    "    'sws_acquisition': 7, \n",
    "    'hpro_acquisition': 8, \n",
    "    'whsia_acquisition': 9\n",
    "}\n",
    "\n",
    "df_train['target'] = df_train['model_scenario'].map(scenario_to_target)\n",
    "df_val['target'] = df_val['model_scenario'].map(scenario_to_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdfa4ea-7085-493e-8d21-bae9d244155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"gs://divg-groovyhoon-pr-d2eab4-default/downloads/df_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e270188-696f-4b14-8f4a-6463bf1552f4",
   "metadata": {},
   "source": [
    "### Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214d3a22-8203-4a36-be64-8e2f5f5054ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['demogr_lifestage_sort'].fillna(6, inplace=True)\n",
    "df_train['cust_pref_lang_txt'].fillna('NOT AVAILABLE', inplace=True)\n",
    "df_train['demogr_census_division_typ'].fillna('NA', inplace=True)\n",
    "df_train['cust_prov_state_cd'].fillna('N/AVAIL', inplace=True)\n",
    "df_train['cust_cr_val_txt'].fillna('NOT AVAILABLE', inplace=True)\n",
    "df_train['acct_ebill_ind'].fillna('N', inplace=True)\n",
    "\n",
    "df_val['demogr_lifestage_sort'].fillna(6, inplace=True)\n",
    "df_val['cust_pref_lang_txt'].fillna('NOT AVAILABLE', inplace=True)\n",
    "df_val['demogr_census_division_typ'].fillna('NA', inplace=True)\n",
    "df_val['cust_prov_state_cd'].fillna('N/AVAIL', inplace=True)\n",
    "df_val['cust_cr_val_txt'].fillna('NOT AVAILABLE', inplace=True)\n",
    "df_val['acct_ebill_ind'].fillna('N', inplace=True)\n",
    "\n",
    "print(f'df_train: {df_train.shape}')\n",
    "print(f'df_val: {df_val.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94961cc0-7e92-44eb-9e1e-15d16d2024ed",
   "metadata": {},
   "source": [
    "### define train, test, and val dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc272d97-768b-401b-9100-923cc63cf1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "df_train, df_test = train_test_split(df_train, test_size=0.3, random_state=42, stratify=df_train['target'])\n",
    "\n",
    "cols_to_impute = ['demogr_avg_child','demogr_avg_household_size','demogr_avg_income','demogr_employment_rate'\n",
    "                        ,'demogr_med_age','demogr_med_income','demogr_pct_family_with_child_living_at_home']\n",
    "\n",
    "def impute_with_average(df, cols_to_impute):\n",
    "    \n",
    "    for col in cols_to_impute:\n",
    "        column_mean = df[col].mean()\n",
    "        df[col].fillna(column_mean, inplace=True)\n",
    "        \n",
    "    return df\n",
    "\n",
    "df_train = impute_with_average(df_train, cols_to_impute)\n",
    "df_test = impute_with_average(df_test, cols_to_impute)\n",
    "df_val = impute_with_average(df_val, cols_to_impute)\n",
    "\n",
    "#set up features (list)\n",
    "cols_1 = df_train.columns.values\n",
    "cols_2 = df_test.columns.values\n",
    "cols_3 = df_val.columns.values\n",
    "\n",
    "cols = set(cols_1).intersection(set(cols_2))\n",
    "cols = set(cols).intersection(set(cols_3))\n",
    "\n",
    "id_variables = ['split_type', 'model_scenario', 'ref_dt', 'cust_id', 'cust_src_id', 'ban', 'ban_src_id', 'lpds_id', 'fms_address_id', 'label', 'label_dt', 'target']\n",
    "\n",
    "features_to_include = ['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id', 'target', \n",
    "'prod_tos_cnt', 'prod_hsic_cnt', 'prod_ttv_cnt', 'prod_sing_cnt', 'cust_prov_state_cd', 'prod_smhm_cnt', 'sing_tenure_days', 'prod_stv_cnt', 'demogr_census_division_typ', \n",
    "'clk_wln_wifi_plus_cnt_r30d', 'hsic_tenure_days', 'prod_whsia_cnt', 'bill_wln_avg_hsic_debit_amt', 'bill_wln_avg_ttv_debit_amt', 'ttv_tenure_days', 'bill_wln_avg_smhm_debit_amt', 'smhm_tenure_days', \n",
    "'clk_wln_sing_cnt_r30d', 'hs_usg_avg_ul_gb', 'clk_health_livingwell_cnt_r30d', 'hs_usg_avg_dl_gb', 'prod_other_cnt', 'hs_usg_avg_tot_gb', 'bill_wln_avg_ban_discount_amt', 'clk_wln_hsic_cnt_r30d', \n",
    "'acct_ebill_ind', 'clk_wln_smarthome_security_cnt_r30d', 'prod_wifiplus_cnt', 'demogr_avg_household_size', 'clk_wln_optik_cnt_r30d', 'clk_wln_security_cnt_r30d', 'demogr_lifestage_sort', 'demogr_family_flag', \n",
    "'bill_wln_avg_sing_debit_amt', 'ffh_tenure']\n",
    "\n",
    "feature_weights = np.array([0.213879816752624, 0.210757495491595, 0.155501726318607, 0.0602533075942611, 0.0523079604892021, 0.0284744355928214, 0.0270177329993315, 0.0230154595375712, 0.0170414919779867, \n",
    "0.0155022847023418, 0.0146792811382145, 0.0142657660878289, 0.0133395133806645, 0.0132004812421698, 0.0118557752793018, 0.0107220666341007, 0.0101260045930694, 0.00916686983616255, 0.00906401454998682, \n",
    "0.00764893682885285, 0.00758425900125304, 0.00754070816218072, 0.00732201897328859, 0.00718525467726317, 0.00656270026430169, 0.00655088122633769, 0.00584749485663925, 0.00488302137330835, 0.00484122456343937, \n",
    "0.00462082166402429, 0.00433701614023153, 0.00390846588739282, 0.00374843937884009, 0.00366447156176259, 0.003582801])\n",
    "\n",
    "features = [f for f in cols if f in features_to_include]\n",
    "\n",
    "train_id = df_train[id_variables]\n",
    "ban_train = df_train[['ban', 'lpds_id']]\n",
    "X_train = df_train[features]\n",
    "y_train = np.squeeze(df_train['target'].values)\n",
    "y_train = y_train.astype(int)\n",
    "target_train = df_train['target']\n",
    "\n",
    "test_id = df_test[id_variables]\n",
    "ban_test = df_test[['ban', 'lpds_id']]\n",
    "X_test = df_test[features]\n",
    "y_test = np.squeeze(df_test['target'].values)\n",
    "y_test = y_test.astype(int)\n",
    "target_test = df_test['target']\n",
    "\n",
    "val_id = df_val[id_variables]\n",
    "ban_val = df_val[['ban', 'lpds_id']]\n",
    "X_val = df_val[features]\n",
    "y_val = np.squeeze(df_val['target'].values)\n",
    "y_val = y_val.astype(int)\n",
    "target_val = df_val['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11027df1-f59a-47e9-9db7-32325ac3b1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to transform the features of the feature store.\n",
    "def encode_categorical_features(train_df, test_df, val_df):\n",
    "    # Get a list of all categorical columns\n",
    "    cat_columns = train_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    # Encode each categorical column\n",
    "    for col in cat_columns:\n",
    "        le = LabelEncoder()\n",
    "        train_df[col] = le.fit_transform(train_df[col])\n",
    "        test_df[col] = le.fit_transform(test_df[col])\n",
    "        val_df[col] = le.fit_transform(val_df[col])\n",
    "        \n",
    "    return train_df, test_df, val_df\n",
    "\n",
    "#excluding the customer ID so it doesn't get encoded\n",
    "train_label_data=X_train[X_train.columns.difference(['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id', 'target'])] \n",
    "test_label_data=X_test[X_test.columns.difference(['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id', 'target'])]\n",
    "val_label_data=X_val[X_val.columns.difference(['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id', 'target'])]\n",
    "\n",
    "X_train_feat_enc, X_test_feat_enc, X_val_feat_enc = encode_categorical_features(train_label_data,test_label_data,val_label_data)\n",
    "\n",
    "X_train_feat_enc.fillna(0, inplace=True)\n",
    "X_test_feat_enc.fillna(0, inplace=True)\n",
    "X_val_feat_enc.fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9127c70c-3dbc-46d8-ab9e-611dafd840da",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5239ce34-132c-48d4-bdf5-535235ffce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_pca(X, n): \n",
    "    \n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    pca = PCA(n_components = n) \n",
    "    \n",
    "    X_reduced = pca.fit_transform(X)\n",
    "    \n",
    "    cols = [f'PC{i}' for i in range(n)]\n",
    "    \n",
    "    df_reduced = pd.DataFrame(data=X_reduced, columns=cols) \n",
    "    \n",
    "    return df_reduced\n",
    "\n",
    "X_train_feat_enc_pca = perform_pca(X_train_feat_enc, 20)\n",
    "X_test_feat_enc_pca = perform_pca(X_test_feat_enc, 20)\n",
    "X_val_feat_enc_pca = perform_pca(X_val_feat_enc, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983c97b2-a70a-47dd-8062-5e2cb8f0dd57",
   "metadata": {},
   "source": [
    "### Re-attach ID Variables to the Features Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23d0e68-8787-47ee-95d9-d7c083ea1565",
   "metadata": {},
   "outputs": [],
   "source": [
    "##bringing back the customer ids keys\n",
    "X_train_feat_enc['cust_id'] = X_train['cust_id']\n",
    "X_train_feat_enc['ban'] = X_train['ban']\n",
    "X_train_feat_enc['lpds_id'] = X_train['lpds_id']\n",
    "X_train_feat_enc['ref_dt'] = X_train['ref_dt'] \n",
    "X_train_feat_enc['model_scenario'] = X_train['model_scenario']\n",
    "X_train_feat_enc['target'] = y_train\n",
    "\n",
    "X_test_feat_enc['cust_id'] = X_test['cust_id']\n",
    "X_test_feat_enc['ban'] = X_test['ban'] \n",
    "X_test_feat_enc['lpds_id'] = X_test['lpds_id']\n",
    "X_test_feat_enc['ref_dt'] = X_test['ref_dt'] \n",
    "X_test_feat_enc['model_scenario'] = X_test['model_scenario'] \n",
    "X_test_feat_enc['target'] = y_test\n",
    "\n",
    "X_val_feat_enc['cust_id'] = X_val['cust_id'] \n",
    "X_val_feat_enc['ban'] = X_val['ban']\n",
    "X_val_feat_enc['lpds_id'] = X_val['lpds_id'] \n",
    "X_val_feat_enc['ref_dt'] = X_val['ref_dt']\n",
    "X_val_feat_enc['model_scenario'] = X_val['model_scenario']\n",
    "X_val_feat_enc['target'] = y_val\n",
    "\n",
    "train = pd.merge(X_train_feat_enc, train_id[['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id']],how = 'inner',on=['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id'])\n",
    "test = pd.merge(X_test_feat_enc, test_id[['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id']],how = 'inner',on=['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id'])\n",
    "val = pd.merge(X_val_feat_enc, val_id[['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id']],how = 'inner',on=['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id'])\n",
    "\n",
    "##bringing back the customer ids keys\n",
    "X_train_feat_enc_pca['cust_id'] = X_train['cust_id']\n",
    "X_train_feat_enc_pca['ban'] = X_train['ban']\n",
    "X_train_feat_enc_pca['lpds_id'] = X_train['lpds_id']\n",
    "X_train_feat_enc_pca['ref_dt'] = X_train['ref_dt'] \n",
    "X_train_feat_enc_pca['model_scenario'] = X_train['model_scenario']\n",
    "X_train_feat_enc_pca['target'] = y_train\n",
    "\n",
    "X_test_feat_enc_pca['cust_id'] = X_test['cust_id']\n",
    "X_test_feat_enc_pca['ban'] = X_test['ban'] \n",
    "X_test_feat_enc_pca['lpds_id'] = X_test['lpds_id']\n",
    "X_test_feat_enc_pca['ref_dt'] = X_test['ref_dt'] \n",
    "X_test_feat_enc_pca['model_scenario'] = X_test['model_scenario'] \n",
    "X_test_feat_enc_pca['target'] = y_test\n",
    "\n",
    "X_val_feat_enc_pca['cust_id'] = X_val['cust_id'] \n",
    "X_val_feat_enc_pca['ban'] = X_val['ban']\n",
    "X_val_feat_enc_pca['lpds_id'] = X_val['lpds_id'] \n",
    "X_val_feat_enc_pca['ref_dt'] = X_val['ref_dt']\n",
    "X_val_feat_enc_pca['model_scenario'] = X_val['model_scenario']\n",
    "X_val_feat_enc_pca['target'] = y_val\n",
    "\n",
    "train_pca = pd.merge(X_train_feat_enc_pca, train_id[['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id']],how = 'inner',on=['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id'])\n",
    "test_pca = pd.merge(X_test_feat_enc_pca, test_id[['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id']],how = 'inner',on=['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id'])\n",
    "val_pca = pd.merge(X_val_feat_enc_pca, val_id[['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id']],how = 'inner',on=['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id'])\n",
    "\n",
    "### Use SMOTE over-sampling to work around the imbalanced classification problem\n",
    "\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# X_smote = X_train_feat_enc\n",
    "# X_smote = X_smote.drop(columns=['ref_dt', 'model_scenario'])\n",
    "# X_smote = X_smote.astype(int)\n",
    "\n",
    "# # Apply SMOTE to perform oversampling\n",
    "# smote = SMOTE()\n",
    "# X_resampled, y_resampled = smote.fit_resample(X_smote, y_train)\n",
    "\n",
    "# # Convert the resampled dataset into a DataFrame for visualization\n",
    "# df_resampled = pd.DataFrame(X_resampled, columns=[f'feature_{i}' for i in range(X_resampled.shape[1])])\n",
    "# df_resampled['target'] = y_resampled\n",
    "\n",
    "# # Count the class distribution after oversampling\n",
    "# print(\"\\nClass distribution after oversampling:\")\n",
    "# print(df_resampled['target'].value_counts())\n",
    "\n",
    "# X_resampled['target'] = y_resampled\n",
    "\n",
    "# train_smote = X_resampled\n",
    "\n",
    "# target_to_scenario = {\n",
    "#     0: 'hsic_acquisition',\n",
    "#     1: 'ttv_acquisition',\n",
    "#     2: 'sing_acquisition',\n",
    "#     3: 'shs_acquisition', \n",
    "#     4: 'tos_acquisition', \n",
    "#     5: 'wifi_acquisition', \n",
    "#     6: 'lwc_acquisition', \n",
    "#     7: 'sws_acquisition', \n",
    "#     8: 'hpro_acquisition', \n",
    "#     9: 'whsia_acquisition'\n",
    "# }\n",
    "\n",
    "# train_smote['model_scenario'] = train_smote['target'].map(target_to_scenario)\n",
    "# train_smote['ref_dt'] = scoringDate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdfbb80-9392-49b4-93a8-48c2cf553832",
   "metadata": {},
   "source": [
    "### Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860ec674-171e-438c-b660-c550bb1acf52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def final_prod_reco(prod_mix, prod_reco_list): \n",
    "    \n",
    "    # This function accepts the customer's product mix and product recommenndations as input parameters, \n",
    "    # and filters out the product recommendations that the customer already has.\n",
    "    \n",
    "    # 1. check the customer's product intensity\n",
    "    cust_prod_list = []\n",
    "\n",
    "    if prod_mix[0] > 0: \n",
    "        cust_prod_list.append('hsic')\n",
    "    if prod_mix[1] + prod_mix[2] > 0: \n",
    "        cust_prod_list.append('ttv')\n",
    "    if prod_mix[3] > 0: \n",
    "        cust_prod_list.append('sing')\n",
    "    if prod_mix[4] > 0: \n",
    "        cust_prod_list.append('smhm')\n",
    "    if prod_mix[5] > 0: \n",
    "        cust_prod_list.append('tos')\n",
    "    if prod_mix[6] > 0: \n",
    "        cust_prod_list.append('wifi')\n",
    "    if prod_mix[7] > 0: \n",
    "        cust_prod_list.append('whsia')\n",
    "    ### add for lwc, sws, hrpo as product count labels become available ###\n",
    "    # if prod_mix[n] > 0: \n",
    "    #     cust_prod_list.append('lwc')\n",
    "    # if prod_mix[n] > 0: \n",
    "    #     cust_prod_list.append('sws')\n",
    "    # if prod_mix[n] > 0: \n",
    "    #     cust_prod_list.append('hpro')\n",
    "\n",
    "    # 2. trim the prodduct reco list to remove \"_acquisition\" for comparison. \n",
    "    prod_reco_list = [value.replace('_acquisition', '') for value in prod_reco_list]\n",
    "\n",
    "    final_prod_reco_list = [item for item in prod_reco_list if item not in cust_prod_list]\n",
    "\n",
    "    return final_prod_reco_list\n",
    "\n",
    "def get_recommended_offers (df: pd.DataFrame, \n",
    "                            id_cols: list,\n",
    "                            prod_mix_cols: list, \n",
    "                            features: list, \n",
    "                            feature_weights: list, \n",
    "                            ban: int, \n",
    "                            ref_dt, \n",
    "                            distance_func: str,\n",
    "                            n: int,\n",
    "                            minimal_threshold: float,\n",
    "                            max_offers_to_return: int\n",
    "                            ):\n",
    "    \n",
    "    # define df_id\n",
    "    df_id = df[id_cols]\n",
    "\n",
    "    # extract the feature vectors of all customers\n",
    "    X = df[features].values\n",
    "\n",
    "    # extract the feature vector of the given customer\n",
    "    index = df[(df['ban'] == ban) & (df['ref_dt']==ref_dt)].index[0]\n",
    "    x = X[index]\n",
    "    \n",
    "    X = X * feature_weights\n",
    "    x = x * feature_weights\n",
    "\n",
    "    # compute the distances between the feature vectors\n",
    "    if distance_func == 'euclidean':\n",
    "        distances = euclidean_distances(X, x.reshape(1, -1)).flatten()\n",
    "    elif distance_func == 'manhattan':\n",
    "        distances = manhattan_distances(X, x.reshape(1, -1)).flatten()\n",
    "    elif distance_func == 'cosine':\n",
    "        distances = 1 - cosine_similarity(X, x.reshape(1, -1)).flatten()\n",
    "    else:\n",
    "        raise ValueError('Invalid distance function specified.')\n",
    "\n",
    "    # find the indices of the n customers with lowest distance\n",
    "    most_similar_indices = distances.argsort()[:n]\n",
    "\n",
    "    # extract the customer data for the most similar customers\n",
    "    similar_customers = df.iloc[most_similar_indices]\n",
    "    \n",
    "    # calculate the target label distribution\n",
    "    target_dist = df[['cust_id', 'ban', 'lpds_id', 'model_scenario', 'target']].groupby(['model_scenario', 'target'])['ban'].count().reset_index(name='ban_count_total').sort_values(by='ban_count_total', ascending=False)\n",
    "    target_dist['perc_total_all'] = target_dist['ban_count_total']/target_dist['ban_count_total'].sum()\n",
    "\n",
    "    # count the top offers of the similar customers\n",
    "    # top_offers = similar_customers[['cust_id', 'ban', 'lpds_id', 'model_scenario', 'target_y']].groupby(['model_scenario', 'target_y']).agg({'cust_id':'count'}).reset_index().sort_values(by = 'cust_id', ascending = False)\n",
    "    top_offers = similar_customers[['cust_id', 'ban', 'lpds_id', 'model_scenario', 'target']].groupby(['model_scenario', 'target'])['ban'].count().reset_index(name='ban_count').sort_values(by='ban_count', ascending=False)\n",
    "    top_offers['perc_total'] = top_offers['ban_count']/top_offers['ban_count'].sum()\n",
    "    \n",
    "    # generate product recommendations list based on collaborative filtering\n",
    "    prod_mix = df[prod_mix_cols].values[index]\n",
    "    prod_reco_list = list(top_offers['model_scenario'][:])\n",
    "    \n",
    "    # filter out product recos that the customer already has\n",
    "    final_prod_reco_list = final_prod_reco(prod_mix, prod_reco_list)\n",
    "\n",
    "    return final_prod_reco_list \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42850d55-692e-480f-b31e-2fd4534a594a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def final_prod_reco(prod_mix, prod_reco_list): \n",
    "    \n",
    "    # This function accepts the customer's product mix and product recommenndations as input parameters, \n",
    "    # and filters out the product recommendations that the customer already has.\n",
    "    \n",
    "    # 1. check the customer's product intensity\n",
    "    cust_prod_list = []\n",
    "\n",
    "    if prod_mix[0] > 0: \n",
    "        cust_prod_list.append('hsic')\n",
    "    if prod_mix[1] + prod_mix[2] > 0: \n",
    "        cust_prod_list.append('ttv')\n",
    "    if prod_mix[3] > 0: \n",
    "        cust_prod_list.append('sing')\n",
    "    if prod_mix[4] > 0: \n",
    "        cust_prod_list.append('smhm')\n",
    "    if prod_mix[5] > 0: \n",
    "        cust_prod_list.append('tos')\n",
    "    if prod_mix[6] > 0: \n",
    "        cust_prod_list.append('wifi')\n",
    "    if prod_mix[7] > 0: \n",
    "        cust_prod_list.append('whsia')\n",
    "    ### add for lwc, sws, hrpo as product count labels become available ###\n",
    "    # if prod_mix[n] > 0: \n",
    "    #     cust_prod_list.append('lwc')\n",
    "    # if prod_mix[n] > 0: \n",
    "    #     cust_prod_list.append('sws')\n",
    "    # if prod_mix[n] > 0: \n",
    "    #     cust_prod_list.append('hpro')\n",
    "\n",
    "    # 2. trim the prodduct reco list to remove \"_acquisition\" for comparison. \n",
    "    prod_reco_list = [value.replace('_acquisition', '') for value in prod_reco_list]\n",
    "\n",
    "    final_prod_reco_list = [item for item in prod_reco_list if item not in cust_prod_list]\n",
    "\n",
    "    return final_prod_reco_list\n",
    "\n",
    "def get_recommended_offers (df: pd.DataFrame, \n",
    "                            id_cols: list,\n",
    "                            prod_mix_cols: list, \n",
    "                            features: list, \n",
    "                            feature_weights: list, \n",
    "                            ban: int, \n",
    "                            ref_dt, \n",
    "                            distance_func: str,\n",
    "                            n: int,\n",
    "                            minimal_threshold: float,\n",
    "                            max_offers_to_return: int\n",
    "                            ):\n",
    "\n",
    "    def perform_pca(X, n): \n",
    "\n",
    "        from sklearn.decomposition import PCA\n",
    "\n",
    "        pca = PCA(n_components = n) \n",
    "\n",
    "        X_reduced = pca.fit_transform(X)\n",
    "\n",
    "        cols = [f'PC{i}' for i in range(n)]\n",
    "\n",
    "        df_reduced = pd.DataFrame(data=X_reduced, columns=cols) \n",
    "\n",
    "        return df_reduced\n",
    "    \n",
    "    # define df_id\n",
    "    df_id = df[id_cols]\n",
    "\n",
    "    # define df_X \n",
    "    df_X = df[features] * feature_weights\n",
    "    \n",
    "    # define df_X_pca\n",
    "    df_X_pca = perform_pca(df_X, 30)\n",
    "    \n",
    "    # extract the feature vectors of all customers\n",
    "    X = df_X_pca.values\n",
    "\n",
    "    # extract the feature vector of the given customer\n",
    "    index = df[(df['ban'] == ban) & (df['ref_dt']==ref_dt)].index[0]\n",
    "    x = X[index]\n",
    "\n",
    "    # compute the distances between the feature vectors\n",
    "    if distance_func == 'euclidean':\n",
    "        distances = euclidean_distances(X, x.reshape(1, -1)).flatten()\n",
    "    elif distance_func == 'manhattan':\n",
    "        distances = manhattan_distances(X, x.reshape(1, -1)).flatten()\n",
    "    elif distance_func == 'cosine':\n",
    "        distances = 1 - cosine_similarity(X, x.reshape(1, -1)).flatten()\n",
    "    else:\n",
    "        raise ValueError('Invalid distance function specified.')\n",
    "\n",
    "    # find the indices of the n customers with lowest distance\n",
    "    most_similar_indices = distances.argsort()[:n]\n",
    "\n",
    "    # extract the customer data for the most similar customers\n",
    "    similar_customers = df.iloc[most_similar_indices]\n",
    "    \n",
    "    # calculate the target label distribution\n",
    "    target_dist = df[['cust_id', 'ban', 'lpds_id', 'model_scenario', 'target']].groupby(['model_scenario', 'target'])['ban'].count().reset_index(name='ban_count_total').sort_values(by='ban_count_total', ascending=False)\n",
    "    target_dist['perc_total_all'] = target_dist['ban_count_total']/target_dist['ban_count_total'].sum()\n",
    "\n",
    "    # count the top offers of the similar customers\n",
    "    # top_offers = similar_customers[['cust_id', 'ban', 'lpds_id', 'model_scenario', 'target_y']].groupby(['model_scenario', 'target_y']).agg({'cust_id':'count'}).reset_index().sort_values(by = 'cust_id', ascending = False)\n",
    "    top_offers = similar_customers[['cust_id', 'ban', 'lpds_id', 'model_scenario', 'target']].groupby(['model_scenario', 'target'])['ban'].count().reset_index(name='ban_count').sort_values(by='ban_count', ascending=False)\n",
    "    top_offers['perc_total'] = top_offers['ban_count']/top_offers['ban_count'].sum()\n",
    "    \n",
    "    # generate product recommendations list based on collaborative filtering\n",
    "    prod_mix = df[prod_mix_cols].values[index]\n",
    "    prod_reco_list = list(top_offers['model_scenario'][:])\n",
    "    \n",
    "    # filter out product recos that the customer already has\n",
    "    final_prod_reco_list = final_prod_reco(prod_mix, prod_reco_list)\n",
    "\n",
    "    return final_prod_reco_list \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e62dc6-b503-41bb-b125-e5880e4d29ab",
   "metadata": {},
   "source": [
    "### latest function to use for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96b3cd7-37d9-42da-9250-8c9398daf51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_prod_reco(prod_mix, prod_reco_list): \n",
    "    \n",
    "    # This function accepts the customer's product mix and product recommenndations as input parameters, \n",
    "    # and filters out the product recommendations that the customer already has.\n",
    "    \n",
    "    # 1. check the customer's product intensity\n",
    "    cust_prod_list = []\n",
    "\n",
    "    if prod_mix[0] > 0: \n",
    "        cust_prod_list.append('hsic')\n",
    "    if prod_mix[1] + prod_mix[2] > 0: \n",
    "        cust_prod_list.append('ttv')\n",
    "    if prod_mix[3] > 0: \n",
    "        cust_prod_list.append('sing')\n",
    "    if prod_mix[4] > 0: \n",
    "        cust_prod_list.append('shs')\n",
    "    if prod_mix[5] > 0: \n",
    "        cust_prod_list.append('tos')\n",
    "    if prod_mix[6] > 0: \n",
    "        cust_prod_list.append('wifi')\n",
    "    if prod_mix[7] > 0: \n",
    "        cust_prod_list.append('whsia')\n",
    "    ### add for lwc, sws, hrpo as product count labels become available ###\n",
    "    # if prod_mix[n] > 0: \n",
    "    #     cust_prod_list.append('lwc')\n",
    "    # if prod_mix[n] > 0: \n",
    "    #     cust_prod_list.append('sws')\n",
    "    # if prod_mix[n] > 0: \n",
    "    #     cust_prod_list.append('hpro')\n",
    "\n",
    "    # 2. trim the prodduct reco list to remove \"_acquisition\" for comparison. \n",
    "    prod_reco_list = [value.replace('_acquisition', '') for value in prod_reco_list]\n",
    "\n",
    "    final_prod_reco_list = [item for item in prod_reco_list if item not in cust_prod_list]\n",
    "\n",
    "    return final_prod_reco_list\n",
    "\n",
    "def get_recommended_offers (df: pd.DataFrame, \n",
    "                            id_cols: list,\n",
    "                            prod_mix_cols: list, \n",
    "                            features: list, \n",
    "                            feature_weights: list, \n",
    "                            ban: int, \n",
    "                            ref_dt, \n",
    "                            distance_func: str,\n",
    "                            n: int,\n",
    "                            minimal_threshold: float,\n",
    "                            max_offers_to_return: int\n",
    "                            ):\n",
    "\n",
    "    def perform_pca(X, n): \n",
    "\n",
    "        from sklearn.decomposition import PCA\n",
    "\n",
    "        pca = PCA(n_components = n) \n",
    "\n",
    "        X_reduced = pca.fit_transform(X)\n",
    "\n",
    "        cols = [f'PC{i}' for i in range(n)]\n",
    "\n",
    "        df_reduced = pd.DataFrame(data=X_reduced, columns=cols) \n",
    "\n",
    "        return df_reduced\n",
    "\n",
    "    # define df_X \n",
    "    df_X = df[features] * feature_weights\n",
    "    \n",
    "#     # define df_X_pca\n",
    "#     df_X_pca = perform_pca(df_X, 30)\n",
    "    \n",
    "    # extract the feature vectors of all customers\n",
    "    X = df_X.values\n",
    "    \n",
    "    # extract the feature vector of the given customer\n",
    "    index = df[(df['ban'] == ban) & (df['ref_dt']==ref_dt)].index[0]\n",
    "    x = X[index]\n",
    "\n",
    "    # compute the distances between the feature vectors\n",
    "    if distance_func == 'euclidean':\n",
    "        distances = euclidean_distances(X, x.reshape(1, -1)).flatten()\n",
    "    elif distance_func == 'manhattan':\n",
    "        distances = manhattan_distances(X, x.reshape(1, -1)).flatten()\n",
    "    elif distance_func == 'cosine':\n",
    "        distances = 1 - cosine_similarity(X, x.reshape(1, -1)).flatten()\n",
    "    else:\n",
    "        raise ValueError('Invalid distance function specified.')\n",
    "\n",
    "    # find the indices of the n customers with lowest distance\n",
    "    most_similar_indices = distances.argsort()[:n]\n",
    "    \n",
    "    # extract the customer data for the most similar customers\n",
    "    similar_customers = df.iloc[most_similar_indices]\n",
    "    \n",
    "    # count the top offers of the similar customers\n",
    "    # top_offers = similar_customers[['cust_id', 'ban', 'lpds_id', 'model_scenario', 'target_y']].groupby(['model_scenario', 'target_y']).agg({'cust_id':'count'}).reset_index().sort_values(by = 'cust_id', ascending = False)\n",
    "    top_offers = similar_customers[['cust_id', 'ban', 'lpds_id', 'model_scenario', 'target']].groupby(['model_scenario', 'target'])['ban'].count().reset_index(name='ban_count').sort_values(by='ban_count', ascending=False)\n",
    "    top_offers['perc_total'] = top_offers['ban_count']/top_offers['ban_count'].sum()\n",
    "    \n",
    "    # generate product recommendations list based on collaborative filtering\n",
    "    prod_mix = df[prod_mix_cols].values[index]\n",
    "    prod_reco_list = list(top_offers['model_scenario'][:])\n",
    "    \n",
    "    # filter out product recos that the customer already has\n",
    "    final_prod_reco_list = final_prod_reco(prod_mix, prod_reco_list)\n",
    "    \n",
    "    print(final_prod_reco_list)\n",
    "\n",
    "    return final_prod_reco_list \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb27a1cd-f385-4428-9cef-233fbe538341",
   "metadata": {},
   "source": [
    "### 3. apply the function to the entire scoring set (without bootstrapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b545c5b-c84a-4ee0-bcae-aa66a3807576",
   "metadata": {},
   "source": [
    "### 50 similar customers x 2000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94542dd-5aba-4278-b262-58d778aa6826",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = train\n",
    "# prod_mix_cols: internet, tv, tv, sing, smhm, tos, wifi, whsia\n",
    "prod_mix_cols = ['prod_hsic_cnt','prod_ttv_cnt','prod_stv_cnt','prod_sing_cnt','prod_smhm_cnt','prod_tos_cnt','prod_wifiplus_cnt','prod_whsia_cnt']\n",
    "id_cols=['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id', 'target']\n",
    "distance_func = 'cosine'\n",
    "n = 50\n",
    "minimal_threshold = 0.10\n",
    "max_offers_to_return = 3\n",
    "\n",
    "features = ['prod_tos_cnt', 'prod_hsic_cnt', 'prod_ttv_cnt', 'prod_sing_cnt', 'cust_prov_state_cd', 'prod_smhm_cnt', 'sing_tenure_days', 'prod_stv_cnt', 'demogr_census_division_typ', \n",
    "'clk_wln_wifi_plus_cnt_r30d', 'hsic_tenure_days', 'prod_whsia_cnt', 'bill_wln_avg_hsic_debit_amt', 'bill_wln_avg_ttv_debit_amt', 'ttv_tenure_days', 'bill_wln_avg_smhm_debit_amt', 'smhm_tenure_days', \n",
    "'clk_wln_sing_cnt_r30d', 'hs_usg_avg_ul_gb', 'clk_health_livingwell_cnt_r30d', 'hs_usg_avg_dl_gb', 'prod_other_cnt', 'hs_usg_avg_tot_gb', 'bill_wln_avg_ban_discount_amt', 'clk_wln_hsic_cnt_r30d', \n",
    "'acct_ebill_ind', 'clk_wln_smarthome_security_cnt_r30d', 'prod_wifiplus_cnt', 'demogr_avg_household_size', 'clk_wln_optik_cnt_r30d', 'clk_wln_security_cnt_r30d', 'demogr_lifestage_sort', 'demogr_family_flag', \n",
    "'bill_wln_avg_sing_debit_amt', 'ffh_tenure']\n",
    "\n",
    "feature_weights = np.array([0.213879816752624, 0.210757495491595, 0.155501726318607, 0.0602533075942611, 0.0523079604892021, 0.0284744355928214, 0.0270177329993315, 0.0230154595375712, 0.0170414919779867, \n",
    "0.0155022847023418, 0.0146792811382145, 0.0142657660878289, 0.0133395133806645, 0.0132004812421698, 0.0118557752793018, 0.0107220666341007, 0.0101260045930694, 0.00916686983616255, 0.00906401454998682, \n",
    "0.00764893682885285, 0.00758425900125304, 0.00754070816218072, 0.00732201897328859, 0.00718525467726317, 0.00656270026430169, 0.00655088122633769, 0.00584749485663925, 0.00488302137330835, 0.00484122456343937, \n",
    "0.00462082166402429, 0.00433701614023153, 0.00390846588739282, 0.00374843937884009, 0.00366447156176259, 0.003582801])\n",
    "\n",
    "def production_model (df, train, test, id_cols, prod_mix_cols, distance_func, n, minimal_threshold, max_offers_to_return):\n",
    "    \n",
    "    frame = pd.DataFrame()\n",
    "    \n",
    "    train_backup = train\n",
    "    test_backup = test\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    # For each customer in each month\n",
    "    for cust in list(df['ban'].unique()):\n",
    "        i += 1\n",
    "        if i % 50 == 0: \n",
    "            print(f'{i} customers processed')\n",
    "            \n",
    "        train = train_backup\n",
    "        test = test_backup\n",
    "        \n",
    "        for dt in list(df[df['ban']==cust]['ref_dt'].unique()):\n",
    "            #This part of the code adds the line we want to get offers to the training set, so we can use the distance formula\n",
    "            data = pd.DataFrame()\n",
    "            \n",
    "            add_test = test[(test['ban']==cust)&(test['ref_dt']==dt)]\n",
    "            \n",
    "            if 'target' in add_test.columns: \n",
    "                add_test.drop(columns=['target'], inplace=True)\n",
    "            \n",
    "            add_test['target'] = 0\n",
    "                        \n",
    "            data = data.reset_index()\n",
    "\n",
    "            results = get_recommended_offers(data, id_cols, prod_mix_cols, features, feature_weights, cust, dt, distance_func, n, minimal_threshold=minimal_threshold, max_offers_to_return=max_offers_to_return)\n",
    "            \n",
    "            data = {'ban': [cust],\n",
    "                  'dt': [dt],\n",
    "                  'offers': [results]}\n",
    "            \n",
    "            frame1 =  pd.DataFrame(data)\n",
    "            frame = frame.append(frame1)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "save_data_path = 'gs://divg-groovyhoon-pr-d2eab4-default/downloads/offer_recommendation_50_similar_customers.csv'\n",
    "test_samples = test.sample(n=10000, random_state=27)\n",
    "frame_production_100_samples = production_model(test_samples, train, test, id_cols, prod_mix_cols, distance_func, n, minimal_threshold, max_offers_to_return)\n",
    "frame_production_100_samples.to_csv(save_data_path,index=False)\n",
    "print(f'csv saved in {save_data_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5e52e0-d8f5-4b04-8bf6-af5bcc38e16c",
   "metadata": {},
   "source": [
    "### 100 similar customers x 2000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda0b064-b2ed-4cda-b205-35036281a544",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = train\n",
    "# prod_mix_cols: internet, tv, tv, sing, smhm, tos, wifi, whsia\n",
    "prod_mix_cols = ['prod_hsic_cnt','prod_ttv_cnt','prod_stv_cnt','prod_sing_cnt','prod_smhm_cnt','prod_tos_cnt','prod_wifiplus_cnt','prod_whsia_cnt']\n",
    "id_cols=['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id', 'target']\n",
    "distance_func = 'cosine'\n",
    "n = 100\n",
    "minimal_threshold = 0.10\n",
    "max_offers_to_return = 3\n",
    "\n",
    "features = ['prod_tos_cnt', 'prod_hsic_cnt', 'prod_ttv_cnt', 'prod_sing_cnt', 'cust_prov_state_cd', 'prod_smhm_cnt', 'sing_tenure_days', 'prod_stv_cnt', 'demogr_census_division_typ', \n",
    "'clk_wln_wifi_plus_cnt_r30d', 'hsic_tenure_days', 'prod_whsia_cnt', 'bill_wln_avg_hsic_debit_amt', 'bill_wln_avg_ttv_debit_amt', 'ttv_tenure_days', 'bill_wln_avg_smhm_debit_amt', 'smhm_tenure_days', \n",
    "'clk_wln_sing_cnt_r30d', 'hs_usg_avg_ul_gb', 'clk_health_livingwell_cnt_r30d', 'hs_usg_avg_dl_gb', 'prod_other_cnt', 'hs_usg_avg_tot_gb', 'bill_wln_avg_ban_discount_amt', 'clk_wln_hsic_cnt_r30d', \n",
    "'acct_ebill_ind', 'clk_wln_smarthome_security_cnt_r30d', 'prod_wifiplus_cnt', 'demogr_avg_household_size', 'clk_wln_optik_cnt_r30d', 'clk_wln_security_cnt_r30d', 'demogr_lifestage_sort', 'demogr_family_flag', \n",
    "'bill_wln_avg_sing_debit_amt', 'ffh_tenure']\n",
    "\n",
    "feature_weights = np.array([0.213879816752624, 0.210757495491595, 0.155501726318607, 0.0602533075942611, 0.0523079604892021, 0.0284744355928214, 0.0270177329993315, 0.0230154595375712, 0.0170414919779867, \n",
    "0.0155022847023418, 0.0146792811382145, 0.0142657660878289, 0.0133395133806645, 0.0132004812421698, 0.0118557752793018, 0.0107220666341007, 0.0101260045930694, 0.00916686983616255, 0.00906401454998682, \n",
    "0.00764893682885285, 0.00758425900125304, 0.00754070816218072, 0.00732201897328859, 0.00718525467726317, 0.00656270026430169, 0.00655088122633769, 0.00584749485663925, 0.00488302137330835, 0.00484122456343937, \n",
    "0.00462082166402429, 0.00433701614023153, 0.00390846588739282, 0.00374843937884009, 0.00366447156176259, 0.003582801])\n",
    "\n",
    "def production_model (df, train, test, id_cols, prod_mix_cols, distance_func, n, minimal_threshold, max_offers_to_return):\n",
    "    \n",
    "    frame = pd.DataFrame()\n",
    "    \n",
    "    test_backup = test\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    # For each customer in each month\n",
    "    for cust in list(df['ban'].unique()):\n",
    "        i += 1\n",
    "        if i % 1 == 0: \n",
    "            print(f'{i} customers processed')\n",
    "            \n",
    "        test = test_backup\n",
    "        \n",
    "        for dt in list(df[df['ban']==cust]['ref_dt'].unique()):\n",
    "            #This part of the code adds the line we want to get offers to the training set, so we can use the distance formula\n",
    "            data = pd.DataFrame()\n",
    "            \n",
    "            add_test = test[(test['ban']==cust)&(test['ref_dt']==dt)]\n",
    "            \n",
    "            if 'target' in add_test.columns: \n",
    "                add_test.drop(columns=['target'], inplace=True)\n",
    "            \n",
    "            add_test['target'] = 0\n",
    "                        \n",
    "            data = train.append(add_test)\n",
    "            data= data.reset_index()\n",
    "\n",
    "            results = get_recommended_offers(data, id_cols, prod_mix_cols, features, feature_weights, cust, dt, distance_func, n, minimal_threshold=minimal_threshold, max_offers_to_return=max_offers_to_return)\n",
    "            \n",
    "            data = {'ban': [cust],\n",
    "                  'dt': [dt],\n",
    "                  'offers': [results]}\n",
    "            \n",
    "            frame1 =  pd.DataFrame(data)\n",
    "            frame = frame.append(frame1)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "save_data_path = 'gs://divg-groovyhoon-pr-d2eab4-default/downloads/offer_recommendation_100_similar_customers.csv'\n",
    "test_samples = test.sample(n=10000, random_state=2153)\n",
    "frame_production_100_samples = production_model(test_samples, train, test, id_cols, prod_mix_cols, distance_func, n, minimal_threshold, max_offers_to_return)\n",
    "frame_production_100_samples.to_csv(save_data_path,index=False)\n",
    "print(f'csv saved in {save_data_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be81ceae-80a4-4716-b9b9-bbb15767c42d",
   "metadata": {},
   "source": [
    "### 200 similar customers x 2000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880539f3-69bf-417a-ad5e-200f3dee83b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = train\n",
    "# prod_mix_cols: internet, tv, tv, sing, smhm, tos, wifi, whsia\n",
    "prod_mix_cols = ['prod_hsic_cnt','prod_ttv_cnt','prod_stv_cnt','prod_sing_cnt','prod_smhm_cnt','prod_tos_cnt','prod_wifiplus_cnt','prod_whsia_cnt']\n",
    "id_cols=['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id', 'target']\n",
    "distance_func = 'cosine'\n",
    "n = 200\n",
    "minimal_threshold = 0.10\n",
    "max_offers_to_return = 3\n",
    "\n",
    "features = ['prod_tos_cnt', 'prod_hsic_cnt', 'prod_ttv_cnt', 'prod_sing_cnt', 'cust_prov_state_cd', 'prod_smhm_cnt', 'sing_tenure_days', 'prod_stv_cnt', 'demogr_census_division_typ', \n",
    "'clk_wln_wifi_plus_cnt_r30d', 'hsic_tenure_days', 'prod_whsia_cnt', 'bill_wln_avg_hsic_debit_amt', 'bill_wln_avg_ttv_debit_amt', 'ttv_tenure_days', 'bill_wln_avg_smhm_debit_amt', 'smhm_tenure_days', \n",
    "'clk_wln_sing_cnt_r30d', 'hs_usg_avg_ul_gb', 'clk_health_livingwell_cnt_r30d', 'hs_usg_avg_dl_gb', 'prod_other_cnt', 'hs_usg_avg_tot_gb', 'bill_wln_avg_ban_discount_amt', 'clk_wln_hsic_cnt_r30d', \n",
    "'acct_ebill_ind', 'clk_wln_smarthome_security_cnt_r30d', 'prod_wifiplus_cnt', 'demogr_avg_household_size', 'clk_wln_optik_cnt_r30d', 'clk_wln_security_cnt_r30d', 'demogr_lifestage_sort', 'demogr_family_flag', \n",
    "'bill_wln_avg_sing_debit_amt', 'ffh_tenure']\n",
    "\n",
    "feature_weights = np.array([0.213879816752624, 0.210757495491595, 0.155501726318607, 0.0602533075942611, 0.0523079604892021, 0.0284744355928214, 0.0270177329993315, 0.0230154595375712, 0.0170414919779867, \n",
    "0.0155022847023418, 0.0146792811382145, 0.0142657660878289, 0.0133395133806645, 0.0132004812421698, 0.0118557752793018, 0.0107220666341007, 0.0101260045930694, 0.00916686983616255, 0.00906401454998682, \n",
    "0.00764893682885285, 0.00758425900125304, 0.00754070816218072, 0.00732201897328859, 0.00718525467726317, 0.00656270026430169, 0.00655088122633769, 0.00584749485663925, 0.00488302137330835, 0.00484122456343937, \n",
    "0.00462082166402429, 0.00433701614023153, 0.00390846588739282, 0.00374843937884009, 0.00366447156176259, 0.003582801])\n",
    "\n",
    "def production_model (df, train, test, id_cols, prod_mix_cols, distance_func, n, minimal_threshold, max_offers_to_return):\n",
    "    \n",
    "    frame = pd.DataFrame()\n",
    "    \n",
    "    test_backup = test\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    # For each customer in each month\n",
    "    for cust in list(df['ban'].unique()):\n",
    "        i += 1\n",
    "        if i % 1 == 0: \n",
    "            print(f'{i} customers processed')\n",
    "            \n",
    "        test = test_backup\n",
    "        \n",
    "        for dt in list(df[df['ban']==cust]['ref_dt'].unique()):\n",
    "            #This part of the code adds the line we want to get offers to the training set, so we can use the distance formula\n",
    "            data = pd.DataFrame()\n",
    "            \n",
    "            add_test = test[(test['ban']==cust)&(test['ref_dt']==dt)]\n",
    "            \n",
    "            if 'target' in add_test.columns: \n",
    "                add_test.drop(columns=['target'], inplace=True)\n",
    "            \n",
    "            add_test['target'] = 0\n",
    "                        \n",
    "            data = train.append(add_test)\n",
    "            data= data.reset_index()\n",
    "\n",
    "            results = get_recommended_offers(data, id_cols, prod_mix_cols, features, feature_weights, cust, dt, distance_func, n, minimal_threshold=minimal_threshold, max_offers_to_return=max_offers_to_return)\n",
    "            \n",
    "            data = {'ban': [cust],\n",
    "                  'dt': [dt],\n",
    "                  'offers': [results]}\n",
    "            \n",
    "            frame1 =  pd.DataFrame(data)\n",
    "            frame = frame.append(frame1)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "save_data_path = 'gs://divg-groovyhoon-pr-d2eab4-default/downloads/offer_recommendation_200_similar_customers.csv'\n",
    "test_samples = test.sample(n=2000, random_state=128)\n",
    "frame_production_100_samples = production_model(test_samples, train, test, id_cols, prod_mix_cols, distance_func, n, minimal_threshold, max_offers_to_return)\n",
    "frame_production_100_samples.to_csv(save_data_path,index=False)\n",
    "print(f'csv saved in {save_data_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbceacc-fb78-4f47-897c-c6ca60c3a3dd",
   "metadata": {},
   "source": [
    "### 500 similar customers x 2000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2096f78-704c-467a-813e-21d76dea044b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train\n",
    "# prod_mix_cols: internet, tv, tv, sing, smhm, tos, wifi, whsia\n",
    "prod_mix_cols = ['prod_hsic_cnt','prod_ttv_cnt','prod_stv_cnt','prod_sing_cnt','prod_smhm_cnt','prod_tos_cnt','prod_wifiplus_cnt','prod_whsia_cnt']\n",
    "id_cols=['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id', 'target']\n",
    "distance_func = 'cosine'\n",
    "n = 500\n",
    "minimal_threshold = 0.10\n",
    "max_offers_to_return = 3\n",
    "\n",
    "features = ['prod_tos_cnt', 'prod_hsic_cnt', 'prod_ttv_cnt', 'prod_sing_cnt', 'cust_prov_state_cd', 'prod_smhm_cnt', 'sing_tenure_days', 'prod_stv_cnt', 'demogr_census_division_typ', \n",
    "'clk_wln_wifi_plus_cnt_r30d', 'hsic_tenure_days', 'prod_whsia_cnt', 'bill_wln_avg_hsic_debit_amt', 'bill_wln_avg_ttv_debit_amt', 'ttv_tenure_days', 'bill_wln_avg_smhm_debit_amt', 'smhm_tenure_days', \n",
    "'clk_wln_sing_cnt_r30d', 'hs_usg_avg_ul_gb', 'clk_health_livingwell_cnt_r30d', 'hs_usg_avg_dl_gb', 'prod_other_cnt', 'hs_usg_avg_tot_gb', 'bill_wln_avg_ban_discount_amt', 'clk_wln_hsic_cnt_r30d', \n",
    "'acct_ebill_ind', 'clk_wln_smarthome_security_cnt_r30d', 'prod_wifiplus_cnt', 'demogr_avg_household_size', 'clk_wln_optik_cnt_r30d', 'clk_wln_security_cnt_r30d', 'demogr_lifestage_sort', 'demogr_family_flag', \n",
    "'bill_wln_avg_sing_debit_amt', 'ffh_tenure']\n",
    "\n",
    "feature_weights = np.array([0.213879816752624, 0.210757495491595, 0.155501726318607, 0.0602533075942611, 0.0523079604892021, 0.0284744355928214, 0.0270177329993315, 0.0230154595375712, 0.0170414919779867, \n",
    "0.0155022847023418, 0.0146792811382145, 0.0142657660878289, 0.0133395133806645, 0.0132004812421698, 0.0118557752793018, 0.0107220666341007, 0.0101260045930694, 0.00916686983616255, 0.00906401454998682, \n",
    "0.00764893682885285, 0.00758425900125304, 0.00754070816218072, 0.00732201897328859, 0.00718525467726317, 0.00656270026430169, 0.00655088122633769, 0.00584749485663925, 0.00488302137330835, 0.00484122456343937, \n",
    "0.00462082166402429, 0.00433701614023153, 0.00390846588739282, 0.00374843937884009, 0.00366447156176259, 0.003582801])\n",
    "\n",
    "def production_model (df, train, test, id_cols, prod_mix_cols, distance_func, n, minimal_threshold, max_offers_to_return):\n",
    "    \n",
    "    frame = pd.DataFrame()\n",
    "    \n",
    "    test_backup = test\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    # For each customer in each month\n",
    "    for cust in list(df['ban'].unique()):\n",
    "        i += 1\n",
    "        if i % 1 == 0: \n",
    "            print(f'{i} customers processed')\n",
    "            \n",
    "        test = test_backup\n",
    "        \n",
    "        for dt in list(df[df['ban']==cust]['ref_dt'].unique()):\n",
    "            #This part of the code adds the line we want to get offers to the training set, so we can use the distance formula\n",
    "            data = pd.DataFrame()\n",
    "            \n",
    "            add_test = test[(test['ban']==cust)&(test['ref_dt']==dt)]\n",
    "            \n",
    "            if 'target' in add_test.columns: \n",
    "                add_test.drop(columns=['target'], inplace=True)\n",
    "            \n",
    "            add_test['target'] = 0\n",
    "                        \n",
    "            data = train.append(add_test)\n",
    "            data= data.reset_index()\n",
    "\n",
    "            results = get_recommended_offers(data, id_cols, prod_mix_cols, features, feature_weights, cust, dt, distance_func, n, minimal_threshold=minimal_threshold, max_offers_to_return=max_offers_to_return)\n",
    "            \n",
    "            data = {'ban': [cust],\n",
    "                  'dt': [dt],\n",
    "                  'offers': [results]}\n",
    "            \n",
    "            frame1 =  pd.DataFrame(data)\n",
    "            frame = frame.append(frame1)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "save_data_path = 'gs://divg-groovyhoon-pr-d2eab4-default/downloads/offer_recommendation_500_similar_customers.csv'\n",
    "test_samples = test.sample(n=2000, random_state=654)\n",
    "frame_production_100_samples = production_model(test_samples, train, test, id_cols, prod_mix_cols, distance_func, n, minimal_threshold, max_offers_to_return)\n",
    "frame_production_100_samples.to_csv(save_data_path,index=False)\n",
    "print(f'csv saved in {save_data_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f8c7c8-06b4-4d57-9d1e-5bf86ff9c234",
   "metadata": {},
   "source": [
    "### 4. apply the function to the entire scoring set (with bootstrapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef04a93-c294-4beb-aef6-5ef5fb91164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = train\n",
    "# prod_mix_cols: internet, tv, tv, sing, smhm, tos, wifi, whsia\n",
    "id_cols=['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id', 'target']\n",
    "prod_mix_cols = ['prod_hsic_cnt','prod_ttv_cnt','prod_stv_cnt','prod_sing_cnt','prod_smhm_cnt','prod_tos_cnt','prod_wifiplus_cnt','prod_whsia_cnt']\n",
    "distance_funcs = ['euclidean', 'manhattan', 'cosine']\n",
    "n_values = [500, 1000, 2000, 5000, 10000] \n",
    "minimal_threshold = 0.10\n",
    "max_offers_to_return = 3\n",
    "\n",
    "def production_model (df, train, test, id_cols, prod_mix_cols, distance_funcs, n_values, minimal_threshold, max_offers_to_return):\n",
    "    \n",
    "    frame = pd.DataFrame()\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    # For each customer in each month\n",
    "    for cust in list(df['ban'].unique()):\n",
    "        i += 1\n",
    "        if i % 50 == 0: \n",
    "            print(f'{i} customers processed')\n",
    "        \n",
    "        for dt in list(df[df['ban']==cust]['ref_dt'].unique()):\n",
    "            #This part of the code adds the line we want to get offers to the training set, so we can use the distance formula\n",
    "            data = pd.DataFrame()\n",
    "            data = train.append(test[(test['ban']==cust)&(test['ref_dt']==dt)])\n",
    "            data= data.reset_index()\n",
    "\n",
    "            results = get_recommended_offers_multiple(data, id_cols, prod_mix_cols, cust, dt, distance_funcs, n_values, minimal_threshold=minimal_threshold, max_offers_to_return=max_offers_to_return)\n",
    "            data = {'ban': [cust],\n",
    "                  'dt': [dt],\n",
    "                  'offers': [results]}\n",
    "            \n",
    "            frame1 =  pd.DataFrame(data)\n",
    "            frame = frame.append(frame1)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "test_samples = test.sample(n=1000, random_state=42)\n",
    "frame_production_100_samples = production_model(test_samples, train, test, id_cols, prod_mix_cols, distance_funcs, n_values, minimal_threshold, max_offers_to_return)\n",
    "frame_production_100_samples.to_csv('gs://divg-groovyhoon-pr-d2eab4-default/downloads/offer_recommendation_with_bootstap_1000.csv',index=False)\n",
    "print(f'csv saved in gs://divg-groovyhoon-pr-d2eab4-default/downloads/offer_recommendation_with_bootstap_1000.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426e602c-d3dd-4748-9617-410bc68b193e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c667351-fbe6-470c-bc4b-2cc2f1715182",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": ".m116",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m116"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
