{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6dc6e5-5e6d-47d1-92b4-145c5fcf5a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip install matplotlib\n",
    "# !pip install retrying\n",
    "# !pip install scikit-learn\n",
    "# !pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02b5d6a-8a6a-432c-aa9f-01c46c9f9b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import global modules\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from yaml import safe_load\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.metrics import Recall\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "\n",
    "# Set global vars\n",
    "pth_project = Path(os.getcwd().split('notebooks')[0])\n",
    "pth_data = pth_project / 'data'\n",
    "pth_utils = pth_project / 'utils'\n",
    "pth_queries = pth_project / 'queries'\n",
    "pth_creds = pth_project / 'conf' / 'local' / 'project_config.yaml'\n",
    "pth_recommenders = pth_data / 'recommenders'\n",
    "sys.path.insert(0, pth_project.as_posix())\n",
    "d_config = safe_load(pth_creds.open())\n",
    "\n",
    "# import local modules\n",
    "from utils.gcp import connect_bq_services, connect_pandas_bq_services\n",
    "from utils.extract import extract_bq_data\n",
    "from utils.modeling import process_features, extract_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc69db0-7ad3-48a0-90ac-4f803d34d410",
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_client = connect_bq_services(d_config['gcp-project-name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1f062a-260b-4add-b76f-af134a9e9bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092b6ad1-e8fb-4a16-86c7-ea8f573cfa44",
   "metadata": {},
   "source": [
    "#### Extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f54d5d-7114-4173-afb2-fba807c0c37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract trainning data\n",
    "sql = f\"\"\"\n",
    "  select *\n",
    "    from `divg-groovyhoon-pr-d2eab4.nba_product_reco_model.nba_training_dataset_v9`\n",
    "\"\"\"\n",
    "df_train = extract_bq_data(bq_client, sql)\n",
    "print(df_train.shape)\n",
    "\n",
    "# extract validation data\n",
    "sql = f\"\"\"\n",
    "  select *\n",
    "    from `divg-groovyhoon-pr-d2eab4.nba_product_reco_model.nba_val_dataset_v9`\n",
    "\"\"\"\n",
    "df_validation = extract_bq_data(bq_client, sql)\n",
    "print(df_validation.shape)\n",
    "\n",
    "# extract test data\n",
    "sql = f\"\"\"\n",
    "  select *\n",
    "    from `divg-groovyhoon-pr-d2eab4.nba_product_reco_model.nba_test_dataset_v9`\n",
    "\"\"\"\n",
    "df_test = extract_bq_data(bq_client, sql)\n",
    "print(df_validation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fe0f0f-daef-481a-b7e5-d3cb71b87d1d",
   "metadata": {},
   "source": [
    "#### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c60ae5-a241-4401-ae9a-88a8a5f259ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_target_mapping = {\n",
    " 'sing_acquisition': 0,\n",
    " 'shs_acquisition': 1,\n",
    " 'tos_acquisition': 2,\n",
    " 'wifi_acquisition': 3,\n",
    " 'ttv_acquisition': 4,\n",
    " 'sws_acquisition': 5,\n",
    " 'hsic_acquisition': 6,\n",
    " 'lwc_acquisition': 7,\n",
    " 'hpro_acquisition': 8,\n",
    " 'whsia_acquisition': 9\n",
    "}\n",
    "\n",
    "# load features metadata\n",
    "d_features_metadata = safe_load((pth_utils / 'parameters' / 'acquisition_features_v7.yaml').open())\n",
    "\n",
    "# process training data\n",
    "df_train_processed = process_features(df_train, d_features_metadata, 'model_scenario', d_target_mapping)\n",
    "df_validation_processed = process_features(df_validation, d_features_metadata, 'model_scenario', d_target_mapping)\n",
    "df_test_processed = process_features(df_test, d_features_metadata, 'model_scenario', d_target_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1971f546-1957-40df-8b07-c538fef00d20",
   "metadata": {},
   "source": [
    "### Prepare data for CNN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503faf53-e054-4bab-8f44-d941d4c7f88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) attach ids to df_processed\n",
    "# 2) transform df_processed \n",
    "# 3) reattach features by joining on id cols\n",
    "\n",
    "# attach id cols to df_train_processed\n",
    "df_train_processed_ids  = pd.merge(df_train[['split_type', 'model_scenario',  'ref_dt', 'cust_id', 'ban', 'lpds_id', 'label', 'label_dt']], df_train_processed, left_index=True, right_index=True, how='inner')\n",
    "\n",
    "# # attach id cols to df_validation_processed\n",
    "# df_validation_processed_ids \n",
    "df_validation_processed_ids = pd.merge(df_validation[['split_type', 'model_scenario',  'ref_dt', 'cust_id', 'ban', 'lpds_id', 'label', 'label_dt']], df_validation_processed, left_index=True, right_index=True, how='inner')\n",
    "\n",
    "# # attach id cols to df_test_processed\n",
    "df_test_processed_ids = pd.merge(df_test[['split_type', 'model_scenario',  'ref_dt', 'cust_id', 'ban', 'lpds_id', 'label', 'label_dt']], df_test_processed, left_index=True, right_index=True, how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe74659-9df8-4342-8ac8-d432f7d2abd3",
   "metadata": {},
   "source": [
    "### Transform train, val, test sets so that each row is unique for one cust_id+ban+lpds_id AND target labels are one-hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5618991-0993-4088-96e7-e9f41f0ac85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_processed_unique = df_train_processed_ids.drop(['split_type', 'model_scenario', 'ref_dt', 'label', 'label_dt', 'target'], axis=1)\n",
    "df_validation_processed_unique = df_validation_processed_ids.drop(['split_type', 'model_scenario', 'ref_dt', 'label', 'label_dt', 'target'], axis=1)\n",
    "df_test_processed_unique = df_test_processed_ids.drop(['split_type', 'model_scenario', 'ref_dt', 'label', 'label_dt', 'target'], axis=1)\n",
    "\n",
    "df_train_processed_unique = df_train_processed_ids.drop_duplicates(subset=['cust_id', 'ban', 'lpds_id'])\n",
    "df_validation_processed_unique = df_validation_processed_ids.drop_duplicates(subset=['cust_id', 'ban', 'lpds_id'])\n",
    "df_test_processed_unique = df_test_processed_ids.drop_duplicates(subset=['cust_id', 'ban', 'lpds_id'])\n",
    "\n",
    "df_train_processed_unique.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b6ce69-d231-401e-91d7-eab5378eb989",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_grouped = df_train_processed_ids.groupby(['lpds_id', 'ban'])['model_scenario'].\\\n",
    "    apply(lambda x: ','.join(map(str, x))).reset_index()\n",
    "\n",
    "for label in d_target_mapping.keys():\n",
    "    df_train_grouped[label] = df_train_grouped.apply(\n",
    "        lambda row: 1 if label in row['model_scenario'] else 0, axis = 1\n",
    "    )\n",
    "\n",
    "df_train_grouped_final = pd.merge(df_train_grouped, df_train_processed_unique, on=['ban', 'lpds_id'], how='left')\n",
    "\n",
    "df_train_grouped_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dc8227-4279-4e6d-8cc1-f7f06b924b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation_grouped = df_validation_processed_ids.groupby(['lpds_id', 'ban'])['model_scenario'].\\\n",
    "    apply(lambda x: ','.join(map(str, x))).reset_index()\n",
    "\n",
    "for label in d_target_mapping.keys():\n",
    "    df_validation_grouped[label] = df_validation_grouped.apply(\n",
    "        lambda row: 1 if label in row['model_scenario'] else 0, axis = 1\n",
    "    )\n",
    "\n",
    "df_validation_grouped_final = pd.merge(df_validation_grouped, df_validation_processed_unique, on=['ban', 'lpds_id'], how='left')\n",
    "\n",
    "df_validation_grouped_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38565ee-7142-4566-af97-6647d3a05151",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_grouped = df_test_processed_ids.groupby(['lpds_id', 'ban'])['model_scenario'].\\\n",
    "    apply(lambda x: ','.join(map(str, x))).reset_index()\n",
    "\n",
    "for label in d_target_mapping.keys():\n",
    "    df_test_grouped[label] = df_test_grouped.apply(\n",
    "        lambda row: 1 if label in row['model_scenario'] else 0, axis = 1\n",
    "    )\n",
    "\n",
    "df_test_grouped_final = pd.merge(df_test_grouped, df_test_processed_unique, on=['ban', 'lpds_id'], how='left')\n",
    "\n",
    "df_test_grouped_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289bb24a-96d0-43f2-b00b-96e1857d1961",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_grouped_final.drop(['model_scenario_x', 'model_scenario_y', 'split_type', 'ref_dt', 'label', 'label_dt', 'target'], axis=1, inplace=True)\n",
    "df_validation_grouped_final.drop(['model_scenario_x', 'model_scenario_y', 'split_type', 'ref_dt', 'label', 'label_dt', 'target'], axis=1, inplace=True)\n",
    "df_test_grouped_final.drop(['model_scenario_x', 'model_scenario_y', 'split_type', 'ref_dt', 'label', 'label_dt', 'target'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216acca2-f237-49b0-b5a0-c2e974a7731b",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_label_cols = [col for col in df_train_grouped_final.columns if '_acquisition' in col] \n",
    "l_id_cols = ['cust_id', 'ban', 'lpds_id'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395c7fe1-43c7-4c2b-bce3-befb8793e47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train_grouped_final.drop(columns=l_label_cols + l_id_cols) \n",
    "y_train = df_train_grouped_final[l_label_cols] \n",
    "\n",
    "X_val = df_validation_grouped_final.drop(columns=l_label_cols + l_id_cols) \n",
    "y_val = df_validation_grouped_final[l_label_cols] \n",
    "\n",
    "X_test = df_test_grouped_final.drop(columns=l_label_cols + l_id_cols) \n",
    "y_test = df_test_grouped_final[l_label_cols] \n",
    "\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148c2016-ea0c-4ccb-854c-ee3b67b3ffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate normalization parameters on the training data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Apply normalization to the training data\n",
    "X_train_normalized = scaler.transform(X_train)\n",
    "\n",
    "# Apply normalization to the validation data using the same parameters\n",
    "X_val_normalized = scaler.transform(X_val)\n",
    "X_test_normalized = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4c78f8-8a66-46ba-a5ac-19acd5a341e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply random oversampling to balance the dataset\n",
    "# sampling = RandomOverSampler(random_state=42, sampling_strategy=sampling_strategy)\n",
    "# sampling = SMOTE(random_state=42)\n",
    "\n",
    "# X_train_resampled, y_train_resampled = sampling.fit_resample(X_train_normalized.astype('float'), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8c527d-366a-4fb3-930d-c4b12ea28f2d",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e876c5-df41-48b3-9351-e182cc78eb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the input data to include the sequence length dimension\n",
    "X_train_reshaped = np.reshape(X_train_normalized, (X_train_normalized.shape[0], X_train_normalized.shape[1], 1))\n",
    "\n",
    "# Define the input shape based on the reshaped data\n",
    "input_shape = X_train_reshaped.shape[1:]  # Shape excluding batch size\n",
    "\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09a130f-4b3a-4ced-9e46-181590c5d8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4339cd-0489-445c-8ae6-8bb8f86a3c16",
   "metadata": {},
   "source": [
    "### CNN: Current Capture Rate of ~78%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5327af4-818a-4697-9018-1cbe714cfd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.layers import Conv2D, Flatten\n",
    "\n",
    "# Reshape the input data to include the sequence length dimension\n",
    "X_train_reshaped = np.reshape(X_train_normalized, (X_train_normalized.shape[0], X_train_normalized.shape[1], 1))\n",
    "\n",
    "# Define the input shape based on the reshaped data\n",
    "input_shape = X_train_reshaped.shape[1:]  # Shape excluding batch size\n",
    "\n",
    "# Define your CNN model\n",
    "cnn_model = models.Sequential()\n",
    "\n",
    "# # Create a 1D CNN model\n",
    "cnn_model.add(layers.Conv1D(32, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "# cnn_model.add(layers.Conv1D(64, kernel_size=3, activation='relu'))\n",
    "# # cnn_model.add(layers.Conv1D(128, kernel_size=3, activation='relu'))\n",
    "\n",
    "# Add global max pooling layer\n",
    "cnn_model.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "# # Add dense layers\n",
    "# cnn_model.add(layers.Dense(64, activation='relu'))\n",
    "\n",
    "cnn_model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "# Compile the model\n",
    "cnn_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.TopKCategoricalAccuracy(k=3)])\n",
    "\n",
    "# Print the model summary\n",
    "cnn_model.summary()\n",
    "\n",
    "# Train the model\n",
    "cnn_model.fit(X_train_reshaped, y_train, epochs=20, batch_size=512, validation_split = 0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3746ac26-136c-45e1-ad4b-c7dd83cc24b6",
   "metadata": {},
   "source": [
    "### More Experiments with CNN 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151416f3-6c13-4386-a4c1-815132058156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, models\n",
    "# from tensorflow.keras.layers import Conv2D, Flatten\n",
    "\n",
    "# # Reshape the input data to include the sequence length dimension\n",
    "# X_train_reshaped = np.reshape(X_train_normalized, (X_train_normalized.shape[0], X_train_normalized.shape[1], 1))\n",
    "\n",
    "# # Define the input shape based on the reshaped data\n",
    "# input_shape = X_train_reshaped.shape[1:]  # Shape excluding batch size\n",
    "\n",
    "# # Define your CNN model\n",
    "# cnn_model = models.Sequential()\n",
    "\n",
    "# # Create a 1D CNN model\n",
    "# cnn_model.add(layers.Conv1D(32, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "# # cnn_model.add(layers.Conv1D(64, kernel_size=3, activation='relu'))\n",
    "# # cnn_model.add(layers.Conv1D(128, kernel_size=3, activation='relu'))\n",
    "\n",
    "# # # Add global max pooling layer\n",
    "# # cnn_model.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "# # # Add dense layers\n",
    "# # cnn_model.add(layers.Dense(64, activation='relu'))\n",
    "\n",
    "# cnn_model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "# # Compile the model\n",
    "# cnn_model.compile(optimizer='adam',\n",
    "#               loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "#               metrics=[tf.keras.metrics.TopKCategoricalAccuracy(k=3)])\n",
    "\n",
    "# # Print the model summary\n",
    "# cnn_model.summary()\n",
    "\n",
    "# # Train the model\n",
    "# cnn_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=512, validation_split = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0e3b46-2bd7-4964-941f-750c2b3124ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_values(arr, i):\n",
    "    \"\"\"\n",
    "    Convert values in a NumPy array.\n",
    "    \n",
    "    Args:\n",
    "    - arr: NumPy array\n",
    "    \n",
    "    Returns:\n",
    "    - Converted NumPy array\n",
    "    \"\"\"\n",
    "    # Make a copy of the input array to avoid modifying the original array\n",
    "    converted_arr = arr.copy()\n",
    "    \n",
    "    # Use boolean indexing to identify elements with value 1\n",
    "    indices = converted_arr == 1\n",
    "    \n",
    "    # Replace elements with value 1 with 2\n",
    "    converted_arr[indices] = i\n",
    "    \n",
    "    return converted_arr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e68cb17-c0fc-4007-b1ae-13194eef5d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_conv = y_val.copy()\n",
    "y_val_conv['sing_acquisition'] = convert_values(np.array(y_val['sing_acquisition']), 1)\n",
    "y_val_conv['shs_acquisition'] = convert_values(np.array(y_val['shs_acquisition']), 2)\n",
    "y_val_conv['tos_acquisition'] = convert_values(np.array(y_val['tos_acquisition']), 3)\n",
    "y_val_conv['wifi_acquisition'] = convert_values(np.array(y_val['wifi_acquisition']), 4)\n",
    "y_val_conv['ttv_acquisition'] = convert_values(np.array(y_val['ttv_acquisition']), 5)\n",
    "y_val_conv['sws_acquisition'] = convert_values(np.array(y_val['sws_acquisition']), 6)\n",
    "y_val_conv['hsic_acquisition'] = convert_values(np.array(y_val['hsic_acquisition']), 7)\n",
    "y_val_conv['lwc_acquisition'] = convert_values(np.array(y_val['lwc_acquisition']), 8)\n",
    "y_val_conv['hpro_acquisition'] = convert_values(np.array(y_val['hpro_acquisition']), 9)\n",
    "y_val_conv['whsia_acquisition'] = convert_values(np.array(y_val['whsia_acquisition']), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef86064-c609-4f0b-9da7-8eb647dd6e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_conv = y_test.copy()\n",
    "y_test_conv['sing_acquisition'] = convert_values(np.array(y_test['sing_acquisition']), 1)\n",
    "y_test_conv['shs_acquisition'] = convert_values(np.array(y_test['shs_acquisition']), 2)\n",
    "y_test_conv['tos_acquisition'] = convert_values(np.array(y_test['tos_acquisition']), 3)\n",
    "y_test_conv['wifi_acquisition'] = convert_values(np.array(y_test['wifi_acquisition']), 4)\n",
    "y_test_conv['ttv_acquisition'] = convert_values(np.array(y_test['ttv_acquisition']), 5)\n",
    "y_test_conv['sws_acquisition'] = convert_values(np.array(y_test['sws_acquisition']), 6)\n",
    "y_test_conv['hsic_acquisition'] = convert_values(np.array(y_test['hsic_acquisition']), 7)\n",
    "y_test_conv['lwc_acquisition'] = convert_values(np.array(y_test['lwc_acquisition']), 8)\n",
    "y_test_conv['hpro_acquisition'] = convert_values(np.array(y_test['hpro_acquisition']), 9)\n",
    "y_test_conv['whsia_acquisition'] = convert_values(np.array(y_test['whsia_acquisition']), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b9e54d-811a-4727-acc8-d95bea707c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model.predict(X_val_normalized)\n",
    "y_pred = cnn_model.predict(X_val_normalized)\n",
    "\n",
    "y_pred_0 = y_pred[:, 0]\n",
    "y_pred_1 = y_pred[:, 1]\n",
    "y_pred_2 = y_pred[:, 2]\n",
    "y_pred_3 = y_pred[:, 3]\n",
    "y_pred_4 = y_pred[:, 4]\n",
    "y_pred_5 = y_pred[:, 5]\n",
    "y_pred_6 = y_pred[:, 6]\n",
    "y_pred_7 = y_pred[:, 7]\n",
    "y_pred_8 = y_pred[:, 8]\n",
    "y_pred_9 = y_pred[:, 9]\n",
    "\n",
    "df_X_val_normalized = pd.DataFrame(X_val_normalized, columns=[f'col_{i}' for i in range(X_val_normalized.shape[1])])\n",
    "\n",
    "df_val_exp = pd.merge(df_X_val_normalized, y_val_conv, left_index=True, right_index=True, how='inner')\n",
    "# df_val_exp['y_val'] = y_val\n",
    "df_val_exp['y_pred_proba_0'] = y_pred_0\n",
    "df_val_exp['y_pred_proba_1'] = y_pred_1\n",
    "df_val_exp['y_pred_proba_2'] = y_pred_2\n",
    "df_val_exp['y_pred_proba_3'] = y_pred_3\n",
    "df_val_exp['y_pred_proba_4'] = y_pred_4\n",
    "df_val_exp['y_pred_proba_5'] = y_pred_5\n",
    "df_val_exp['y_pred_proba_6'] = y_pred_6\n",
    "df_val_exp['y_pred_proba_7'] = y_pred_7\n",
    "df_val_exp['y_pred_proba_8'] = y_pred_8\n",
    "df_val_exp['y_pred_proba_9'] = y_pred_9\n",
    "\n",
    "df_val_exp.to_csv(\"gs://divg-groovyhoon-pr-d2eab4-default/downloads/df_val_exp_dl.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03117630-55b3-44b9-b187-7a33f05f63b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model.predict(X_val_normalized)\n",
    "y_pred = cnn_model.predict(X_test_normalized)\n",
    "\n",
    "y_pred_0 = y_pred[:, 0]\n",
    "y_pred_1 = y_pred[:, 1]\n",
    "y_pred_2 = y_pred[:, 2]\n",
    "y_pred_3 = y_pred[:, 3]\n",
    "y_pred_4 = y_pred[:, 4]\n",
    "y_pred_5 = y_pred[:, 5]\n",
    "y_pred_6 = y_pred[:, 6]\n",
    "y_pred_7 = y_pred[:, 7]\n",
    "y_pred_8 = y_pred[:, 8]\n",
    "y_pred_9 = y_pred[:, 9]\n",
    "\n",
    "df_X_test_normalized = pd.DataFrame(X_test_normalized, columns=[f'col_{i}' for i in range(X_test_normalized.shape[1])])\n",
    "\n",
    "df_test_exp = pd.merge(df_X_test_normalized, y_test_conv, left_index=True, right_index=True, how='inner')\n",
    "# df_val_exp['y_val'] = y_val\n",
    "df_test_exp['y_pred_proba_0'] = y_pred_0\n",
    "df_test_exp['y_pred_proba_1'] = y_pred_1\n",
    "df_test_exp['y_pred_proba_2'] = y_pred_2\n",
    "df_test_exp['y_pred_proba_3'] = y_pred_3\n",
    "df_test_exp['y_pred_proba_4'] = y_pred_4\n",
    "df_test_exp['y_pred_proba_5'] = y_pred_5\n",
    "df_test_exp['y_pred_proba_6'] = y_pred_6\n",
    "df_test_exp['y_pred_proba_7'] = y_pred_7\n",
    "df_test_exp['y_pred_proba_8'] = y_pred_8\n",
    "df_test_exp['y_pred_proba_9'] = y_pred_9\n",
    "\n",
    "df_val_exp.to_csv(\"gs://divg-groovyhoon-pr-d2eab4-default/downloads/df_test_exp_dl.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad9e078-2ced-4ef2-a97c-5562c769a07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5694086-cf64-4aa2-be5a-5ddf4747fa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in (1, 2, 3):\n",
    "    probabilities =  model.predict(X_val_normalized)\n",
    "    results_ranked = np.argsort(-probabilities, axis=1)\n",
    "    display(extract_stats(n, results_ranked, y_val, d_target_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a698a0-d342-49af-9e96-8f52e5a1625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_val_normalized)\n",
    "\n",
    "y_pred_0 = y_pred[:, 0]\n",
    "y_pred_1 = y_pred[:, 1]\n",
    "y_pred_2 = y_pred[:, 2]\n",
    "y_pred_3 = y_pred[:, 3]\n",
    "y_pred_4 = y_pred[:, 4]\n",
    "y_pred_5 = y_pred[:, 5]\n",
    "y_pred_6 = y_pred[:, 6]\n",
    "y_pred_7 = y_pred[:, 7]\n",
    "y_pred_8 = y_pred[:, 8]\n",
    "y_pred_9 = y_pred[:, 9]\n",
    "\n",
    "df_X_val_normalized = pd.DataFrame(X_val_normalized, columns=[f'col_{i}' for i in range(X_val_normalized.shape[1])])\n",
    "\n",
    "df_val_exp = pd.merge(df_X_val_normalized, y_val, left_index=True, right_index=True, how='inner')\n",
    "# df_val_exp['y_val'] = y_val\n",
    "df_val_exp['y_pred_proba_0'] = y_pred_0\n",
    "df_val_exp['y_pred_proba_1'] = y_pred_1\n",
    "df_val_exp['y_pred_proba_2'] = y_pred_2\n",
    "df_val_exp['y_pred_proba_3'] = y_pred_3\n",
    "df_val_exp['y_pred_proba_4'] = y_pred_4\n",
    "df_val_exp['y_pred_proba_5'] = y_pred_5\n",
    "df_val_exp['y_pred_proba_6'] = y_pred_6\n",
    "df_val_exp['y_pred_proba_7'] = y_pred_7\n",
    "df_val_exp['y_pred_proba_8'] = y_pred_8\n",
    "df_val_exp['y_pred_proba_9'] = y_pred_9\n",
    "\n",
    "df_val_exp.to_csv(\"gs://divg-groovyhoon-pr-d2eab4-default/downloads/df_val_exp_dl.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a88803a-ca40-4470-bced-d4b9cd6ca12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_normalized)\n",
    "\n",
    "y_pred_0 = y_pred[:, 0]\n",
    "y_pred_1 = y_pred[:, 1]\n",
    "y_pred_2 = y_pred[:, 2]\n",
    "y_pred_3 = y_pred[:, 3]\n",
    "y_pred_4 = y_pred[:, 4]\n",
    "y_pred_5 = y_pred[:, 5]\n",
    "y_pred_6 = y_pred[:, 6]\n",
    "y_pred_7 = y_pred[:, 7]\n",
    "y_pred_8 = y_pred[:, 8]\n",
    "y_pred_9 = y_pred[:, 9]\n",
    "\n",
    "df_val_exp = pd.merge(X_test_normalized, y_test, left_index=True, right_index=True, how='inner')\n",
    "# df_val_exp['y_test'] = y_test\n",
    "df_val_exp['y_pred_proba_0'] = y_pred_0\n",
    "df_val_exp['y_pred_proba_1'] = y_pred_1\n",
    "df_val_exp['y_pred_proba_2'] = y_pred_2\n",
    "df_val_exp['y_pred_proba_3'] = y_pred_3\n",
    "df_val_exp['y_pred_proba_4'] = y_pred_4\n",
    "df_val_exp['y_pred_proba_5'] = y_pred_5\n",
    "df_val_exp['y_pred_proba_6'] = y_pred_6\n",
    "df_val_exp['y_pred_proba_7'] = y_pred_7\n",
    "df_val_exp['y_pred_proba_8'] = y_pred_8\n",
    "df_val_exp['y_pred_proba_9'] = y_pred_9\n",
    "\n",
    "df_val_exp.to_csv(\"gs://divg-groovyhoon-pr-d2eab4-default/downloads/y_test_exp_dl.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da443cf4-4ed6-4eca-8db1-9d62f6414b03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bb57ea-a4f5-4d6f-99e0-ecc918e19c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2ee210-34a3-4b11-afdd-7339b197de52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba8676c-7e42-40d3-af8d-5a3c0bf1a16f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d198335c-ce17-46d6-899b-85ed22dac93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n= 3\n",
    "probabilities =  model.predict(X_val_normalized)\n",
    "results_ranked = np.argsort(-probabilities, axis=1)\n",
    "display(extract_stats(n, results_ranked, y_val, d_target_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54c8ab4-6605-40f7-853b-bb4442b8c920",
   "metadata": {},
   "outputs": [],
   "source": [
    "n= 3\n",
    "probabilities =  model.predict(X_val_normalized)\n",
    "results_ranked = np.argsort(-probabilities, axis=1)\n",
    "display(extract_stats(n, results_ranked, y_val, d_target_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56317423-0616-4481-9d86-9c5abb325d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "n= 3\n",
    "probabilities =  model.predict(X_val_normalized)\n",
    "results_ranked = np.argsort(-probabilities, axis=1)\n",
    "display(extract_stats(n, results_ranked, y_val, d_target_mapping))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59a00872-c96a-44cf-9f81-fd7146049ae7",
   "metadata": {},
   "source": [
    "n= 3\n",
    "probabilities =  model.predict(X_val_normalized)\n",
    "results_ranked = np.argsort(-probabilities, axis=1)\n",
    "display(extract_stats(n, results_ranked, y_val, d_target_mapping))\n",
    "\n",
    "#### Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7325edc-554a-4555-ad9e-f8bd8c15a7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow scikeras scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea6f7ee-1902-4b28-96f6-612a2688a62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5b00dc-4013-4931-bab2-8a693bbfcc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_size, output_size, hidden_layer_dim, activation):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation=activation, input_dim=input_size))\n",
    "    model.add(Dense(hidden_layer_dim, activation=activation))\n",
    "    model.add(Dense(output_size, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126cad17-501a-4bdb-ad40-02c315cd7b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create KerasClassifier\n",
    "model = KerasClassifier(\n",
    "    create_model,\n",
    "    input_size=input_size,\n",
    "    output_size=output_size,\n",
    "    hidden_layer_dim=100,\n",
    "    activation='relu',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.SparseTopKCategoricalAccuracy(k=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f22fccf-3ca8-4400-933b-f34d0eb43a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import top_k_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181dd733-4c1f-444c-9585-22e7faf839a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters to tune\n",
    "param_grid = {\n",
    "    'hidden_layer_dim': [50, 100, 200, 300, 400, 500],\n",
    "    'optimizer': ['adam', 'sgd'],\n",
    "    'optimizer__learning_rate': [0.0001, 0.0005, 0.001],\n",
    "    'activation': ['relu', 'sigmoid'],\n",
    "    'batch_size': [128, 256, 512, 1024, 2048,4096]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model, \n",
    "    param_grid=param_grid, \n",
    "    cv=5, \n",
    "    scoring=make_scorer(top_k_accuracy_score, k=3, response_method='predict_proba')\n",
    "    #scoring=make_scorer(top_k_accuracy_score, k=3, labels=list(d_target_mapping.values()))\n",
    ")\n",
    "grid_search.fit(X_train_normalized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8200e85-c50f-418c-86d2-f7c2ec4e599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.best_score_, grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf88b66-0939-4aa9-a5cf-320a0bf058cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best parameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6984641a-960f-4365-82de-066badecb39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n= 3\n",
    "probabilities =  best_model.predict_proba(X_val_normalized)\n",
    "results_ranked = np.argsort(-probabilities, axis=1)\n",
    "display(extract_stats(n, results_ranked, y_val, d_target_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf541313-16b2-49c3-9c82-adbc3875a708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the best model\n",
    "best_model.fit(X_train_normalized, y_train, epochs=1000, validation_data=(X_test_normalized, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cf7806-c262-47b8-b29c-6fc137e35f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "n= 3\n",
    "probabilities =  best_model.predict_proba(X_val_normalized)\n",
    "results_ranked = np.argsort(-probabilities, axis=1)\n",
    "display(extract_stats(n, results_ranked, y_val, d_target_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d591c7a-917a-4b82-9348-1ffc7fe5d83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba5761b-16fa-460c-85af-bf60cbc2ff87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
