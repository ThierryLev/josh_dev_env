{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa00a55f-09ef-4149-9ae1-b456f3c9b9b6",
   "metadata": {},
   "source": [
    "### import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df86feee-d170-44a4-852b-be7f164619b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import google\n",
    "from google.oauth2 import credentials\n",
    "from google.oauth2 import service_account\n",
    "from google.oauth2.service_account import Credentials\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# first neural network with keras tutorial\n",
    "from numpy import loadtxt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "SERVICE_TYPE = 'nba_product_reco_model'\n",
    "DATASET_ID = 'nba_product_reco_model'\n",
    "PROJECT_ID = 'divg-groovyhoon-pr-d2eab4' #mapping['PROJECT_ID']\n",
    "RESOURCE_BUCKET = 'divg-groovyhoon-pr-d2eab4-default' #mapping['resources_bucket']\n",
    "FILE_BUCKET = 'divg-groovyhoon-pr-d2eab4-default' #mapping['gcs_csv_bucket']\n",
    "REGION = 'northamerica-northeast1' #mapping['REGION']\n",
    "MODEL_ID = '9999'\n",
    "FOLDER_NAME = 'nba_product_reco_model'.format(MODEL_ID)\n",
    "QUERIES_PATH = 'vertex_pipelines/' + FOLDER_NAME + '/queries/'\n",
    "TRAIN_TABLE_ID = 'nba_training_dataset_v7'\n",
    "VAL_TABLE_ID = 'nba_val_dataset_v7'\n",
    "TEST_TABLE_ID = 'nba_test_dataset_v7'\n",
    "SCORE_TABLE_ID = 'bq_product_reco_scores'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f467673d-7cb4-419f-8877-bf4933eea8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c17315-66af-435e-8ca1-c08aa2958bb8",
   "metadata": {},
   "source": [
    "### import bq to dataframe function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8898f797-be01-48b1-96be-53ba92e5f86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import credentials\n",
    "\n",
    "def import_bq_to_dataframe(project_id, dataset_id, table_id, client): \n",
    "    \n",
    "    \"\"\"\n",
    "    Imports a specific table from BigQuery to a DataFrame. \n",
    "    \n",
    "    Args: \n",
    "        project_id: The name of the project_id where the table is located.\n",
    "        dataset_id: The name of the dataset_id where the table is located.\n",
    "        table_id: The name of the table_id you wish to import to DataFrame.\n",
    "        client: A BigQuery client instance. e.g. client = bigquery.Client(project=project_id).\n",
    "\n",
    "    Returns: \n",
    "        A DataFrame\n",
    "        \n",
    "    Example: \n",
    "        import_bq_to_dataframe('bi-stg-divg-speech-pr-9d940b', 'call_to_retention_dataset', 'bq_ctr_pipeline_dataset')\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    sql = f\"SELECT * FROM `{project_id}.{dataset_id}.{table_id}`\"\n",
    "    \n",
    "    df_return = client.query(sql).to_dataframe()\n",
    "\n",
    "    return df_return\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47df6af4-9162-466b-b503-54536b3399bb",
   "metadata": {},
   "source": [
    "### define get_lift function, import df_train and df_test from gcs bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a984377-27a4-4499-a644-263228241232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "project_id = PROJECT_ID\n",
    "region = REGION\n",
    "resource_bucket = RESOURCE_BUCKET\n",
    "file_bucket = FILE_BUCKET\n",
    "service_type=SERVICE_TYPE\n",
    "project_id=PROJECT_ID\n",
    "dataset_id=DATASET_ID\n",
    "train_table_id = TRAIN_TABLE_ID\n",
    "val_table_id = VAL_TABLE_ID\n",
    "test_table_id = TEST_TABLE_ID\n",
    "\n",
    "def get_lift(prob, y_test, q):\n",
    "    result = pd.DataFrame(columns=['Prob', 'Churn'])\n",
    "    result['Prob'] = prob\n",
    "    result['Churn'] = y_test\n",
    "    # result['Decile'] = pd.qcut(1-result['Prob'], 10, labels = False)\n",
    "    result['Decile'] = pd.qcut(result['Prob'], q, labels=[i for i in range(q, 0, -1)])\n",
    "    add = pd.DataFrame(result.groupby('Decile')['Churn'].mean()).reset_index()\n",
    "    add.columns = ['Decile', 'avg_real_churn_rate']\n",
    "    result = result.merge(add, on='Decile', how='left')\n",
    "    result.sort_values('Decile', ascending=True, inplace=True)\n",
    "    lg = pd.DataFrame(result.groupby('Decile')['Prob'].mean()).reset_index()\n",
    "    lg.columns = ['Decile', 'avg_model_pred_churn_rate']\n",
    "    lg.sort_values('Decile', ascending=False, inplace=True)\n",
    "    lg['avg_churn_rate_total'] = result['Churn'].mean()\n",
    "    lg = lg.merge(add, on='Decile', how='left')\n",
    "    lg['lift'] = lg['avg_real_churn_rate'] / lg['avg_churn_rate_total']\n",
    "\n",
    "    return lg\n",
    "\n",
    "def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "    token = os.popen('gcloud auth print-access-token').read()\n",
    "    token = re.sub(f'\\n$', '', token)\n",
    "    credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "    bq_client = bigquery.Client(project=project_id)\n",
    "    if use_local_credential:\n",
    "        bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "    return bq_client\n",
    "\n",
    "client = get_gcp_bqclient(project_id)\n",
    "\n",
    "df_train = import_bq_to_dataframe(project_id, dataset_id, train_table_id, client) #-- Jan to Oct \n",
    "df_val = import_bq_to_dataframe(project_id, dataset_id, val_table_id, client) #-- Nov and Dec\n",
    "df_test = import_bq_to_dataframe(project_id, dataset_id, test_table_id, client) #-- Nov and Dec\n",
    "\n",
    "scenario_to_target = {\n",
    "    'hsic_acquisition': 0,\n",
    "    'ttv_acquisition': 1,\n",
    "    'sing_acquisition': 2,\n",
    "    'shs_acquisition': 3, \n",
    "    'tos_acquisition': 4, \n",
    "    'wifi_acquisition': 5, \n",
    "    'lwc_acquisition': 6, \n",
    "    'sws_acquisition': 7, \n",
    "    'hpro_acquisition': 8, \n",
    "    'whsia_acquisition': 9, \n",
    "    'ttv_upsell': 10, \n",
    "    'shs_renewal': 11, \n",
    "    'shs_upsell': 12\n",
    "    #'mobility_acquisition': 8,\n",
    "    #'tos_upsell': 8\n",
    "}\n",
    "\n",
    "df_train['target'] = df_train['model_scenario'].map(scenario_to_target)\n",
    "df_val['target'] = df_val['model_scenario'].map(scenario_to_target)\n",
    "df_test['target'] = df_test['model_scenario'].map(scenario_to_target)\n",
    "\n",
    "print(f'df_train: {df_train.shape}')\n",
    "print(f'df_val: {df_val.shape}')\n",
    "print(f'df_test: {df_test.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78521b60-956f-4494-bb4b-d09883960064",
   "metadata": {},
   "source": [
    "### fillna text cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a18b480-daf2-4595-83af-ef5a84687d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['demogr_lifestage_sort'].fillna(6, inplace=True)\n",
    "df_train['cust_pref_lang_txt'].fillna('NOT AVAILABLE', inplace=True)\n",
    "df_train['demogr_census_division_typ'].fillna('NA', inplace=True)\n",
    "df_train['cust_prov_state_cd'].fillna('N/AVAIL', inplace=True)\n",
    "df_train['cust_cr_val_txt'].fillna('NOT AVAILABLE', inplace=True)\n",
    "df_train['acct_ebill_ind'].fillna('N', inplace=True)\n",
    "\n",
    "df_val['demogr_lifestage_sort'].fillna(6, inplace=True)\n",
    "df_val['cust_pref_lang_txt'].fillna('NOT AVAILABLE', inplace=True)\n",
    "df_val['demogr_census_division_typ'].fillna('NA', inplace=True)\n",
    "df_val['cust_prov_state_cd'].fillna('N/AVAIL', inplace=True)\n",
    "df_val['cust_cr_val_txt'].fillna('NOT AVAILABLE', inplace=True)\n",
    "df_val['acct_ebill_ind'].fillna('N', inplace=True)\n",
    "\n",
    "df_test['demogr_lifestage_sort'].fillna(6, inplace=True)\n",
    "df_test['cust_pref_lang_txt'].fillna('NOT AVAILABLE', inplace=True)\n",
    "df_test['demogr_census_division_typ'].fillna('NA', inplace=True)\n",
    "df_test['cust_prov_state_cd'].fillna('N/AVAIL', inplace=True)\n",
    "df_test['cust_cr_val_txt'].fillna('NOT AVAILABLE', inplace=True)\n",
    "df_test['acct_ebill_ind'].fillna('N', inplace=True)\n",
    "\n",
    "print(f'df_train: {df_train.shape}')\n",
    "print(f'df_val: {df_val.shape}')\n",
    "print(f'df_test: {df_val.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eac73d9-f15d-4dd4-944c-a6935b54cbd1",
   "metadata": {},
   "source": [
    "### define train, val, test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48ef48b-feef-48b1-8d45-384c94b862da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "# df train - Jan to Aug \n",
    "# df train - Sep to Oct \n",
    "df_train, df_test = train_test_split(df_train, test_size=0.3, random_state=42, stratify=df_train['target'])\n",
    "\n",
    "# df_train.to_csv('gs://{}/{}/{}_train_final.csv'.format(FILE_BUCKET, SERVICE_TYPE, SERVICE_TYPE), index=False)\n",
    "# df_test.to_csv('gs://{}/{}/{}_test_final.csv'.format(FILE_BUCKET, SERVICE_TYPE, SERVICE_TYPE), index=False)\n",
    "# df_val.to_csv('gs://{}/{}/{}_val_final.csv'.format(FILE_BUCKET, SERVICE_TYPE, SERVICE_TYPE), index=False)\n",
    "\n",
    "#set up features (list)\n",
    "cols_1 = df_train.columns.values\n",
    "cols_2 = df_val.columns.values\n",
    "cols_3 = df_test.columns.values\n",
    "\n",
    "cols = set(cols_1).intersection(set(cols_2))\n",
    "cols = set(cols).intersection(set(cols_3))\n",
    "\n",
    "features_to_exclude = ['split_type','model_scenario','ref_dt','cust_id','cust_src_id','ban','ban_src_id','lpds_id',\n",
    "                       'fms_address_id','label','label_dt', 'prod_latest_actvn_dt', 'prod_latest_deactvn_dt', 'target', \n",
    "                       'contract_end_date_hsic', 'contract_end_date_hsic', 'contract_end_date_sing', 'contract_end_date_ttv', 'contract_end_date_smhm'] \n",
    "\n",
    "features = [f for f in cols if f not in features_to_exclude]\n",
    "\n",
    "ban_train = df_train[['ban', 'lpds_id']]\n",
    "X_train = df_train[features]\n",
    "y_train = np.squeeze(df_train['target'].values)\n",
    "target_train = df_train['target']\n",
    "\n",
    "ban_val = df_val[['ban', 'lpds_id']]\n",
    "X_val = df_val[features]\n",
    "y_val = np.squeeze(df_val['target'].values)\n",
    "target_val = df_val['target']\n",
    "\n",
    "ban_test = df_test[['ban', 'lpds_id']]\n",
    "X_test = df_test[features]\n",
    "y_test = np.squeeze(df_test['target'].values)\n",
    "target_test = df_test['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30e3a7b-46fd-41ca-b2ed-2774e29754b6",
   "metadata": {},
   "source": [
    "### label encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f8682d-f5da-4013-93df-f2820c673efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to transform the features of the feature store.\n",
    "def encode_categorical_features(train_df, test_df, val_df):\n",
    "    # Get a list of all categorical columns\n",
    "    cat_columns = train_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    # Encode each categorical column\n",
    "    for col in cat_columns:\n",
    "        le = LabelEncoder()\n",
    "        train_df[col] = le.fit_transform(train_df[col])\n",
    "        test_df[col] = le.fit_transform(test_df[col])\n",
    "        val_df[col] = le.fit_transform(val_df[col])\n",
    "        \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "#excluding the customer ID so it doesn't get encoded\n",
    "train_label_data=X_train[X_train.columns.difference(['cust_id','ban','lpds_id','ref_dt','model_scenario'])]\n",
    "val_label_data=X_val[X_val.columns.difference(['cust_id','ban','lpds_id','ref_dt','model_scenario'])]\n",
    "test_label_data=X_test[X_test.columns.difference(['cust_id','ban','lpds_id','ref_dt','model_scenario'])]\n",
    "\n",
    "X_train, X_val, X_test = encode_categorical_features(train_label_data, val_label_data, test_label_data)\n",
    "\n",
    "#set up features (list)\n",
    "cols_1 = X_train.columns.values\n",
    "cols_2 = X_val.columns.values\n",
    "cols_3 = X_test.columns.values\n",
    "\n",
    "cols = set(cols_1).intersection(set(cols_2))\n",
    "cols = set(cols).intersection(set(cols_3))\n",
    "\n",
    "features = [f for f in cols if f not in features_to_exclude]\n",
    "\n",
    "X_train = X_train[features] \n",
    "X_val = X_val[features]\n",
    "X_test = X_test[features] \n",
    "\n",
    "X_train_array = X_train.values\n",
    "X_val_array = X_val.values\n",
    "X_test_array = X_test.values\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f352863e-9d3e-4757-8cdf-e6175183721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define baseline model\n",
    "# def baseline_model():\n",
    "#     # create model\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(8, input_dim=176, activation='relu'))\n",
    "#     model.add(Dense(3, activation='relu'))\n",
    "#     model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "#     # Compile model\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)\n",
    "\n",
    "# kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "# results = cross_val_score(estimator, X_train, y_train, cv=kfold)\n",
    "\n",
    "# print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a06702-a2ec-4af8-8c8c-fd28763abde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(hidden_layer_dim, meta):\n",
    "    # note that meta is a special argument that will be\n",
    "    # handed a dict containing input metadata\n",
    "    n_features_in_ = meta[\"n_features_in_\"]\n",
    "    X_shape_ = meta[\"X_shape_\"]\n",
    "    n_classes_ = meta[\"n_classes_\"]\n",
    "\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(n_features_in_, input_shape=X_shape_[1:]))\n",
    "    model.add(keras.layers.Activation(\"relu\"))\n",
    "    model.add(keras.layers.Dense(hidden_layer_dim))\n",
    "    model.add(keras.layers.Activation(\"relu\"))\n",
    "    model.add(keras.layers.Dense(n_classes_))\n",
    "    model.add(keras.layers.Activation(\"softmax\"))\n",
    "    return model\n",
    "\n",
    "clf = KerasClassifier(\n",
    "    get_model,\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    hidden_layer_dim=100,\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_tarin)\n",
    "y_proba = clf.predict_proba(X_val)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m116",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m116"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
