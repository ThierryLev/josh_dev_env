{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b0c603b-3f78-40d0-bc02-7dac3ea2a76c",
   "metadata": {},
   "source": [
    "### Import Libraries, declare variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf87789-5a0e-488a-a4bf-7969f104635e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "CREATE OR REPLACE TABLE `divg-groovyhoon-pr-d2eab4.nba_product_reco_model.nba_training_dataset` AS \n",
    "\n",
    "SELECT split_type\n",
    ", model_scenario\n",
    ", ref_dt\n",
    ", cust_id\n",
    ", cust_src_id\n",
    ", ban\n",
    ", ban_src_id\n",
    ", lpds_id\n",
    ", fms_address_id\n",
    ", label\n",
    ", label_dt\n",
    ", avg_usg_cnt_4w_news\n",
    ", avg_usg_dl_4w_news\n",
    ", avg_usg_ul_4w_news\n",
    ", avg_usg_cnt_4w_sports\n",
    ", avg_usg_dl_4w_sports\n",
    ", avg_usg_ul_4w_sports\n",
    ", `avg_usg_cnt_4w_tv and movies` AS avg_usg_cnt_4w_tv_and_movies\n",
    ", `avg_usg_dl_4w_tv and movies` AS avg_usg_dl_4w_tv_and_movies\n",
    ", `avg_usg_ul_4w_tv and movies` AS avg_usg_ul_4w_tv_and_movies\n",
    ", bill_wln_zscore_ban_subtotal_amt\n",
    ", CAST(bill_wln_avg_ban_subtotal_amt AS FLOAT64) AS bill_wln_avg_ban_subtotal_amt\n",
    ", bill_wln_zscore_ban_debit_amt\n",
    ", CAST(bill_wln_avg_ban_debit_amt AS FLOAT64) AS bill_wln_avg_ban_debit_amt\n",
    ", bill_wln_zscore_ban_discount_amt\n",
    ", CAST(bill_wln_avg_ban_discount_amt AS FLOAT64) AS bill_wln_avg_ban_discount_amt\n",
    ", bill_wln_zscore_ban_credit_amt\n",
    ", CAST(bill_wln_avg_ban_credit_amt AS FLOAT64) AS bill_wln_avg_ban_credit_amt\n",
    ", CAST(bill_wln_avg_sing_debit_amt AS FLOAT64) AS bill_wln_avg_sing_debit_amt\n",
    ", CAST(bill_wln_avg_sing_discount_amt AS FLOAT64) AS bill_wln_avg_sing_discount_amt\n",
    ", CAST(bill_wln_avg_sing_credit_amt AS FLOAT64) AS bill_wln_avg_sing_credit_amt\n",
    ", CAST(bill_wln_avg_hsic_debit_amt AS FLOAT64) AS bill_wln_avg_hsic_debit_amt\n",
    ", CAST(bill_wln_avg_hsic_discount_amt AS FLOAT64) AS bill_wln_avg_hsic_discount_amt\n",
    ", CAST(bill_wln_avg_hsic_credit_amt AS FLOAT64) AS bill_wln_avg_hsic_credit_amt\n",
    ", CAST(bill_wln_avg_ttv_debit_amt AS FLOAT64) AS bill_wln_avg_ttv_debit_amt\n",
    ", CAST(bill_wln_avg_ttv_discount_amt AS FLOAT64) AS bill_wln_avg_ttv_discount_amt\n",
    ", CAST(bill_wln_avg_ttv_credit_amt AS FLOAT64) AS bill_wln_avg_ttv_credit_amt\n",
    ", CAST(bill_wln_avg_smhm_debit_amt AS FLOAT64) AS bill_wln_avg_smhm_debit_amt\n",
    ", CAST(bill_wln_avg_smhm_discount_amt AS FLOAT64) AS bill_wln_avg_smhm_discount_amt\n",
    ", CAST(bill_wln_avg_smhm_credit_amt AS FLOAT64) AS bill_wln_avg_smhm_credit_amt\n",
    ", CAST(bill_wln_avg_sing_ld_na_call_cnt AS FLOAT64) AS bill_wln_avg_sing_ld_na_call_cnt\n",
    ", CAST(bill_wln_avg_sing_ld_intl_call_cnt AS FLOAT64) AS bill_wln_avg_sing_ld_intl_call_cnt\n",
    ", CAST(bill_wln_avg_hsic_usg_gb AS FLOAT64) AS bill_wln_avg_hsic_usg_gb\n",
    ", CAST(bill_wln_avg_ttv_ppv_cnt AS FLOAT64) AS bill_wln_avg_ttv_ppv_cnt\n",
    ", CAST(bill_wln_avg_ttv_vod_cnt AS FLOAT64) AS bill_wln_avg_ttv_vod_cnt\n",
    ", clk_wls_tot_cnt_r30d\n",
    ", clk_wls_plan_cnt_r30d\n",
    ", clk_wls_device_cnt_r30d\n",
    ", clk_wls_smartwatch_cnt_r30d\n",
    ", clk_wls_tablet_cnt_r30d\n",
    ", clk_wln_tot_cnt_r30d\n",
    ", clk_wln_eligibility_cnt_r30d\n",
    ", clk_wln_sing_cnt_r30d\n",
    ", clk_wln_hsic_cnt_r30d\n",
    ", clk_wln_fibre_cnt_r30d\n",
    ", clk_wln_whsia_cnt_r30d\n",
    ", clk_wln_wifi_plus_cnt_r30d\n",
    ", clk_wln_tv_cnt_r30d\n",
    ", clk_wln_optik_cnt_r30d\n",
    ", clk_wln_pik_cnt_r30d\n",
    ", clk_wln_streamplus_cnt_r30d\n",
    ", clk_wln_streaming_cnt_r30d\n",
    ", clk_wln_security_cnt_r30d\n",
    ", clk_wln_smarthome_security_cnt_r30d\n",
    ", clk_wln_online_security_cnt_r30d\n",
    ", clk_wln_smartwear_security_cnt_r30d\n",
    ", clk_deal_tot_cnt_r30d\n",
    ", clk_deal_wls_cnt_r30d\n",
    ", clk_deal_wln_cnt_r30d\n",
    ", clk_deal_wln_sing_cnt_r30d\n",
    ", clk_deal_wln_hsic_cnt_r30d\n",
    ", clk_deal_wln_whsia_cnt_r30d\n",
    ", clk_deal_wln_tv_cnt_r30d\n",
    ", clk_deal_wln_security_cnt_r30d\n",
    ", clk_upgr_tot_cnt_r30d\n",
    ", clk_upgr_wls_cnt_r30d\n",
    ", clk_upgr_wln_cnt_r30d\n",
    ", clk_upgr_wln_sing_cnt_r30d\n",
    ", clk_upgr_wln_hsic_cnt_r30d\n",
    ", clk_upgr_wln_whsia_cnt_r30d\n",
    ", clk_upgr_wln_tv_cnt_r30d\n",
    ", clk_upgr_wln_security_cnt_r30d\n",
    ", clk_chg_tot_cnt_r30d\n",
    ", clk_chg_wls_cnt_r30d\n",
    ", clk_chg_wln_cnt_r30d\n",
    ", clk_chg_wln_sing_cnt_r30d\n",
    ", clk_chg_wln_hsic_cnt_r30d\n",
    ", clk_chg_wln_whsia_cnt_r30d\n",
    ", clk_chg_wln_tv_cnt_r30d\n",
    ", clk_chg_wln_security_cnt_r30d\n",
    ", clk_health_livingwell_cnt_r30d\n",
    ", clk_health_mypet_cnt_r30d\n",
    ", clk_travel_cnt_r30d\n",
    ", clk_billing_cnt_r30d\n",
    ", clk_service_agreement_cnt_r30d\n",
    ", acct_cr_risk_txt\n",
    ", acct_ebill_ind\n",
    ", cust_cr_val_txt\n",
    ", cust_pref_lang_txt\n",
    ", cust_prov_state_cd\n",
    ", cust_age_yr_num\n",
    ", demogr_med_age\n",
    ", demogr_avg_child\n",
    ", demogr_pct_family_with_child_living_at_home\n",
    ", demogr_employment_rate\n",
    ", demogr_avg_household_size\n",
    ", demogr_avg_income\n",
    ", demogr_med_income\n",
    ", demogr_urban_flag\n",
    ", demogr_rural_flag\n",
    ", demogr_family_flag\n",
    ", demogr_lsname_large_diverse_families\n",
    ", demogr_lsname_younger_singles_and_couples\n",
    ", demogr_lsname_very_young_singles_and_couples\n",
    ", demogr_lsname_older_families_and_empty_nests\n",
    ", demogr_lsname_middle_age_families\n",
    ", demogr_lsname_mature_singles_and_couples\n",
    ", demogr_lsname_young_families\n",
    ", demogr_lsname_school_age_families\n",
    ", demogr_retired_pstl_cd_ind\n",
    ", demogr_census_division_typ\n",
    ", demogr_lifestage_sort\n",
    ", CAST(hs_usg_avg_tot_gb AS FLOAT64) AS hs_usg_avg_tot_gb\n",
    ", CAST(hs_usg_avg_dl_gb AS FLOAT64) AS hs_usg_avg_dl_gb\n",
    ", CAST(hs_usg_avg_ul_gb AS FLOAT64) AS hs_usg_avg_ul_gb\n",
    ", prod_latest_actvn_dt\n",
    ", prod_latest_deactvn_dt\n",
    ", prod_tot_cnt\n",
    ", prod_wln_cnt\n",
    ", prod_wls_cnt\n",
    ", prod_mob_cnt\n",
    ", prod_sing_cnt\n",
    ", prod_hsic_cnt\n",
    ", prod_whsia_cnt\n",
    ", prod_ttv_cnt\n",
    ", prod_smhm_cnt\n",
    ", prod_tos_cnt\n",
    ", prod_wifiplus_cnt\n",
    ", prod_stv_cnt\n",
    ", prod_other_cnt\n",
    ", prod_deact_prod_cnt\n",
    ", prod_act_prod_cnt_r7d\n",
    ", prod_act_wln_cnt_r7d\n",
    ", prod_deact_prod_cnt_r7d\n",
    ", prod_deact_wln_cnt_r7d\n",
    ", hsic_tenure_days\n",
    ", contract_end_date_hsic\n",
    ", sing_tenure_days\n",
    ", contract_end_date_sing\n",
    ", ttv_tenure_days\n",
    ", contract_end_date_ttv\n",
    ", smhm_tenure_days\n",
    ", contract_end_date_smhm\n",
    ", ffh_tenure\n",
    ", new_account_ind\n",
    "\n",
    "FROM `divg-groovyhoon-pr-d2eab4.nba_product_reco_model.nba_training_dataset` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1294f3f-ee1c-4c57-bbb1-635ef08b52d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import google\n",
    "from google.oauth2 import credentials\n",
    "from google.oauth2 import service_account\n",
    "from google.oauth2.service_account import Credentials\n",
    "import datetime\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# build model\n",
    "import xgboost as xgb\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, manhattan_distances\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "\n",
    "SERVICE_TYPE = 'nba_product_reco_model'\n",
    "DATASET_ID = 'nba_product_reco_model'\n",
    "PROJECT_ID = 'divg-groovyhoon-pr-d2eab4' #mapping['PROJECT_ID']\n",
    "RESOURCE_BUCKET = 'divg-groovyhoon-pr-d2eab4-default' #mapping['resources_bucket']\n",
    "FILE_BUCKET = 'divg-groovyhoon-pr-d2eab4-default' #mapping['gcs_csv_bucket']\n",
    "REGION = 'northamerica-northeast1' #mapping['REGION']\n",
    "MODEL_ID = '9999'\n",
    "FOLDER_NAME = 'nba_product_reco_model'.format(MODEL_ID)\n",
    "QUERIES_PATH = 'vertex_pipelines/' + FOLDER_NAME + '/queries/'\n",
    "TRAIN_TABLE_ID = 'nba_training_dataset_v5'\n",
    "VAL_TABLE_ID = 'nba_test_dataset_v5'\n",
    "SCORE_TABLE_ID = 'bq_product_reco_scores'\n",
    "\n",
    "scoringDate = date(2023, 10, 13)  # date.today() - relativedelta(days=2)- relativedelta(months=30)\n",
    "valScoringDate = date(2023, 11, 13)  # scoringDate - relativedelta(days=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb30300f-ff71-417d-956f-6d3dcede6c29",
   "metadata": {},
   "source": [
    "### import bq to dataframe function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e098413-7211-4346-b1c9-eafd38ef076d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import credentials\n",
    "\n",
    "def import_bq_to_dataframe(project_id, dataset_id, table_id, client): \n",
    "    \n",
    "    \"\"\"\n",
    "    Imports a specific table from BigQuery to a DataFrame. \n",
    "    \n",
    "    Args: \n",
    "        project_id: The name of the project_id where the table is located.\n",
    "        dataset_id: The name of the dataset_id where the table is located.\n",
    "        table_id: The name of the table_id you wish to import to DataFrame.\n",
    "        client: A BigQuery client instance. e.g. client = bigquery.Client(project=project_id).\n",
    "\n",
    "    Returns: \n",
    "        A DataFrame\n",
    "        \n",
    "    Example: \n",
    "        import_bq_to_dataframe('bi-stg-divg-speech-pr-9d940b', 'call_to_retention_dataset', 'bq_ctr_pipeline_dataset')\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    sql = f\"SELECT * FROM `{project_id}.{dataset_id}.{table_id}`\"\n",
    "    \n",
    "    df_return = client.query(sql).to_dataframe()\n",
    "\n",
    "    return df_return\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a31e45-47f9-445e-aab4-1e2d8243e1f9",
   "metadata": {},
   "source": [
    "### define get_lift function, import df_train and df_test from gcs bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3837b0-0180-4a8a-978d-b56402062efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "project_id = PROJECT_ID\n",
    "region = REGION\n",
    "resource_bucket = RESOURCE_BUCKET\n",
    "file_bucket = FILE_BUCKET\n",
    "service_type=SERVICE_TYPE\n",
    "project_id=PROJECT_ID\n",
    "dataset_id=DATASET_ID\n",
    "train_table_id = TRAIN_TABLE_ID\n",
    "val_table_id = VAL_TABLE_ID\n",
    "\n",
    "def get_lift(prob, y_test, q):\n",
    "    result = pd.DataFrame(columns=['Prob', 'Churn'])\n",
    "    result['Prob'] = prob\n",
    "    result['Churn'] = y_test\n",
    "    # result['Decile'] = pd.qcut(1-result['Prob'], 10, labels = False)\n",
    "    result['Decile'] = pd.qcut(result['Prob'], q, labels=[i for i in range(q, 0, -1)])\n",
    "    add = pd.DataFrame(result.groupby('Decile')['Churn'].mean()).reset_index()\n",
    "    add.columns = ['Decile', 'avg_real_churn_rate']\n",
    "    result = result.merge(add, on='Decile', how='left')\n",
    "    result.sort_values('Decile', ascending=True, inplace=True)\n",
    "    lg = pd.DataFrame(result.groupby('Decile')['Prob'].mean()).reset_index()\n",
    "    lg.columns = ['Decile', 'avg_model_pred_churn_rate']\n",
    "    lg.sort_values('Decile', ascending=False, inplace=True)\n",
    "    lg['avg_churn_rate_total'] = result['Churn'].mean()\n",
    "    lg = lg.merge(add, on='Decile', how='left')\n",
    "    lg['lift'] = lg['avg_real_churn_rate'] / lg['avg_churn_rate_total']\n",
    "\n",
    "    return lg\n",
    "\n",
    "def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "    token = os.popen('gcloud auth print-access-token').read()\n",
    "    token = re.sub(f'\\n$', '', token)\n",
    "    credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "    bq_client = bigquery.Client(project=project_id)\n",
    "    if use_local_credential:\n",
    "        bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "    return bq_client\n",
    "\n",
    "client = get_gcp_bqclient(project_id)\n",
    "\n",
    "df_train = import_bq_to_dataframe(project_id, dataset_id, train_table_id, client)\n",
    "df_val = import_bq_to_dataframe(project_id, dataset_id, val_table_id, client)\n",
    "\n",
    "scenario_to_target = {\n",
    "    'hsic_acquisition': 0,\n",
    "    'ttv_acquisition': 1,\n",
    "    'sing_acquisition': 2,\n",
    "    'shs_acquisition': 3, \n",
    "    'tos_acquisition': 4, \n",
    "    'wifi_acquisition': 5, \n",
    "    'lwc_acquisition': 6, \n",
    "    'sws_acquisition': 7, \n",
    "    'hpro_acquisition': 8, \n",
    "    'whsia_acquisition': 9\n",
    "}\n",
    "\n",
    "df_train['target'] = df_train['model_scenario'].map(scenario_to_target)\n",
    "df_val['target'] = df_val['model_scenario'].map(scenario_to_target)\n",
    "\n",
    "df_train_preserve = df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e270188-696f-4b14-8f4a-6463bf1552f4",
   "metadata": {},
   "source": [
    "### Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214d3a22-8203-4a36-be64-8e2f5f5054ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['demogr_lifestage_sort'].fillna(6, inplace=True)\n",
    "df_train['cust_pref_lang_txt'].fillna('NOT AVAILABLE', inplace=True)\n",
    "df_train['demogr_census_division_typ'].fillna('NA', inplace=True)\n",
    "df_train['cust_prov_state_cd'].fillna('N/AVAIL', inplace=True)\n",
    "df_train['cust_cr_val_txt'].fillna('NOT AVAILABLE', inplace=True)\n",
    "df_train['acct_ebill_ind'].fillna('N', inplace=True)\n",
    "\n",
    "df_val['demogr_lifestage_sort'].fillna(6, inplace=True)\n",
    "df_val['cust_pref_lang_txt'].fillna('NOT AVAILABLE', inplace=True)\n",
    "df_val['demogr_census_division_typ'].fillna('NA', inplace=True)\n",
    "df_val['cust_prov_state_cd'].fillna('N/AVAIL', inplace=True)\n",
    "df_val['cust_cr_val_txt'].fillna('NOT AVAILABLE', inplace=True)\n",
    "df_val['acct_ebill_ind'].fillna('N', inplace=True)\n",
    "\n",
    "print(f'df_train: {df_train.shape}')\n",
    "print(f'df_val: {df_val.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94961cc0-7e92-44eb-9e1e-15d16d2024ed",
   "metadata": {},
   "source": [
    "### define train, test, and val dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc272d97-768b-401b-9100-923cc63cf1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_impute = ['demogr_avg_child','demogr_avg_household_size','demogr_avg_income','demogr_employment_rate'\n",
    "                        ,'demogr_med_age','demogr_med_income','demogr_pct_family_with_child_living_at_home']\n",
    "\n",
    "def impute_with_average(df, cols_to_impute):\n",
    "    \n",
    "    for col in cols_to_impute:\n",
    "        column_mean = df[col].mean()\n",
    "        df[col].fillna(column_mean, inplace=True)\n",
    "        \n",
    "    return df\n",
    "\n",
    "df_train = impute_with_average(df_train, cols_to_impute)\n",
    "df_val = impute_with_average(df_val, cols_to_impute)\n",
    "\n",
    "#set up features (list)\n",
    "cols_1 = df_train.columns.values\n",
    "cols_3 = df_val.columns.values\n",
    "\n",
    "cols = set(cols_1).intersection(set(cols_3))\n",
    "\n",
    "id_variables = ['split_type', 'model_scenario', 'ref_dt', 'cust_id', 'cust_src_id', 'ban', 'ban_src_id', 'lpds_id', 'fms_address_id', 'label', 'label_dt', 'target']\n",
    "\n",
    "features_to_include = ['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id', 'target', \n",
    "'prod_tos_cnt', 'prod_hsic_cnt', 'prod_ttv_cnt', 'prod_sing_cnt', 'cust_prov_state_cd', 'prod_smhm_cnt', 'sing_tenure_days', 'prod_stv_cnt', 'demogr_census_division_typ', \n",
    "'clk_wln_wifi_plus_cnt_r30d', 'hsic_tenure_days', 'prod_whsia_cnt', 'bill_wln_avg_hsic_debit_amt', 'bill_wln_avg_ttv_debit_amt', 'ttv_tenure_days', 'bill_wln_avg_smhm_debit_amt', 'smhm_tenure_days', \n",
    "'clk_wln_sing_cnt_r30d', 'hs_usg_avg_ul_gb', 'clk_health_livingwell_cnt_r30d', 'hs_usg_avg_dl_gb', 'prod_other_cnt', 'hs_usg_avg_tot_gb', 'bill_wln_avg_ban_discount_amt', 'clk_wln_hsic_cnt_r30d', \n",
    "'acct_ebill_ind', 'clk_wln_smarthome_security_cnt_r30d', 'prod_wifiplus_cnt', 'demogr_avg_household_size', 'clk_wln_optik_cnt_r30d', 'clk_wln_security_cnt_r30d', 'demogr_lifestage_sort', 'demogr_family_flag', \n",
    "'bill_wln_avg_sing_debit_amt', 'ffh_tenure']\n",
    "\n",
    "feature_weights = np.array([0.213879816752624, 0.210757495491595, 0.155501726318607, 0.0602533075942611, 0.0523079604892021, 0.0284744355928214, 0.0270177329993315, 0.0230154595375712, 0.0170414919779867, \n",
    "0.0155022847023418, 0.0146792811382145, 0.0142657660878289, 0.0133395133806645, 0.0132004812421698, 0.0118557752793018, 0.0107220666341007, 0.0101260045930694, 0.00916686983616255, 0.00906401454998682, \n",
    "0.00764893682885285, 0.00758425900125304, 0.00754070816218072, 0.00732201897328859, 0.00718525467726317, 0.00656270026430169, 0.00655088122633769, 0.00584749485663925, 0.00488302137330835, 0.00484122456343937, \n",
    "0.00462082166402429, 0.00433701614023153, 0.00390846588739282, 0.00374843937884009, 0.00366447156176259, 0.003582801])\n",
    "\n",
    "features = [f for f in cols if f in features_to_include]\n",
    "\n",
    "train_id = df_train[id_variables]\n",
    "ban_train = df_train[['ban', 'lpds_id']]\n",
    "X_train = df_train[features]\n",
    "y_train = np.squeeze(df_train['target'].values)\n",
    "y_train = y_train.astype(int)\n",
    "target_train = df_train['target']\n",
    "\n",
    "val_id = df_val[id_variables]\n",
    "ban_val = df_val[['ban', 'lpds_id']]\n",
    "X_val = df_val[features]\n",
    "y_val = np.squeeze(df_val['target'].values)\n",
    "y_val = y_val.astype(int)\n",
    "target_val = df_val['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11027df1-f59a-47e9-9db7-32325ac3b1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to transform the features of the feature store.\n",
    "def encode_categorical_features(train_df, val_df):\n",
    "    # Get a list of all categorical columns\n",
    "    cat_columns = train_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    # Encode each categorical column\n",
    "    for col in cat_columns:\n",
    "        le = LabelEncoder()\n",
    "        train_df[col] = le.fit_transform(train_df[col])\n",
    "        val_df[col] = le.fit_transform(val_df[col])\n",
    "        \n",
    "    return train_df, val_df\n",
    "\n",
    "#excluding the customer ID so it doesn't get encoded\n",
    "train_label_data=X_train[X_train.columns.difference(['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id', 'target'])] \n",
    "val_label_data=X_val[X_val.columns.difference(['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id', 'target'])]\n",
    "\n",
    "X_train_feat_enc, X_val_feat_enc = encode_categorical_features(train_label_data, val_label_data)\n",
    "\n",
    "X_train_feat_enc.fillna(0, inplace=True)\n",
    "X_val_feat_enc.fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9127c70c-3dbc-46d8-ab9e-611dafd840da",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5239ce34-132c-48d4-bdf5-535235ffce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def perform_pca(X, n): \n",
    "    \n",
    "#     from sklearn.decomposition import PCA\n",
    "\n",
    "#     pca = PCA(n_components = n) \n",
    "    \n",
    "#     X_reduced = pca.fit_transform(X)\n",
    "    \n",
    "#     cols = [f'PC{i}' for i in range(n)]\n",
    "    \n",
    "#     df_reduced = pd.DataFrame(data=X_reduced, columns=cols) \n",
    "    \n",
    "#     return df_reduced\n",
    "\n",
    "# X_train_feat_enc_pca = perform_pca(X_train_feat_enc, 20)\n",
    "# X_val_feat_enc_pca = perform_pca(X_val_feat_enc, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983c97b2-a70a-47dd-8062-5e2cb8f0dd57",
   "metadata": {},
   "source": [
    "### Re-attach ID Variables to the Features Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23d0e68-8787-47ee-95d9-d7c083ea1565",
   "metadata": {},
   "outputs": [],
   "source": [
    "##bringing back the customer ids keys\n",
    "X_train_feat_enc['cust_id'] = X_train['cust_id']\n",
    "X_train_feat_enc['ban'] = X_train['ban']\n",
    "X_train_feat_enc['lpds_id'] = X_train['lpds_id']\n",
    "X_train_feat_enc['ref_dt'] = X_train['ref_dt'] \n",
    "X_train_feat_enc['model_scenario'] = X_train['model_scenario']\n",
    "X_train_feat_enc['target'] = y_train\n",
    "\n",
    "X_val_feat_enc['cust_id'] = X_val['cust_id'] \n",
    "X_val_feat_enc['ban'] = X_val['ban']\n",
    "X_val_feat_enc['lpds_id'] = X_val['lpds_id'] \n",
    "X_val_feat_enc['ref_dt'] = X_val['ref_dt']\n",
    "X_val_feat_enc['model_scenario'] = X_val['model_scenario']\n",
    "X_val_feat_enc['target'] = y_val\n",
    "\n",
    "train = pd.merge(X_train_feat_enc, train_id[['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id']],how = 'inner',on=['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id'])\n",
    "val = pd.merge(X_val_feat_enc, val_id[['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id']],how = 'inner',on=['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfb6909-f145-4183-aec8-c525aa16dddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preserve = train\n",
    "\n",
    "# train = train[(train['ref_dt'] >= date(2023, 9, 1)) & (train['ref_dt'] <= date(2023, 10, 1))]\n",
    "# print(train.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdfbb80-9392-49b4-93a8-48c2cf553832",
   "metadata": {},
   "source": [
    "### Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e62dc6-b503-41bb-b125-e5880e4d29ab",
   "metadata": {},
   "source": [
    "### latest function to use for experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7013e2a4-5ba2-47c4-ac07-244f4f6dd2c0",
   "metadata": {},
   "source": [
    "### get_recommended_offers_group \n",
    "\n",
    "This function shares the same objective of get_recommended_offers function, but it operates at scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2f035d-64ff-448f-8a9f-1b6334cfb72b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = train\n",
    "prod_mix_cols = ['prod_hsic_cnt','prod_ttv_cnt','prod_stv_cnt','prod_sing_cnt','prod_smhm_cnt','prod_tos_cnt','prod_wifiplus_cnt','prod_whsia_cnt']\n",
    "id_cols = ['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id', 'target']\n",
    "distance_func = 'cosine'\n",
    "n = 100\n",
    "minimal_threshold = 0.10\n",
    "max_offers_to_return = 3\n",
    "\n",
    "features = ['prod_tos_cnt', \n",
    "'prod_hsic_cnt', \n",
    "'prod_ttv_cnt', \n",
    "'prod_sing_cnt', \n",
    "'cust_prov_state_cd', \n",
    "'prod_smhm_cnt', \n",
    "'sing_tenure_days', \n",
    "'prod_stv_cnt', \n",
    "'demogr_census_division_typ', \n",
    "'clk_wln_wifi_plus_cnt_r30d', \n",
    "'hsic_tenure_days', \n",
    "'prod_whsia_cnt', \n",
    "'bill_wln_avg_hsic_debit_amt', \n",
    "'bill_wln_avg_ttv_debit_amt', \n",
    "'ttv_tenure_days', \n",
    "'bill_wln_avg_smhm_debit_amt', \n",
    "'smhm_tenure_days', \n",
    "'clk_wln_sing_cnt_r30d', \n",
    "'hs_usg_avg_ul_gb', \n",
    "'clk_health_livingwell_cnt_r30d']\n",
    "\n",
    "feature_weights = [0.213879816752624, \n",
    "0.210757495491595, \n",
    "0.155501726318607, \n",
    "0.0602533075942611, \n",
    "0.0523079604892021, \n",
    "0.0284744355928214, \n",
    "0.0270177329993315, \n",
    "0.0230154595375712, \n",
    "0.0170414919779867, \n",
    "0.0155022847023418, \n",
    "0.0146792811382145, \n",
    "0.0142657660878289, \n",
    "0.0133395133806645, \n",
    "0.0132004812421698, \n",
    "0.0118557752793018, \n",
    "0.0107220666341007, \n",
    "0.0101260045930694, \n",
    "0.00916686983616255, \n",
    "0.00906401454998682, \n",
    "0.00764893682885285]\n",
    "\n",
    "# features = ['prod_tos_cnt', 'prod_hsic_cnt', 'prod_ttv_cnt', 'prod_sing_cnt', 'cust_prov_state_cd', 'prod_smhm_cnt', 'sing_tenure_days', 'prod_stv_cnt', 'demogr_census_division_typ', \n",
    "# 'clk_wln_wifi_plus_cnt_r30d', 'hsic_tenure_days', 'prod_whsia_cnt', 'bill_wln_avg_hsic_debit_amt', 'bill_wln_avg_ttv_debit_amt', 'ttv_tenure_days', 'bill_wln_avg_smhm_debit_amt', 'smhm_tenure_days', \n",
    "# 'clk_wln_sing_cnt_r30d', 'hs_usg_avg_ul_gb', 'clk_health_livingwell_cnt_r30d', 'hs_usg_avg_dl_gb', 'prod_other_cnt', 'hs_usg_avg_tot_gb', 'bill_wln_avg_ban_discount_amt', 'clk_wln_hsic_cnt_r30d', \n",
    "# 'acct_ebill_ind', 'clk_wln_smarthome_security_cnt_r30d', 'prod_wifiplus_cnt', 'demogr_avg_household_size', 'clk_wln_optik_cnt_r30d', 'clk_wln_security_cnt_r30d', 'demogr_lifestage_sort', 'demogr_family_flag', \n",
    "# 'bill_wln_avg_sing_debit_amt', 'ffh_tenure']\n",
    "\n",
    "# feature_weights = np.array([0.213879816752624, 0.210757495491595, 0.155501726318607, 0.0602533075942611, 0.0523079604892021, 0.0284744355928214, 0.0270177329993315, 0.0230154595375712, 0.0170414919779867, \n",
    "# 0.0155022847023418, 0.0146792811382145, 0.0142657660878289, 0.0133395133806645, 0.0132004812421698, 0.0118557752793018, 0.0107220666341007, 0.0101260045930694, 0.00916686983616255, 0.00906401454998682, \n",
    "# 0.00764893682885285, 0.00758425900125304, 0.00754070816218072, 0.00732201897328859, 0.00718525467726317, 0.00656270026430169, 0.00655088122633769, 0.00584749485663925, 0.00488302137330835, 0.00484122456343937, \n",
    "# 0.00462082166402429, 0.00433701614023153, 0.00390846588739282, 0.00374843937884009, 0.00366447156176259, 0.003582801])\n",
    "\n",
    "def final_prod_reco(prod_mix, prod_reco_list): \n",
    "    \n",
    "    # This function accepts the customer's product mix and product recommenndations as input parameters, \n",
    "    # and filters out the product recommendations that the customer already has.\n",
    "    \n",
    "    # 1. check the customer's product intensity\n",
    "    cust_prod_list = []\n",
    "\n",
    "    if prod_mix[0] > 0: \n",
    "        cust_prod_list.append('hsic')\n",
    "    if prod_mix[1] + prod_mix[2] > 0: \n",
    "        cust_prod_list.append('ttv')\n",
    "    if prod_mix[3] > 0: \n",
    "        cust_prod_list.append('sing')\n",
    "    if prod_mix[4] > 0: \n",
    "        cust_prod_list.append('shs')\n",
    "    if prod_mix[5] > 0: \n",
    "        cust_prod_list.append('tos')\n",
    "    if prod_mix[6] > 0: \n",
    "        cust_prod_list.append('wifi')\n",
    "    if prod_mix[7] > 0: \n",
    "        cust_prod_list.append('whsia')\n",
    "    ### add for lwc, sws, hrpo as product count labels become available ###\n",
    "    # if prod_mix[n] > 0: \n",
    "    #     cust_prod_list.append('lwc')\n",
    "    # if prod_mix[n] > 0: \n",
    "    #     cust_prod_list.append('sws')\n",
    "    # if prod_mix[n] > 0: \n",
    "    #     cust_prod_list.append('hpro')\n",
    "\n",
    "    # 2. trim the prodduct reco list to remove \"_acquisition\" for comparison. \n",
    "    prod_reco_list = [value.replace('_acquisition', '') for value in prod_reco_list]\n",
    "\n",
    "    final_prod_reco_list = [item for item in prod_reco_list if item not in cust_prod_list]\n",
    "\n",
    "    return final_prod_reco_list    \n",
    "\n",
    "def get_recommended_offers_group(train,\n",
    "                                 val, \n",
    "                                 prod_mix_cols, \n",
    "                                 id_cols,\n",
    "                                 distance_func, \n",
    "                                 features, \n",
    "                                 feature_weights, \n",
    "                                 n, \n",
    "                                 minimal_threshold,\n",
    "                                 max_offers_to_return \n",
    "                                ): \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    df = train\n",
    "\n",
    "    df_X = df[features] * feature_weights\n",
    "    X = df_X.values \n",
    "\n",
    "    df_x = val[features] * feature_weights\n",
    "    x = df_x.values\n",
    "\n",
    "    simsMatrix = cosine_similarity(x, X) \n",
    "\n",
    "    # compute the distances between the feature vectors\n",
    "    if distance_func == 'euclidean':\n",
    "        simsMatrix = euclidean_distances(x, X)\n",
    "    elif distance_func == 'manhattan':\n",
    "        simsMatrix = manhattan_distances(x, X)\n",
    "    elif distance_func == 'cosine':\n",
    "        simsMatrix = cosine_similarity(x, X)\n",
    "    else:\n",
    "        raise ValueError('Invalid distance function specified.')\n",
    "\n",
    "    # Create an empty list to store the indices of top 200 most similar customers for each customer in the test set\n",
    "    top_indices = []\n",
    "    \n",
    "    # Iterate through each row (vector) of the simsMatrix\n",
    "    for row in simsMatrix:\n",
    "        # Use np.argsort() to get the indices that would sort the row in ascending order\n",
    "        sorted_indices = np.argsort(row) ## sorted array of simiarlity score from least similar to most similar\n",
    "        \n",
    "        # compute the distances between the feature vectors\n",
    "        if distance_func == 'euclidean':\n",
    "            top_200_indices = sorted_indices[:n]\n",
    "        elif distance_func == 'manhattan':\n",
    "            top_200_indices = sorted_indices[:n]\n",
    "        elif distance_func == 'cosine':\n",
    "            top_200_indices =sorted_indices[-n:]\n",
    "        else:\n",
    "            raise ValueError('Invalid distance function specified.')        # Get the indices of the top 200 most similar customers\n",
    "        \n",
    "        # Add the top 200 indices to the list\n",
    "        top_indices.append(top_200_indices)\n",
    "        \n",
    "    result = []\n",
    "\n",
    "    i = 0 \n",
    "\n",
    "    for idx in top_indices: \n",
    "\n",
    "        # extract the customer data for the most similar customers\n",
    "        similar_customers = df.iloc[idx]\n",
    "        \n",
    "        # count the top offers of the similar customers\n",
    "        top_offers = similar_customers[['cust_id', 'ban', 'lpds_id', 'model_scenario', 'target']].groupby(['model_scenario', 'target'])['ban'].count().reset_index(name='ban_count').sort_values(by='ban_count', ascending=False)\n",
    "        top_offers['perc_total'] = top_offers['ban_count']/top_offers['ban_count'].sum()\n",
    "\n",
    "        # generate product recommendations list based on collaborative filtering\n",
    "        prod_mix = val[prod_mix_cols].iloc[i].values\n",
    "        prod_reco_list = list(top_offers['model_scenario'][:])\n",
    "\n",
    "        # filter out product recos that the customer already has\n",
    "        final_prod_reco_list = final_prod_reco(prod_mix, prod_reco_list)\n",
    "\n",
    "        result.append(final_prod_reco_list)\n",
    "        \n",
    "        i += 1\n",
    "        if i % 100 == 0: \n",
    "            print(f'{i}th customers processed') \n",
    "            print(f'prod_mix: {prod_mix}') \n",
    "            print(f'prod_reco_list: {prod_reco_list}')\n",
    "            print(f'final_prod_reco_list: {final_prod_reco_list}')\n",
    "        \n",
    "    return result\n",
    "    \n",
    "# result = get_recommended_offers_group(train,\n",
    "#                              val,\n",
    "#                              prod_mix_cols,\n",
    "#                              id_cols,\n",
    "#                              distance_func,\n",
    "#                              features,\n",
    "#                              feature_weights,\n",
    "#                              n,\n",
    "#                              minimal_threshold,\n",
    "#                              max_offers_to_return\n",
    "#                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e73e57a-ebcf-4836-9e16-a31a68ba84c9",
   "metadata": {},
   "source": [
    "### postprocess function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab92e3e-2e49-48cb-951c-b81f7c97ddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(result, df_train): \n",
    "    \n",
    "    import pandas as pd \n",
    "    import numpy as np \n",
    "\n",
    "    result = pd.merge(result, df_train[['ban', 'ref_dt', 'target']].drop_duplicates(subset=['ban', 'ref_dt'], keep='first'), how = 'left', on=['ban', 'ref_dt'])\n",
    "\n",
    "    product_mapping = {\n",
    "    'hsic': 0,\n",
    "    'ttv': 1,\n",
    "    'sing': 2,\n",
    "    'shs': 3, \n",
    "    'tos': 4, \n",
    "    'wifi': 5, \n",
    "    'lwc': 6, \n",
    "    'sws': 7, \n",
    "    'hpro': 8, \n",
    "    'whsia': 9\n",
    "    }\n",
    "\n",
    "    # Define a function to extract elements from the list\n",
    "    def extract_offer_values(offer_list, position):\n",
    "        if len(offer_list) >= position:\n",
    "            return offer_list[position - 1]  # Subtract 1 because list indexing starts from 0\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Create new columns for each element of the list\n",
    "    for i in range(1, 4):\n",
    "        result[f'reco{i}'] = result['offers'].apply(lambda x: extract_offer_values(x, i))\n",
    "\n",
    "    result['reco_1'] = result['reco1'].map(product_mapping)\n",
    "    result['reco_2'] = result['reco2'].map(product_mapping)\n",
    "    result['reco_3'] = result['reco3'].map(product_mapping)\n",
    "\n",
    "    result['accuracy1'] = np.where(result['reco_1'] == result['target'], 1, 0)\n",
    "    result['accuracy2'] = np.where(result['reco_2'] == result['target'], 1, 0)\n",
    "    result['accuracy3'] = np.where(result['reco_3'] == result['target'], 1, 0)\n",
    "\n",
    "    result['accuracy_1'] = result.apply(lambda row: 1 if sum(row[['accuracy1']]) > 0 else 0, axis=1)\n",
    "    result['accuracy_2'] =  result.apply(lambda row: 1 if sum(row[['accuracy1', 'accuracy2']]) > 0 else 0, axis=1)\n",
    "    result['accuracy_3'] =  result.apply(lambda row: 1 if sum(row[['accuracy1', 'accuracy2', 'accuracy3']]) > 0 else 0, axis=1)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af36c27-d97a-46a7-9a21-15e101b3dc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_recos(df_result): \n",
    "\n",
    "    reco_1_accuracy = df_result['accuracy1'].sum() / df_result['accuracy1'].count()\n",
    "    reco_2_accuracy = df_result['accuracy2'].sum() / df_result['accuracy2'].count()\n",
    "    reco_3_accuracy = df_result['accuracy3'].sum() / df_result['accuracy3'].count()\n",
    "\n",
    "    reco_1_capture_rate = df_result['accuracy_1'].sum() / df_result['accuracy_1'].count()\n",
    "    reco_12_capture_rate = df_result['accuracy_2'].sum() / df_result['accuracy_1'].count()\n",
    "    reco_123_capture_rate = df_result['accuracy_3'].sum() / df_result['accuracy_1'].count()\n",
    "\n",
    "    print(f'reco 1 accuracy: {round(reco_1_accuracy*100, 1)}%')\n",
    "    print(f'reco 2 accuracy: {round(reco_2_accuracy*100, 1)}%')\n",
    "    print(f'reco 3 accuracy: {round(reco_3_accuracy*100, 1)}%')\n",
    "\n",
    "    print(f'reco 1 capture rate: {round(reco_1_capture_rate*100, 1)}%')\n",
    "    print(f'reco 1 & 2 capture rate: {round(reco_12_capture_rate*100, 1)}%')\n",
    "    print(f'reco 1 & 2 & 3 capture rate: {round(reco_123_capture_rate*100, 1)}%')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb27a1cd-f385-4428-9cef-233fbe538341",
   "metadata": {},
   "source": [
    "### 3. apply the function to the entire scoring set (without bootstrapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2891b7b5-355f-4b45-9fb4-f9e45c83e5f6",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = train\n",
    "# prod_mix_cols: internet, tv, tv, sing, smhm, tos, wifi, whsia\n",
    "prod_mix_cols = ['prod_hsic_cnt','prod_ttv_cnt','prod_stv_cnt','prod_sing_cnt','prod_smhm_cnt','prod_tos_cnt','prod_wifiplus_cnt','prod_whsia_cnt']\n",
    "id_cols=['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id', 'target']\n",
    "distance_func = 'euclidean'\n",
    "n = 100\n",
    "minimal_threshold = 0.10\n",
    "max_offers_to_return = 3\n",
    "\n",
    "# features = ['prod_tos_cnt', \n",
    "# 'prod_hsic_cnt', \n",
    "# 'prod_ttv_cnt', \n",
    "# 'prod_sing_cnt', \n",
    "# 'cust_prov_state_cd', \n",
    "# 'prod_smhm_cnt', \n",
    "# 'sing_tenure_days', \n",
    "# 'prod_stv_cnt', \n",
    "# 'demogr_census_division_typ', \n",
    "# 'clk_wln_wifi_plus_cnt_r30d', \n",
    "# 'hsic_tenure_days', \n",
    "# 'prod_whsia_cnt', \n",
    "# 'bill_wln_avg_hsic_debit_amt', \n",
    "# 'bill_wln_avg_ttv_debit_amt', \n",
    "# 'ttv_tenure_days', \n",
    "# 'bill_wln_avg_smhm_debit_amt', \n",
    "# 'smhm_tenure_days', \n",
    "# 'clk_wln_sing_cnt_r30d', \n",
    "# 'hs_usg_avg_ul_gb', \n",
    "# 'clk_health_livingwell_cnt_r30d']\n",
    "\n",
    "# feature_weights = [0.213879816752624, \n",
    "# 0.210757495491595, \n",
    "# 0.155501726318607, \n",
    "# 0.0602533075942611, \n",
    "# 0.0523079604892021, \n",
    "# 0.0284744355928214, \n",
    "# 0.0270177329993315, \n",
    "# 0.0230154595375712, \n",
    "# 0.0170414919779867, \n",
    "# 0.0155022847023418, \n",
    "# 0.0146792811382145, \n",
    "# 0.0142657660878289, \n",
    "# 0.0133395133806645, \n",
    "# 0.0132004812421698, \n",
    "# 0.0118557752793018, \n",
    "# 0.0107220666341007, \n",
    "# 0.0101260045930694, \n",
    "# 0.00916686983616255, \n",
    "# 0.00906401454998682, \n",
    "# 0.00764893682885285]\n",
    "\n",
    "features = ['prod_tos_cnt', 'prod_hsic_cnt', 'prod_ttv_cnt', 'prod_sing_cnt', 'cust_prov_state_cd', 'prod_smhm_cnt', 'sing_tenure_days', 'prod_stv_cnt', 'demogr_census_division_typ', \n",
    "'clk_wln_wifi_plus_cnt_r30d', 'hsic_tenure_days', 'prod_whsia_cnt', 'bill_wln_avg_hsic_debit_amt', 'bill_wln_avg_ttv_debit_amt', 'ttv_tenure_days', 'bill_wln_avg_smhm_debit_amt', 'smhm_tenure_days', \n",
    "'clk_wln_sing_cnt_r30d', 'hs_usg_avg_ul_gb', 'clk_health_livingwell_cnt_r30d', 'hs_usg_avg_dl_gb', 'prod_other_cnt', 'hs_usg_avg_tot_gb', 'bill_wln_avg_ban_discount_amt', 'clk_wln_hsic_cnt_r30d', \n",
    "'acct_ebill_ind', 'clk_wln_smarthome_security_cnt_r30d', 'prod_wifiplus_cnt', 'demogr_avg_household_size', 'clk_wln_optik_cnt_r30d', 'clk_wln_security_cnt_r30d', 'demogr_lifestage_sort', 'demogr_family_flag', \n",
    "'bill_wln_avg_sing_debit_amt', 'ffh_tenure']\n",
    "\n",
    "feature_weights = np.array([0.213879816752624, 0.210757495491595, 0.155501726318607, 0.0602533075942611, 0.0523079604892021, 0.0284744355928214, 0.0270177329993315, 0.0230154595375712, 0.0170414919779867, \n",
    "0.0155022847023418, 0.0146792811382145, 0.0142657660878289, 0.0133395133806645, 0.0132004812421698, 0.0118557752793018, 0.0107220666341007, 0.0101260045930694, 0.00916686983616255, 0.00906401454998682, \n",
    "0.00764893682885285, 0.00758425900125304, 0.00754070816218072, 0.00732201897328859, 0.00718525467726317, 0.00656270026430169, 0.00655088122633769, 0.00584749485663925, 0.00488302137330835, 0.00484122456343937, \n",
    "0.00462082166402429, 0.00433701614023153, 0.00390846588739282, 0.00374843937884009, 0.00366447156176259, 0.003582801])\n",
    "\n",
    "def production_model (train, test, prod_mix_cols, id_cols, distance_func, features, feature_weights, n, minimal_threshold, max_offers_to_return):\n",
    "\n",
    "    frame = pd.DataFrame()\n",
    "    \n",
    "    # Assuming you have your DataFrame 'df' already in memory\n",
    "    chunk_size = test.shape[0] / 10  # Define the size of each chunk\n",
    "    num_chunks = 10  # Calculate the number of chunks\n",
    "    \n",
    "    print(chunk_size) \n",
    "    print(num_chunks)\n",
    "\n",
    "    # Split the DataFrame into chunks\n",
    "    chunks = np.array_split(test, num_chunks)\n",
    "\n",
    "    # Iterate through the chunks\n",
    "    for chunk in chunks:\n",
    "        \n",
    "        # Process each chunk\n",
    "        result = get_recommended_offers_group(train,\n",
    "                                             chunk,\n",
    "                                             prod_mix_cols,\n",
    "                                             id_cols,\n",
    "                                             distance_func,\n",
    "                                             features,\n",
    "                                             feature_weights,\n",
    "                                             n,\n",
    "                                             minimal_threshold,\n",
    "                                             max_offers_to_return\n",
    "                                            )\n",
    "        \n",
    "        print(chunk.shape) \n",
    "        print(len(result))\n",
    "\n",
    "        frame1 = pd.DataFrame({'ban': chunk['ban'], 'ref_dt': chunk['ref_dt'], 'offers': result})\n",
    "        frame = frame.append(frame1)\n",
    "        \n",
    "        print(f'frame size: {len(frame)}')\n",
    "        \n",
    "    return frame\n",
    "\n",
    "save_data_path = 'gs://divg-groovyhoon-pr-d2eab4-default/downloads/offer_recommendation_100_similar_customers.csv'\n",
    "test_samples = val.sample(n=5000, random_state=2153)\n",
    "result = production_model(train, test_samples, prod_mix_cols, id_cols, distance_func, features, feature_weights, n, minimal_threshold, max_offers_to_return)\n",
    "result.to_csv(save_data_path,index=False)\n",
    "print(f'csv saved in {save_data_path}')\n",
    "### 100 similar customers x 2000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff113e39-784a-4ca6-9f9f-b1e127422627",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = postprocess(result, df_val)\n",
    "evaluate_recos(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69b90f0-ef36-4275-a0d6-65aef8e20729",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = train\n",
    "prod_mix_cols = ['prod_hsic_cnt','prod_ttv_cnt','prod_stv_cnt','prod_sing_cnt','prod_smhm_cnt','prod_tos_cnt','prod_wifiplus_cnt','prod_whsia_cnt']\n",
    "id_cols=['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id', 'target']\n",
    "distance_func = 'cosine'\n",
    "n = 100\n",
    "minimal_threshold = 0.10\n",
    "max_offers_to_return = 3\n",
    "\n",
    "features = ['prod_tos_cnt', 'prod_hsic_cnt', 'prod_ttv_cnt', 'prod_sing_cnt', 'cust_prov_state_cd', 'prod_smhm_cnt', 'sing_tenure_days', 'prod_stv_cnt', 'demogr_census_division_typ', \n",
    "'clk_wln_wifi_plus_cnt_r30d', 'hsic_tenure_days', 'prod_whsia_cnt', 'bill_wln_avg_hsic_debit_amt', 'bill_wln_avg_ttv_debit_amt', 'ttv_tenure_days', 'bill_wln_avg_smhm_debit_amt', 'smhm_tenure_days', \n",
    "'clk_wln_sing_cnt_r30d', 'hs_usg_avg_ul_gb', 'clk_health_livingwell_cnt_r30d', 'hs_usg_avg_dl_gb', 'prod_other_cnt', 'hs_usg_avg_tot_gb', 'bill_wln_avg_ban_discount_amt', 'clk_wln_hsic_cnt_r30d', \n",
    "'acct_ebill_ind', 'clk_wln_smarthome_security_cnt_r30d', 'prod_wifiplus_cnt', 'demogr_avg_household_size', 'clk_wln_optik_cnt_r30d', 'clk_wln_security_cnt_r30d', 'demogr_lifestage_sort', 'demogr_family_flag', \n",
    "'bill_wln_avg_sing_debit_amt', 'ffh_tenure']\n",
    "\n",
    "feature_weights = np.array([0.213879816752624, 0.210757495491595, 0.155501726318607, 0.0602533075942611, 0.0523079604892021, 0.0284744355928214, 0.0270177329993315, 0.0230154595375712, 0.0170414919779867, \n",
    "0.0155022847023418, 0.0146792811382145, 0.0142657660878289, 0.0133395133806645, 0.0132004812421698, 0.0118557752793018, 0.0107220666341007, 0.0101260045930694, 0.00916686983616255, 0.00906401454998682, \n",
    "0.00764893682885285, 0.00758425900125304, 0.00754070816218072, 0.00732201897328859, 0.00718525467726317, 0.00656270026430169, 0.00655088122633769, 0.00584749485663925, 0.00488302137330835, 0.00484122456343937, \n",
    "0.00462082166402429, 0.00433701614023153, 0.00390846588739282, 0.00374843937884009, 0.00366447156176259, 0.003582801])\n",
    "\n",
    "def production_model (df, train, test, id_cols, prod_mix_cols, distance_func, n, minimal_threshold, max_offers_to_return):\n",
    "    \n",
    "    frame = pd.DataFrame()\n",
    "    \n",
    "    train_backup = train\n",
    "    test_backup = test\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    # For each customer in each month\n",
    "    for cust in list(df['ban'].unique()):\n",
    "        i += 1\n",
    "        if i % 1 == 0: \n",
    "            print(f'{i} customers processed')\n",
    "                        \n",
    "        train = train_backup\n",
    "        test = test_backup\n",
    "        \n",
    "        for dt in list(df[df['ban']==cust]['ref_dt'].unique()):\n",
    "            #This part of the code adds the line we want to get offers to the training set, so we can use the distance formula\n",
    "            data = pd.DataFrame()\n",
    "            \n",
    "            add_test = test[(test['ban']==cust)&(test['ref_dt']==dt)]\n",
    "            \n",
    "            if 'target' in add_test.columns: \n",
    "                add_test.drop(columns=['target'], inplace=True)\n",
    "            \n",
    "            add_test['target'] = 0\n",
    "                        \n",
    "            data = train.append(add_test)\n",
    "            data= data.reset_index()\n",
    "\n",
    "            results = get_recommended_offers(data, id_cols, prod_mix_cols, features, feature_weights, cust, dt, distance_func, n, minimal_threshold=minimal_threshold, max_offers_to_return=max_offers_to_return)\n",
    "            \n",
    "            data = {'ban': [cust],\n",
    "                  'ref_dt': [dt],\n",
    "                  'offers': [results]}\n",
    "            \n",
    "            frame1 =  pd.DataFrame(data)\n",
    "            frame = frame.append(frame1)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "save_data_path = 'gs://divg-groovyhoon-pr-d2eab4-default/downloads/offer_recommendation_100_similar_customers.csv'\n",
    "test_samples = val.sample(n=3000, random_state=2153)\n",
    "result = production_model(test_samples, train, val, id_cols, prod_mix_cols, distance_func, n, minimal_threshold, max_offers_to_return)\n",
    "result.to_csv(save_data_path,index=False)\n",
    "print(f'csv saved in {save_data_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda0b064-b2ed-4cda-b205-35036281a544",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = train\n",
    "# prod_mix_cols: internet, tv, tv, sing, smhm, tos, wifi, whsia\n",
    "prod_mix_cols = ['prod_hsic_cnt','prod_ttv_cnt','prod_stv_cnt','prod_sing_cnt','prod_smhm_cnt','prod_tos_cnt','prod_wifiplus_cnt','prod_whsia_cnt']\n",
    "id_cols=['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id', 'target']\n",
    "distance_func = 'cosine'\n",
    "n = 100\n",
    "minimal_threshold = 0.10\n",
    "max_offers_to_return = 3\n",
    "\n",
    "# features = ['prod_tos_cnt', \n",
    "# 'prod_hsic_cnt', \n",
    "# 'prod_ttv_cnt', \n",
    "# 'prod_sing_cnt', \n",
    "# 'cust_prov_state_cd', \n",
    "# 'prod_smhm_cnt', \n",
    "# 'sing_tenure_days', \n",
    "# 'prod_stv_cnt', \n",
    "# 'demogr_census_division_typ', \n",
    "# 'clk_wln_wifi_plus_cnt_r30d', \n",
    "# 'hsic_tenure_days', \n",
    "# 'prod_whsia_cnt', \n",
    "# 'bill_wln_avg_hsic_debit_amt', \n",
    "# 'bill_wln_avg_ttv_debit_amt', \n",
    "# 'ttv_tenure_days', \n",
    "# 'bill_wln_avg_smhm_debit_amt', \n",
    "# 'smhm_tenure_days', \n",
    "# 'clk_wln_sing_cnt_r30d', \n",
    "# 'hs_usg_avg_ul_gb', \n",
    "# 'clk_health_livingwell_cnt_r30d']\n",
    "\n",
    "# feature_weights = [0.213879816752624, \n",
    "# 0.210757495491595, \n",
    "# 0.155501726318607, \n",
    "# 0.0602533075942611, \n",
    "# 0.0523079604892021, \n",
    "# 0.0284744355928214, \n",
    "# 0.0270177329993315, \n",
    "# 0.0230154595375712, \n",
    "# 0.0170414919779867, \n",
    "# 0.0155022847023418, \n",
    "# 0.0146792811382145, \n",
    "# 0.0142657660878289, \n",
    "# 0.0133395133806645, \n",
    "# 0.0132004812421698, \n",
    "# 0.0118557752793018, \n",
    "# 0.0107220666341007, \n",
    "# 0.0101260045930694, \n",
    "# 0.00916686983616255, \n",
    "# 0.00906401454998682, \n",
    "# 0.00764893682885285]\n",
    "\n",
    "features = ['prod_tos_cnt', 'prod_hsic_cnt', 'prod_ttv_cnt', 'prod_sing_cnt', 'cust_prov_state_cd', 'prod_smhm_cnt', 'sing_tenure_days', 'prod_stv_cnt', 'demogr_census_division_typ', \n",
    "'clk_wln_wifi_plus_cnt_r30d', 'hsic_tenure_days', 'prod_whsia_cnt', 'bill_wln_avg_hsic_debit_amt', 'bill_wln_avg_ttv_debit_amt', 'ttv_tenure_days', 'bill_wln_avg_smhm_debit_amt', 'smhm_tenure_days', \n",
    "'clk_wln_sing_cnt_r30d', 'hs_usg_avg_ul_gb', 'clk_health_livingwell_cnt_r30d', 'hs_usg_avg_dl_gb', 'prod_other_cnt', 'hs_usg_avg_tot_gb', 'bill_wln_avg_ban_discount_amt', 'clk_wln_hsic_cnt_r30d', \n",
    "'acct_ebill_ind', 'clk_wln_smarthome_security_cnt_r30d', 'prod_wifiplus_cnt', 'demogr_avg_household_size', 'clk_wln_optik_cnt_r30d', 'clk_wln_security_cnt_r30d', 'demogr_lifestage_sort', 'demogr_family_flag', \n",
    "'bill_wln_avg_sing_debit_amt', 'ffh_tenure']\n",
    "\n",
    "feature_weights = np.array([0.213879816752624, 0.210757495491595, 0.155501726318607, 0.0602533075942611, 0.0523079604892021, 0.0284744355928214, 0.0270177329993315, 0.0230154595375712, 0.0170414919779867, \n",
    "0.0155022847023418, 0.0146792811382145, 0.0142657660878289, 0.0133395133806645, 0.0132004812421698, 0.0118557752793018, 0.0107220666341007, 0.0101260045930694, 0.00916686983616255, 0.00906401454998682, \n",
    "0.00764893682885285, 0.00758425900125304, 0.00754070816218072, 0.00732201897328859, 0.00718525467726317, 0.00656270026430169, 0.00655088122633769, 0.00584749485663925, 0.00488302137330835, 0.00484122456343937, \n",
    "0.00462082166402429, 0.00433701614023153, 0.00390846588739282, 0.00374843937884009, 0.00366447156176259, 0.003582801])\n",
    "\n",
    "def production_model (df, train, test, id_cols, prod_mix_cols, distance_func, n, minimal_threshold, max_offers_to_return):\n",
    "    \n",
    "    frame = pd.DataFrame()\n",
    "    \n",
    "    train_backup = train\n",
    "    test_backup = test\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    # For each customer in each month\n",
    "    for cust in list(df['ban'].unique()):\n",
    "        i += 1\n",
    "        if i % 1 == 0: \n",
    "            print(f'{i} customers processed')\n",
    "                        \n",
    "        train = train_backup\n",
    "        test = test_backup\n",
    "        \n",
    "        for dt in list(df[df['ban']==cust]['ref_dt'].unique()):\n",
    "            #This part of the code adds the line we want to get offers to the training set, so we can use the distance formula\n",
    "            data = pd.DataFrame()\n",
    "            \n",
    "            add_test = test[(test['ban']==cust)&(test['ref_dt']==dt)]\n",
    "            \n",
    "            if 'target' in add_test.columns: \n",
    "                add_test.drop(columns=['target'], inplace=True)\n",
    "            \n",
    "            add_test['target'] = 0\n",
    "                        \n",
    "            data = train.append(add_test)\n",
    "            data= data.reset_index()\n",
    "\n",
    "            results = get_recommended_offers(data, id_cols, prod_mix_cols, features, feature_weights, cust, dt, distance_func, n, minimal_threshold=minimal_threshold, max_offers_to_return=max_offers_to_return)\n",
    "            \n",
    "            data = {'ban': [cust],\n",
    "                  'ref_dt': [dt],\n",
    "                  'offers': [results]}\n",
    "            \n",
    "            frame1 =  pd.DataFrame(data)\n",
    "            frame = frame.append(frame1)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "save_data_path = 'gs://divg-groovyhoon-pr-d2eab4-default/downloads/offer_recommendation_100_similar_customers.csv'\n",
    "test_samples = val.sample(n=3000, random_state=2153)\n",
    "result = production_model(test_samples, train, val, id_cols, prod_mix_cols, distance_func, n, minimal_threshold, max_offers_to_return)\n",
    "result.to_csv(save_data_path,index=False)\n",
    "print(f'csv saved in {save_data_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1dc1ca-3f9f-4779-8155-9fe0c833363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = postprocess(result, df_val)\n",
    "evaluate_recos(df_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be81ceae-80a4-4716-b9b9-bbb15767c42d",
   "metadata": {},
   "source": [
    "### 200 similar customers x 2000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880539f3-69bf-417a-ad5e-200f3dee83b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = train\n",
    "# prod_mix_cols: internet, tv, tv, sing, smhm, tos, wifi, whsia\n",
    "prod_mix_cols = ['prod_hsic_cnt','prod_ttv_cnt','prod_stv_cnt','prod_sing_cnt','prod_smhm_cnt','prod_tos_cnt','prod_wifiplus_cnt','prod_whsia_cnt']\n",
    "id_cols=['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id', 'target']\n",
    "distance_func = 'cosine'\n",
    "n = 200\n",
    "minimal_threshold = 0.10\n",
    "max_offers_to_return = 3\n",
    "\n",
    "features = ['prod_tos_cnt', 'prod_hsic_cnt', 'prod_ttv_cnt', 'prod_sing_cnt', 'cust_prov_state_cd', 'prod_smhm_cnt', 'sing_tenure_days', 'prod_stv_cnt', 'demogr_census_division_typ', \n",
    "'clk_wln_wifi_plus_cnt_r30d', 'hsic_tenure_days', 'prod_whsia_cnt', 'bill_wln_avg_hsic_debit_amt', 'bill_wln_avg_ttv_debit_amt', 'ttv_tenure_days', 'bill_wln_avg_smhm_debit_amt', 'smhm_tenure_days', \n",
    "'clk_wln_sing_cnt_r30d', 'hs_usg_avg_ul_gb', 'clk_health_livingwell_cnt_r30d', 'hs_usg_avg_dl_gb', 'prod_other_cnt', 'hs_usg_avg_tot_gb', 'bill_wln_avg_ban_discount_amt', 'clk_wln_hsic_cnt_r30d', \n",
    "'acct_ebill_ind', 'clk_wln_smarthome_security_cnt_r30d', 'prod_wifiplus_cnt', 'demogr_avg_household_size', 'clk_wln_optik_cnt_r30d', 'clk_wln_security_cnt_r30d', 'demogr_lifestage_sort', 'demogr_family_flag', \n",
    "'bill_wln_avg_sing_debit_amt', 'ffh_tenure']\n",
    "\n",
    "feature_weights = np.array([0.213879816752624, 0.210757495491595, 0.155501726318607, 0.0602533075942611, 0.0523079604892021, 0.0284744355928214, 0.0270177329993315, 0.0230154595375712, 0.0170414919779867, \n",
    "0.0155022847023418, 0.0146792811382145, 0.0142657660878289, 0.0133395133806645, 0.0132004812421698, 0.0118557752793018, 0.0107220666341007, 0.0101260045930694, 0.00916686983616255, 0.00906401454998682, \n",
    "0.00764893682885285, 0.00758425900125304, 0.00754070816218072, 0.00732201897328859, 0.00718525467726317, 0.00656270026430169, 0.00655088122633769, 0.00584749485663925, 0.00488302137330835, 0.00484122456343937, \n",
    "0.00462082166402429, 0.00433701614023153, 0.00390846588739282, 0.00374843937884009, 0.00366447156176259, 0.003582801])\n",
    "\n",
    "def production_model (df, train, test, id_cols, prod_mix_cols, distance_func, n, minimal_threshold, max_offers_to_return):\n",
    "    \n",
    "    frame = pd.DataFrame()\n",
    "    \n",
    "    test_backup = test\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    # For each customer in each month\n",
    "    for cust in list(df['ban'].unique()):\n",
    "        i += 1\n",
    "        if i % 1 == 0: \n",
    "            print(f'{i} customers processed')\n",
    "            \n",
    "        test = test_backup\n",
    "        \n",
    "        for dt in list(df[df['ban']==cust]['ref_dt'].unique()):\n",
    "            #This part of the code adds the line we want to get offers to the training set, so we can use the distance formula\n",
    "            data = pd.DataFrame()\n",
    "            \n",
    "            add_test = test[(test['ban']==cust)&(test['ref_dt']==dt)]\n",
    "            \n",
    "            if 'target' in add_test.columns: \n",
    "                add_test.drop(columns=['target'], inplace=True)\n",
    "            \n",
    "            add_test['target'] = 0\n",
    "                        \n",
    "            data = train.append(add_test)\n",
    "            data= data.reset_index()\n",
    "\n",
    "            results = get_recommended_offers(data, id_cols, prod_mix_cols, features, feature_weights, cust, dt, distance_func, n, minimal_threshold=minimal_threshold, max_offers_to_return=max_offers_to_return)\n",
    "            \n",
    "            data = {'ban': [cust],\n",
    "                  'dt': [dt],\n",
    "                  'offers': [results]}\n",
    "            \n",
    "            frame1 =  pd.DataFrame(data)\n",
    "            frame = frame.append(frame1)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "save_data_path = 'gs://divg-groovyhoon-pr-d2eab4-default/downloads/offer_recommendation_200_similar_customers.csv'\n",
    "test_samples = test.sample(n=2000, random_state=128)\n",
    "frame_production_100_samples = production_model(test_samples, train, test, id_cols, prod_mix_cols, distance_func, n, minimal_threshold, max_offers_to_return)\n",
    "frame_production_100_samples.to_csv(save_data_path,index=False)\n",
    "print(f'csv saved in {save_data_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3ef332-e0d4-4068-a519-c23a371a0239",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_production_100_samples = frame_production_100_samples.rename(columns={'dt':'ref_dt'})\n",
    "df_result = postprocess(frame_production_100_samples, df_train_preserve)\n",
    "evaluate_recos(df_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbceacc-fb78-4f47-897c-c6ca60c3a3dd",
   "metadata": {},
   "source": [
    "### 200 similar customers x 2000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2096f78-704c-467a-813e-21d76dea044b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = train\n",
    "# prod_mix_cols: internet, tv, tv, sing, smhm, tos, wifi, whsia\n",
    "prod_mix_cols = ['prod_hsic_cnt','prod_ttv_cnt','prod_stv_cnt','prod_sing_cnt','prod_smhm_cnt','prod_tos_cnt','prod_wifiplus_cnt','prod_whsia_cnt']\n",
    "id_cols=['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id', 'target']\n",
    "distance_func = 'cosine'\n",
    "n = 500\n",
    "minimal_threshold = 0.10\n",
    "max_offers_to_return = 3\n",
    "\n",
    "features = ['prod_tos_cnt', 'prod_hsic_cnt', 'prod_ttv_cnt', 'prod_sing_cnt', 'cust_prov_state_cd', 'prod_smhm_cnt', 'sing_tenure_days', 'prod_stv_cnt', 'demogr_census_division_typ', \n",
    "'clk_wln_wifi_plus_cnt_r30d', 'hsic_tenure_days', 'prod_whsia_cnt', 'bill_wln_avg_hsic_debit_amt', 'bill_wln_avg_ttv_debit_amt', 'ttv_tenure_days', 'bill_wln_avg_smhm_debit_amt', 'smhm_tenure_days', \n",
    "'clk_wln_sing_cnt_r30d', 'hs_usg_avg_ul_gb', 'clk_health_livingwell_cnt_r30d', 'hs_usg_avg_dl_gb', 'prod_other_cnt', 'hs_usg_avg_tot_gb', 'bill_wln_avg_ban_discount_amt', 'clk_wln_hsic_cnt_r30d', \n",
    "'acct_ebill_ind', 'clk_wln_smarthome_security_cnt_r30d', 'prod_wifiplus_cnt', 'demogr_avg_household_size', 'clk_wln_optik_cnt_r30d', 'clk_wln_security_cnt_r30d', 'demogr_lifestage_sort', 'demogr_family_flag', \n",
    "'bill_wln_avg_sing_debit_amt', 'ffh_tenure']\n",
    "\n",
    "feature_weights = np.array([0.213879816752624, 0.210757495491595, 0.155501726318607, 0.0602533075942611, 0.0523079604892021, 0.0284744355928214, 0.0270177329993315, 0.0230154595375712, 0.0170414919779867, \n",
    "0.0155022847023418, 0.0146792811382145, 0.0142657660878289, 0.0133395133806645, 0.0132004812421698, 0.0118557752793018, 0.0107220666341007, 0.0101260045930694, 0.00916686983616255, 0.00906401454998682, \n",
    "0.00764893682885285, 0.00758425900125304, 0.00754070816218072, 0.00732201897328859, 0.00718525467726317, 0.00656270026430169, 0.00655088122633769, 0.00584749485663925, 0.00488302137330835, 0.00484122456343937, \n",
    "0.00462082166402429, 0.00433701614023153, 0.00390846588739282, 0.00374843937884009, 0.00366447156176259, 0.003582801])\n",
    "\n",
    "def production_model (df, train, test, id_cols, prod_mix_cols, distance_func, n, minimal_threshold, max_offers_to_return):\n",
    "    \n",
    "    frame = pd.DataFrame()\n",
    "    \n",
    "    test_backup = test\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    # For each customer in each month\n",
    "    for cust in list(df['ban'].unique()):\n",
    "        i += 1\n",
    "        if i % 1 == 0: \n",
    "            print(f'{i} customers processed')\n",
    "            \n",
    "        test = test_backup\n",
    "        \n",
    "        for dt in list(df[df['ban']==cust]['ref_dt'].unique()):\n",
    "            #This part of the code adds the line we want to get offers to the training set, so we can use the distance formula\n",
    "            data = pd.DataFrame()\n",
    "            \n",
    "            add_test = test[(test['ban']==cust)&(test['ref_dt']==dt)]\n",
    "            \n",
    "            if 'target' in add_test.columns: \n",
    "                add_test.drop(columns=['target'], inplace=True)\n",
    "            \n",
    "            add_test['target'] = 0\n",
    "                        \n",
    "            data = train.append(add_test)\n",
    "            data= data.reset_index()\n",
    "\n",
    "            results = get_recommended_offers(data, id_cols, prod_mix_cols, features, feature_weights, cust, dt, distance_func, n, minimal_threshold=minimal_threshold, max_offers_to_return=max_offers_to_return)\n",
    "            \n",
    "            data = {'ban': [cust],\n",
    "                  'dt': [dt],\n",
    "                  'offers': [results]}\n",
    "            \n",
    "            frame1 =  pd.DataFrame(data)\n",
    "            frame = frame.append(frame1)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "save_data_path = 'gs://divg-groovyhoon-pr-d2eab4-default/downloads/offer_recommendation_500_similar_customers.csv'\n",
    "test_samples = test.sample(n=2000, random_state=654)\n",
    "frame_production_100_samples = production_model(test_samples, train, test, id_cols, prod_mix_cols, distance_func, n, minimal_threshold, max_offers_to_return)\n",
    "frame_production_100_samples.to_csv(save_data_path,index=False)\n",
    "print(f'csv saved in {save_data_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f8c7c8-06b4-4d57-9d1e-5bf86ff9c234",
   "metadata": {},
   "source": [
    "### 4. apply the function to the entire scoring set (with bootstrapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef04a93-c294-4beb-aef6-5ef5fb91164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = train\n",
    "# prod_mix_cols: internet, tv, tv, sing, smhm, tos, wifi, whsia\n",
    "id_cols=['model_scenario', 'ref_dt', 'cust_id', 'ban', 'lpds_id', 'target']\n",
    "prod_mix_cols = ['prod_hsic_cnt','prod_ttv_cnt','prod_stv_cnt','prod_sing_cnt','prod_smhm_cnt','prod_tos_cnt','prod_wifiplus_cnt','prod_whsia_cnt']\n",
    "distance_funcs = ['euclidean', 'manhattan', 'cosine']\n",
    "n_values = [500, 1000, 2000, 5000, 10000] \n",
    "minimal_threshold = 0.10\n",
    "max_offers_to_return = 3\n",
    "\n",
    "def production_model (df, train, test, id_cols, prod_mix_cols, distance_funcs, n_values, minimal_threshold, max_offers_to_return):\n",
    "    \n",
    "    frame = pd.DataFrame()\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    # For each customer in each month\n",
    "    for cust in list(df['ban'].unique()):\n",
    "        i += 1\n",
    "        if i % 50 == 0: \n",
    "            print(f'{i} customers processed')\n",
    "        \n",
    "        for dt in list(df[df['ban']==cust]['ref_dt'].unique()):\n",
    "            #This part of the code adds the line we want to get offers to the training set, so we can use the distance formula\n",
    "            data = pd.DataFrame()\n",
    "            data = train.append(test[(test['ban']==cust)&(test['ref_dt']==dt)])\n",
    "            data= data.reset_index()\n",
    "\n",
    "            results = get_recommended_offers_multiple(data, id_cols, prod_mix_cols, cust, dt, distance_funcs, n_values, minimal_threshold=minimal_threshold, max_offers_to_return=max_offers_to_return)\n",
    "            data = {'ban': [cust],\n",
    "                  'dt': [dt],\n",
    "                  'offers': [results]}\n",
    "            \n",
    "            frame1 =  pd.DataFrame(data)\n",
    "            frame = frame.append(frame1)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "test_samples = test.sample(n=1000, random_state=42)\n",
    "frame_production_100_samples = production_model(test_samples, train, test, id_cols, prod_mix_cols, distance_funcs, n_values, minimal_threshold, max_offers_to_return)\n",
    "frame_production_100_samples.to_csv('gs://divg-groovyhoon-pr-d2eab4-default/downloads/offer_recommendation_with_bootstap_1000.csv',index=False)\n",
    "print(f'csv saved in gs://divg-groovyhoon-pr-d2eab4-default/downloads/offer_recommendation_with_bootstap_1000.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426e602c-d3dd-4748-9617-410bc68b193e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c667351-fbe6-470c-bc4b-2cc2f1715182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fde9e6-149d-4cef-a5b2-bab3dc5cc64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(result, df_train[['ban', 'ref_dt', 'target']], how = 'left', on=['ban', 'ref_dt'])\n",
    "\n",
    "product_mapping = {\n",
    "    'hsic': 0,\n",
    "    'ttv': 1,\n",
    "    'sing': 2,\n",
    "    'shs': 3, \n",
    "    'tos': 4, \n",
    "    'wifi': 5, \n",
    "    'lwc': 6, \n",
    "    'sws': 7, \n",
    "    'hpro': 8, \n",
    "    'whsia': 9\n",
    "}\n",
    "\n",
    "# Define a function to extract elements from the list\n",
    "def extract_offer_values(offer_list, position):\n",
    "    if len(offer_list) >= position:\n",
    "        return offer_list[position - 1]  # Subtract 1 because list indexing starts from 0\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create new columns for each element of the list\n",
    "for i in range(1, 4):\n",
    "    result[f'reco{i}'] = result['offers'].apply(lambda x: extract_offer_values(x, i))\n",
    "\n",
    "result['reco_1'] = result['reco1'].map(product_mapping)\n",
    "result['reco_2'] = result['reco2'].map(product_mapping)\n",
    "result['reco_3'] = result['reco3'].map(product_mapping)\n",
    "\n",
    "result['accuracy1'] = np.where(result['reco_1'] == result['target'], 1, 0)\n",
    "result['accuracy2'] = np.where(result['reco_2'] == result['target'], 1, 0)\n",
    "result['accuracy3'] = np.where(result['reco_3'] == result['target'], 1, 0)\n",
    "\n",
    "result['accuracy_1'] = result.apply(lambda row: 1 if sum(row[['accuracy1']]) > 0 else 0, axis=1)\n",
    "result['accuracy_2'] =  result.apply(lambda row: 1 if sum(row[['accuracy1', 'accuracy2']]) > 0 else 0, axis=1)\n",
    "result['accuracy_3'] =  result.apply(lambda row: 1 if sum(row[['accuracy1', 'accuracy2', 'accuracy3']]) > 0 else 0, axis=1)\n",
    "\n",
    "result.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea28671e-3558-4fc5-88e4-95178d849d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming simsMatrix is your 67838x3000 matrix containing cosine similarity scores\n",
    "\n",
    "# Create an empty list to store the indices of top 200 most similar customers for each customer in the test set\n",
    "top_indices = []\n",
    "\n",
    "i = 0 \n",
    "# Iterate through each row (vector) of the simsMatrix\n",
    "for row in simsMatrix:\n",
    "    i += 1\n",
    "    if i % 100 == 0: \n",
    "        print(f'{i}th customers processed') \n",
    "    # Use np.argsort() to get the indices that would sort the row in ascending order\n",
    "    sorted_indices = np.argsort(row)\n",
    "    \n",
    "    # Get the indices of the top 200 most similar customers\n",
    "    top_200_indices = sorted_indices[-200:]\n",
    "    \n",
    "    # Add the top 200 indices to the list\n",
    "    top_indices.append(top_200_indices)\n",
    "\n",
    "# Now, top_indices contains the indices of the top 200 most similar customers for each customer in the test set\n",
    "\n",
    "print(len(top_indices[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8543e0b-a18a-40bc-a841-52d97b55bceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_prod_reco(prod_mix, prod_reco_list): \n",
    "    \n",
    "    # This function accepts the customer's product mix and product recommenndations as input parameters, \n",
    "    # and filters out the product recommendations that the customer already has.\n",
    "    \n",
    "    # 1. check the customer's product intensity\n",
    "    cust_prod_list = []\n",
    "\n",
    "    if prod_mix[0] > 0: \n",
    "        cust_prod_list.append('hsic')\n",
    "    if prod_mix[1] + prod_mix[2] > 0: \n",
    "        cust_prod_list.append('ttv')\n",
    "    if prod_mix[3] > 0: \n",
    "        cust_prod_list.append('sing')\n",
    "    if prod_mix[4] > 0: \n",
    "        cust_prod_list.append('shs')\n",
    "    if prod_mix[5] > 0: \n",
    "        cust_prod_list.append('tos')\n",
    "    if prod_mix[6] > 0: \n",
    "        cust_prod_list.append('wifi')\n",
    "    if prod_mix[7] > 0: \n",
    "        cust_prod_list.append('whsia')\n",
    "    ### add for lwc, sws, hrpo as product count labels become available ###\n",
    "    # if prod_mix[n] > 0: \n",
    "    #     cust_prod_list.append('lwc')\n",
    "    # if prod_mix[n] > 0: \n",
    "    #     cust_prod_list.append('sws')\n",
    "    # if prod_mix[n] > 0: \n",
    "    #     cust_prod_list.append('hpro')\n",
    "\n",
    "    # 2. trim the prodduct reco list to remove \"_acquisition\" for comparison. \n",
    "    prod_reco_list = [value.replace('_acquisition', '') for value in prod_reco_list]\n",
    "\n",
    "    final_prod_reco_list = [item for item in prod_reco_list if item not in cust_prod_list]\n",
    "\n",
    "    return final_prod_reco_list\n",
    "\n",
    "result = []\n",
    "\n",
    "i = 0 \n",
    "\n",
    "for idx in top_indices: \n",
    "    \n",
    "    i += 1\n",
    "    if i % 100 == 0: \n",
    "        print(f'{i}th customers processed') \n",
    "\n",
    "    # extract the customer data for the most similar customers\n",
    "    similar_customers = df.iloc[idx]\n",
    "\n",
    "    # count the top offers of the similar customers\n",
    "    top_offers = similar_customers[['cust_id', 'ban', 'lpds_id', 'model_scenario', 'target']].groupby(['model_scenario', 'target'])['ban'].count().reset_index(name='ban_count').sort_values(by='ban_count', ascending=False)\n",
    "    top_offers['perc_total'] = top_offers['ban_count']/top_offers['ban_count'].sum()\n",
    "    \n",
    "    # generate product recommendations list based on collaborative filtering\n",
    "    prod_mix = df[prod_mix_cols].values[idx]\n",
    "    prod_reco_list = list(top_offers['model_scenario'][:])\n",
    "\n",
    "    # filter out product recos that the customer already has\n",
    "    final_prod_reco_list = final_prod_reco(prod_mix, prod_reco_list)\n",
    "\n",
    "    result.append(final_prod_reco_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96b3cd7-37d9-42da-9250-8c9398daf51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_prod_reco(prod_mix, prod_reco_list): \n",
    "    \n",
    "    # This function accepts the customer's product mix and product recommenndations as input parameters, \n",
    "    # and filters out the product recommendations that the customer already has.\n",
    "    \n",
    "    # 1. check the customer's product intensity\n",
    "    cust_prod_list = []\n",
    "\n",
    "    if prod_mix[0] > 0: \n",
    "        cust_prod_list.append('hsic')\n",
    "    if prod_mix[1] + prod_mix[2] > 0: \n",
    "        cust_prod_list.append('ttv')\n",
    "    if prod_mix[3] > 0: \n",
    "        cust_prod_list.append('sing')\n",
    "    if prod_mix[4] > 0: \n",
    "        cust_prod_list.append('shs')\n",
    "    if prod_mix[5] > 0: \n",
    "        cust_prod_list.append('tos')\n",
    "    if prod_mix[6] > 0: \n",
    "        cust_prod_list.append('wifi')\n",
    "    if prod_mix[7] > 0: \n",
    "        cust_prod_list.append('whsia')\n",
    "    ### add for lwc, sws, hrpo as product count labels become available ###\n",
    "    # if prod_mix[n] > 0: \n",
    "    #     cust_prod_list.append('lwc')\n",
    "    # if prod_mix[n] > 0: \n",
    "    #     cust_prod_list.append('sws')\n",
    "    # if prod_mix[n] > 0: \n",
    "    #     cust_prod_list.append('hpro')\n",
    "\n",
    "    # 2. trim the prodduct reco list to remove \"_acquisition\" for comparison. \n",
    "    prod_reco_list = [value.replace('_acquisition', '') for value in prod_reco_list]\n",
    "\n",
    "    final_prod_reco_list = [item for item in prod_reco_list if item not in cust_prod_list]\n",
    "\n",
    "    return final_prod_reco_list\n",
    "\n",
    "def get_recommended_offers (df: pd.DataFrame, \n",
    "                            id_cols: list,\n",
    "                            prod_mix_cols: list, \n",
    "                            features: list, \n",
    "                            feature_weights: list, \n",
    "                            ban: int, \n",
    "                            ref_dt, \n",
    "                            distance_func: str,\n",
    "                            n: int,\n",
    "                            minimal_threshold: float,\n",
    "                            max_offers_to_return: int\n",
    "                            ):\n",
    "\n",
    "    def perform_pca(X, n): \n",
    "\n",
    "        from sklearn.decomposition import PCA\n",
    "\n",
    "        pca = PCA(n_components = n) \n",
    "\n",
    "        X_reduced = pca.fit_transform(X)\n",
    "\n",
    "        cols = [f'PC{i}' for i in range(n)]\n",
    "\n",
    "        df_reduced = pd.DataFrame(data=X_reduced, columns=cols) \n",
    "\n",
    "        return df_reduced\n",
    "\n",
    "    # define df_X \n",
    "    df_X = df[features] * feature_weights\n",
    "    \n",
    "#     # define df_X_pca\n",
    "#     df_X_pca = perform_pca(df_X, 30)\n",
    "    \n",
    "    # extract the feature vectors of all customers\n",
    "    X = df_X.values\n",
    "    \n",
    "    # extract the feature vector of the given customer\n",
    "    index = df[(df['ban'] == ban) & (df['ref_dt']==ref_dt)].index[0]\n",
    "    x = X[index]\n",
    "\n",
    "    # compute the distances between the feature vectors\n",
    "    if distance_func == 'euclidean':\n",
    "        distances = euclidean_distances(X, x.reshape(1, -1)).flatten()\n",
    "    elif distance_func == 'manhattan':\n",
    "        distances = manhattan_distances(X, x.reshape(1, -1)).flatten()\n",
    "    elif distance_func == 'cosine':\n",
    "        distances = 1 - cosine_similarity(X, x.reshape(1, -1)).flatten()\n",
    "    else:\n",
    "        raise ValueError('Invalid distance function specified.')\n",
    "\n",
    "    # find the indices of the n customers with lowest distance\n",
    "    most_similar_indices = distances.argsort()[:n]\n",
    "    \n",
    "    # extract the customer data for the most similar customers\n",
    "    similar_customers = df.iloc[most_similar_indices]\n",
    "    \n",
    "    # count the top offers of the similar customers\n",
    "    # top_offers = similar_customers[['cust_id', 'ban', 'lpds_id', 'model_scenario', 'target_y']].groupby(['model_scenario', 'target_y']).agg({'cust_id':'count'}).reset_index().sort_values(by = 'cust_id', ascending = False)\n",
    "    top_offers = similar_customers[['cust_id', 'ban', 'lpds_id', 'model_scenario', 'target']].groupby(['model_scenario', 'target'])['ban'].count().reset_index(name='ban_count').sort_values(by='ban_count', ascending=False)\n",
    "    top_offers['perc_total'] = top_offers['ban_count']/top_offers['ban_count'].sum()\n",
    "    \n",
    "    # generate product recommendations list based on collaborative filtering\n",
    "    prod_mix = df[prod_mix_cols].values[index]\n",
    "    prod_reco_list = list(top_offers['model_scenario'][:])\n",
    "    \n",
    "    # filter out product recos that the customer already has\n",
    "    final_prod_reco_list = final_prod_reco(prod_mix, prod_reco_list)\n",
    "    \n",
    "    print(final_prod_reco_list)\n",
    "\n",
    "    return final_prod_reco_list \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": ".m116",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m116"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
