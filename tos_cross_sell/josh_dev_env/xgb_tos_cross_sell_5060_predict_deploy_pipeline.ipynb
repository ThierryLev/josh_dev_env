{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac00439-6d76-4c6d-973e-00f79e55c909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(mapping):\n",
    "    print(mapping)\n",
    "    from kfp import dsl\n",
    "    from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output, OutputPath, ClassificationMetrics,\n",
    "                            Metrics, component)\n",
    "    import os\n",
    "    import re\n",
    "    import google\n",
    "    from google.oauth2 import credentials\n",
    "    from google.oauth2 import service_account\n",
    "    from google.oauth2.service_account import Credentials\n",
    "    from datetime import date\n",
    "    from datetime import timedelta\n",
    "    from dateutil.relativedelta import relativedelta\n",
    "\n",
    "    SERVICE_TYPE = 'tos_cross_sell'\n",
    "    DATASET_ID = '{}_dataset'.format(SERVICE_TYPE)\n",
    "    PROJECT_ID = mapping['PROJECT_ID']\n",
    "    RESOURCE_BUCKET = mapping['resources_bucket']\n",
    "    FILE_BUCKET = mapping['gcs_csv_bucket']\n",
    "    REGION = mapping['REGION']\n",
    "    MODEL_ID = '5060'\n",
    "    FOLDER_NAME = 'xgb_{}_{}_predict_deploy'.format(SERVICE_TYPE, MODEL_ID)\n",
    "    QUERIES_PATH = 'vertex_pipelines/' + FOLDER_NAME + '/queries/'\n",
    "\n",
    "    scoringDate = date.today() - relativedelta(days=5)\n",
    "\n",
    "    # current day views\n",
    "    CONSL_VIEW_NAME = '{}_pipeline_consl_data_curr_bi_layer'.format(SERVICE_TYPE)  # done\n",
    "    FFH_BILLING_VIEW_NAME = '{}_pipeline_ffh_billing_data_curr_bi_layer'.format(SERVICE_TYPE)  # done\n",
    "    HS_USAGE_VIEW_NAME = '{}_pipeline_hs_usage_data_curr_bi_layer'.format(SERVICE_TYPE)  # done\n",
    "    DEMO_INCOME_VIEW_NAME = '{}_pipeline_demo_income_data_curr_bi_layer'.format(SERVICE_TYPE)  # done\n",
    "    PROMO_EXPIRY_VIEW_NAME = '{}_pipeline_promo_expiry_data_curr_bi_layer'.format(SERVICE_TYPE)  # done\n",
    "    GPON_COPPER_VIEW_NAME = '{}_pipeline_gpon_copper_data_curr_bi_layer'.format(SERVICE_TYPE)  # done\n",
    "    CLCKSTRM_TELUS_VIEW_NAME = '{}_pipeline_clckstrm_telus_curr_bi_layer'.format(SERVICE_TYPE)\n",
    "    ALARMDOTCOM_APP_USAGE_VIEW_NAME = '{}_pipeline_alarmdotcom_app_usage_curr_bi_layer'.format(SERVICE_TYPE)\n",
    "    TOS_ACTIVE_BANS_VIEW_NAME = '{}_pipeline_tos_active_bans_curr_bi_layer'.format(SERVICE_TYPE) \n",
    "\n",
    "    # dates\n",
    "    SCORE_DATE = scoringDate.strftime('%Y%m%d')  # date.today().strftime('%Y%m%d')\n",
    "    SCORE_DATE_DASH = scoringDate.strftime('%Y-%m-%d')\n",
    "    SCORE_DATE_MINUS_6_MOS_DASH = ((scoringDate - relativedelta(months=6)).replace(day=1)).strftime('%Y-%m-%d')\n",
    "    SCORE_DATE_LAST_MONTH_START_DASH = (scoringDate.replace(day=1) - timedelta(days=1)).replace(day=1).strftime('%Y-%m-%d')\n",
    "    SCORE_DATE_LAST_MONTH_END_DASH = ((scoringDate.replace(day=1)) - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "    SCORE_DATE_LAST_MONTH_YEAR = ((scoringDate.replace(day=1)) - timedelta(days=1)).year\n",
    "    SCORE_DATE_LAST_MONTH_MONTH = ((scoringDate.replace(day=1)) - timedelta(days=1)).month\n",
    "\n",
    "    SCORE_DATE_DELTA = 0\n",
    "    SCORE_DATE_VAL_DELTA = 0\n",
    "    TICKET_DATE_WINDOW = 30  # Days of ticket data to be queried\n",
    "\n",
    "    ACCOUNT_CONSL_QUERY_PATH = QUERIES_PATH + 'create_input_account_consl_query.txt'\n",
    "    ACCOUNT_GPON_COPPER_QUERY_PATH = QUERIES_PATH + 'create_input_account_gpon_copper_query.txt'\n",
    "    ACCOUNT_PROMO_EXPIRY_QUERY_PATH = QUERIES_PATH + 'create_input_account_promo_expiry_query.txt'\n",
    "    ACCOUNT_DEMO_INCOME_QUERY_PATH = QUERIES_PATH + 'create_input_account_demo_income_query.txt'\n",
    "    ACCOUNT_HS_USAGE_QUERY_PATH = QUERIES_PATH + 'create_input_account_hs_usage_query.txt'\n",
    "    ACCOUNT_FFH_BILLING_QUERY_PATH = QUERIES_PATH + 'create_input_account_ffh_billing_query.txt'\n",
    "    ACCOUNT_CLCKSTRM_TELUS_QUERY_PATH = QUERIES_PATH + 'create_input_account_clckstrm_telus_query.txt'\n",
    "    ACCOUNT_ALARMDOTCOM_APP_USAGE_QUERY_PATH = QUERIES_PATH + 'create_input_account_alarmdotcom_app_usage_query.txt'\n",
    "    ACCOUNT_TOS_ACTIVE_BANS_QUERY_PATH = QUERIES_PATH + 'create_input_account_tos_active_bans_query.txt'\n",
    "\n",
    "    @component(\n",
    "        base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/vertex_pipelines/kfp-preprocess-slim:latest\",\n",
    "        output_component_file=\"{}_model_account_consl_view.yaml\".format(SERVICE_TYPE),\n",
    "    )\n",
    "    def create_input_account_consl_view(view_name: str,\n",
    "                                        score_date: str,\n",
    "                                        score_date_delta: str,\n",
    "                                        project_id: str,\n",
    "                                        dataset_id: str,\n",
    "                                        region: str,\n",
    "                                        resource_bucket: str,\n",
    "                                        query_path: str,\n",
    "                                        ):\n",
    "\n",
    "        from google.cloud import bigquery\n",
    "        from google.cloud import storage\n",
    "\n",
    "        def if_tbl_exists(client, table_ref):\n",
    "            from google.cloud.exceptions import NotFound\n",
    "            try:\n",
    "                client.get_table(table_ref)\n",
    "                return True\n",
    "            except NotFound:\n",
    "                return False\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        dataset = bq_client.dataset(dataset_id)\n",
    "        table_ref = dataset.table(view_name)\n",
    "\n",
    "        # load query from .txt file\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(resource_bucket)\n",
    "        blob = bucket.get_blob(query_path)\n",
    "        content = blob.download_as_string()\n",
    "        content = str(content, 'utf-8')\n",
    "\n",
    "        if if_tbl_exists(bq_client, table_ref):\n",
    "            bq_client.delete_table(table_ref)\n",
    "\n",
    "        # content = open(query_path, 'r').read()\n",
    "\n",
    "        create_base_feature_set_query = content.format(score_date=score_date,\n",
    "                                                       score_date_delta=score_date_delta,\n",
    "                                                       view_name=view_name,\n",
    "                                                       dataset_id=dataset_id,\n",
    "                                                       project_id=project_id,\n",
    "                                                       )\n",
    "        shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "        base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "        base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "        base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "        base_feature_set_view = bq_client.create_table(base_feature_set_view)\n",
    "\n",
    "    @component(\n",
    "        base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/vertex_pipelines/kfp-preprocess-slim:latest\",\n",
    "        output_component_file=\"{}_model_ffh_billing_view.yaml\".format(SERVICE_TYPE),\n",
    "    )\n",
    "    def create_input_account_ffh_billing_view(view_name: str,\n",
    "                                                  v_report_date: str,\n",
    "                                                  v_start_date: str,\n",
    "                                                  v_end_date: str,\n",
    "                                                  v_bill_year: str,\n",
    "                                                  v_bill_month: str,\n",
    "                                                  dataset_id: str,\n",
    "                                                  project_id: str,\n",
    "                                                  region: str,\n",
    "                                                  resource_bucket: str,\n",
    "                                                  query_path: str\n",
    "                                                  ):\n",
    "        from google.cloud import bigquery\n",
    "        from google.cloud import storage\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        dataset = bq_client.dataset(dataset_id)\n",
    "        table_ref = dataset.table(view_name)\n",
    "\n",
    "        # load query from .txt file\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(resource_bucket)\n",
    "        blob = bucket.get_blob(query_path)\n",
    "        content = blob.download_as_string()\n",
    "        content = str(content, 'utf-8')\n",
    "\n",
    "        def if_tbl_exists(client, table_ref):\n",
    "            from google.cloud.exceptions import NotFound\n",
    "            try:\n",
    "                client.get_table(table_ref)\n",
    "                return True\n",
    "            except NotFound:\n",
    "                return False\n",
    "\n",
    "        if if_tbl_exists(bq_client, table_ref):\n",
    "            bq_client.delete_table(table_ref)\n",
    "\n",
    "        create_base_feature_set_query = content.format(v_report_date=v_report_date,\n",
    "                                                       v_start_date=v_start_date,\n",
    "                                                       v_end_date=v_end_date,\n",
    "                                                       v_bill_year=v_bill_year,\n",
    "                                                       v_bill_month=v_bill_month,\n",
    "                                                       )\n",
    "\n",
    "        shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "        base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "        base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "        base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "        base_feature_set_view = bq_client.create_table(base_feature_set_view)\n",
    "\n",
    "    @component(\n",
    "        base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/vertex_pipelines/kfp-preprocess-slim:latest\",\n",
    "        output_component_file=\"{}_model_hs_usage_view.yaml\".format(SERVICE_TYPE),\n",
    "    )\n",
    "    def create_input_account_hs_usage_view(view_name: str,\n",
    "                                           v_report_date: str,\n",
    "                                           v_start_date: str,\n",
    "                                           v_end_date: str,\n",
    "                                           v_bill_year: str,\n",
    "                                           v_bill_month: str,\n",
    "                                           dataset_id: str,\n",
    "                                           project_id: str,\n",
    "                                           region: str,\n",
    "                                           resource_bucket: str,\n",
    "                                           query_path: str\n",
    "                                           ):\n",
    "\n",
    "        from google.cloud import bigquery\n",
    "        from google.cloud import storage\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        dataset = bq_client.dataset(dataset_id)\n",
    "        table_ref = dataset.table(view_name)\n",
    "\n",
    "        # load query from .txt file\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(resource_bucket)\n",
    "        blob = bucket.get_blob(query_path)\n",
    "        content = blob.download_as_string()\n",
    "        content = str(content, 'utf-8')\n",
    "\n",
    "        def if_tbl_exists(client, table_ref):\n",
    "            from google.cloud.exceptions import NotFound\n",
    "            try:\n",
    "                client.get_table(table_ref)\n",
    "                return True\n",
    "            except NotFound:\n",
    "                return False\n",
    "\n",
    "        if if_tbl_exists(bq_client, table_ref):\n",
    "            bq_client.delete_table(table_ref)\n",
    "\n",
    "        create_base_feature_set_query = content.format(v_report_date=v_report_date,\n",
    "                                                       v_start_date=v_start_date,\n",
    "                                                       v_end_date=v_end_date,\n",
    "                                                       v_bill_year=v_bill_year,\n",
    "                                                       v_bill_month=v_bill_month,\n",
    "                                                       )\n",
    "\n",
    "        shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "        base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "        base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "        base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "        base_feature_set_view = bq_client.create_table(base_feature_set_view)\n",
    "\n",
    "    @component(\n",
    "        base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/vertex_pipelines/kfp-preprocess-slim:latest\",\n",
    "        output_component_file=\"{}_model_demo_income_view.yaml\".format(SERVICE_TYPE),\n",
    "    )\n",
    "    def create_input_account_demo_income_view(view_name: str,\n",
    "                                              score_date: str,\n",
    "                                              score_date_delta: str,\n",
    "                                              dataset_id: str,\n",
    "                                              project_id: str,\n",
    "                                              region: str,\n",
    "                                              resource_bucket: str,\n",
    "                                              query_path: str\n",
    "                                              ):\n",
    "\n",
    "        from google.cloud import bigquery\n",
    "        from google.cloud import storage\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        dataset = bq_client.dataset(dataset_id)\n",
    "        table_ref = dataset.table(view_name)\n",
    "\n",
    "        # load query from .txt file\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(resource_bucket)\n",
    "        blob = bucket.get_blob(query_path)\n",
    "        content = blob.download_as_string()\n",
    "        content = str(content, 'utf-8')\n",
    "\n",
    "        def if_tbl_exists(client, table_ref):\n",
    "            from google.cloud.exceptions import NotFound\n",
    "            try:\n",
    "                client.get_table(table_ref)\n",
    "                return True\n",
    "            except NotFound:\n",
    "                return False\n",
    "\n",
    "        if if_tbl_exists(bq_client, table_ref):\n",
    "            bq_client.delete_table(table_ref)\n",
    "\n",
    "        create_base_feature_set_query = content.format(score_date=score_date,\n",
    "                                                       score_date_delta=score_date_delta,\n",
    "                                                       project_id=project_id,\n",
    "                                                       dataset_id='common_dataset',\n",
    "                                                       )\n",
    "\n",
    "        shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "        base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "        base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "        base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "        base_feature_set_view = bq_client.create_table(base_feature_set_view)\n",
    "\n",
    "    @component(\n",
    "        base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/vertex_pipelines/kfp-preprocess-slim:latest\",\n",
    "        output_component_file=\"{}_model_promo_expiry_view.yaml\".format(SERVICE_TYPE),\n",
    "    )\n",
    "    def create_input_account_promo_expiry_view(view_name: str,\n",
    "                                               score_date: str,\n",
    "                                               dataset_id: str,\n",
    "                                               project_id: str,\n",
    "                                               region: str,\n",
    "                                               resource_bucket: str,\n",
    "                                               query_path: str\n",
    "                                               ):\n",
    "\n",
    "        from google.cloud import bigquery\n",
    "        from google.cloud import storage\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        dataset = bq_client.dataset(dataset_id)\n",
    "        table_ref = dataset.table(view_name)\n",
    "\n",
    "        # load query from .txt file\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(resource_bucket)\n",
    "        blob = bucket.get_blob(query_path)\n",
    "        content = blob.download_as_string()\n",
    "        content = str(content, 'utf-8')\n",
    "\n",
    "        def if_tbl_exists(client, table_ref):\n",
    "            from google.cloud.exceptions import NotFound\n",
    "            try:\n",
    "                client.get_table(table_ref)\n",
    "                return True\n",
    "            except NotFound:\n",
    "                return False\n",
    "\n",
    "        if if_tbl_exists(bq_client, table_ref):\n",
    "            bq_client.delete_table(table_ref)\n",
    "\n",
    "        create_base_feature_set_query = content.format(score_date=score_date)\n",
    "\n",
    "        create_base_feature_set_query = create_base_feature_set_query.replace('{', '{{')\n",
    "        create_base_feature_set_query = create_base_feature_set_query.replace('}', '}}')\n",
    "\n",
    "        shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "        base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "        base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "        base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "        base_feature_set_view = bq_client.create_table(base_feature_set_view)\n",
    "\n",
    "    @component(\n",
    "        base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/vertex_pipelines/kfp-preprocess-slim:latest\",\n",
    "        output_component_file=\"{}_model_gpon_copper_view.yaml\".format(SERVICE_TYPE),\n",
    "    )\n",
    "    def create_input_account_gpon_copper_view(view_name: str,\n",
    "                                              score_date: str,\n",
    "                                              score_date_delta: str,\n",
    "                                              dataset_id: str,\n",
    "                                              project_id: str,\n",
    "                                              region: str,\n",
    "                                              resource_bucket: str,\n",
    "                                              query_path: str\n",
    "                                              ):\n",
    "\n",
    "        from google.cloud import bigquery\n",
    "        from google.cloud import storage\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        dataset = bq_client.dataset(dataset_id)\n",
    "        table_ref = dataset.table(view_name)\n",
    "\n",
    "        # load query from .txt file\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(resource_bucket)\n",
    "        blob = bucket.get_blob(query_path)\n",
    "        content = blob.download_as_string()\n",
    "        content = str(content, 'utf-8')\n",
    "\n",
    "        def if_tbl_exists(client, table_ref):\n",
    "            from google.cloud.exceptions import NotFound\n",
    "            try:\n",
    "                client.get_table(table_ref)\n",
    "                return True\n",
    "            except NotFound:\n",
    "                return False\n",
    "\n",
    "        if if_tbl_exists(bq_client, table_ref):\n",
    "            bq_client.delete_table(table_ref)\n",
    "\n",
    "        create_base_feature_set_query = content.format(score_date=score_date,\n",
    "                                                       score_date_delta=score_date_delta,\n",
    "                                                       )\n",
    "\n",
    "        shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "        base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "        base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "        base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "        base_feature_set_view = bq_client.create_table(base_feature_set_view)\n",
    "\n",
    "    @component(\n",
    "        base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/vertex_pipelines/kfp-preprocess-slim:latest\",\n",
    "        output_component_file=\"{}_model_clckstrm_telus_view.yaml\".format(SERVICE_TYPE),\n",
    "    )\n",
    "    def create_input_account_clckstrm_telus_view(view_name: str,\n",
    "                                        score_date: str,\n",
    "                                        score_date_delta: str,\n",
    "                                        project_id: str,\n",
    "                                        dataset_id: str,\n",
    "                                        region: str,\n",
    "                                        resource_bucket: str,\n",
    "                                        query_path: str,\n",
    "                                        ):\n",
    "\n",
    "        from google.cloud import bigquery\n",
    "        from google.cloud import storage\n",
    "\n",
    "        def if_tbl_exists(client, table_ref):\n",
    "            from google.cloud.exceptions import NotFound\n",
    "            try:\n",
    "                client.get_table(table_ref)\n",
    "                return True\n",
    "            except NotFound:\n",
    "                return False\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        dataset = bq_client.dataset(dataset_id)\n",
    "        table_ref = dataset.table(view_name)\n",
    "\n",
    "        # load query from .txt file\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(resource_bucket)\n",
    "        blob = bucket.get_blob(query_path)\n",
    "        content = blob.download_as_string()\n",
    "        content = str(content, 'utf-8')\n",
    "\n",
    "        if if_tbl_exists(bq_client, table_ref):\n",
    "            bq_client.delete_table(table_ref)\n",
    "\n",
    "        # content = open(query_path, 'r').read()\n",
    "\n",
    "        create_base_feature_set_query = content.format(score_date=score_date,\n",
    "                                                       score_date_delta=score_date_delta,\n",
    "                                                       view_name=view_name,\n",
    "                                                       dataset_id=dataset_id,\n",
    "                                                       project_id=project_id,\n",
    "                                                       )\n",
    "        shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "        base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "        base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "        base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "        base_feature_set_view = bq_client.create_table(base_feature_set_view)\n",
    "\n",
    "    @component(\n",
    "        base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/vertex_pipelines/kfp-preprocess-slim:latest\",\n",
    "        output_component_file=\"{}_model_alarmdotcom_app_usage_view.yaml\".format(SERVICE_TYPE),\n",
    "    )\n",
    "    def create_input_account_alarmdotcom_app_usage_view(view_name: str,\n",
    "                                        score_date: str,\n",
    "                                        score_date_delta: str,\n",
    "                                        project_id: str,\n",
    "                                        dataset_id: str,\n",
    "                                        region: str,\n",
    "                                        resource_bucket: str,\n",
    "                                        query_path: str,\n",
    "                                        ):\n",
    "\n",
    "        from google.cloud import bigquery\n",
    "        from google.cloud import storage\n",
    "\n",
    "        def if_tbl_exists(client, table_ref):\n",
    "            from google.cloud.exceptions import NotFound\n",
    "            try:\n",
    "                client.get_table(table_ref)\n",
    "                return True\n",
    "            except NotFound:\n",
    "                return False\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        dataset = bq_client.dataset(dataset_id)\n",
    "        table_ref = dataset.table(view_name)\n",
    "\n",
    "        # load query from .txt file\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(resource_bucket)\n",
    "        blob = bucket.get_blob(query_path)\n",
    "        content = blob.download_as_string()\n",
    "        content = str(content, 'utf-8')\n",
    "\n",
    "        if if_tbl_exists(bq_client, table_ref):\n",
    "            bq_client.delete_table(table_ref)\n",
    "\n",
    "        # content = open(query_path, 'r').read()\n",
    "\n",
    "        create_base_feature_set_query = content.format(score_date=score_date,\n",
    "                                                       score_date_delta=score_date_delta,\n",
    "                                                       view_name=view_name,\n",
    "                                                       dataset_id=dataset_id,\n",
    "                                                       project_id=project_id,\n",
    "                                                       )\n",
    "        shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "        base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "        base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "        base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "        base_feature_set_view = bq_client.create_table(base_feature_set_view)\n",
    "\n",
    "    @component(\n",
    "        base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/vertex_pipelines/kfp-preprocess-slim:latest\",\n",
    "        output_component_file=\"{}_model_tos_active_bans_view.yaml\".format(SERVICE_TYPE),\n",
    "    )\n",
    "    def create_input_account_tos_active_bans_view(view_name: str,\n",
    "                                        score_date: str,\n",
    "                                        score_date_delta: str,\n",
    "                                        v_start_date: str,\n",
    "                                        v_end_date: str,\n",
    "                                        project_id: str,\n",
    "                                        dataset_id: str,\n",
    "                                        region: str,\n",
    "                                        resource_bucket: str,\n",
    "                                        query_path: str,\n",
    "                                        ):\n",
    "\n",
    "        from google.cloud import bigquery\n",
    "        from google.cloud import storage\n",
    "\n",
    "        def if_tbl_exists(client, table_ref):\n",
    "            from google.cloud.exceptions import NotFound\n",
    "            try:\n",
    "                client.get_table(table_ref)\n",
    "                return True\n",
    "            except NotFound:\n",
    "                return False\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        dataset = bq_client.dataset(dataset_id)\n",
    "        table_ref = dataset.table(view_name)\n",
    "\n",
    "        # load query from .txt file\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(resource_bucket)\n",
    "        blob = bucket.get_blob(query_path)\n",
    "        content = blob.download_as_string()\n",
    "        content = str(content, 'utf-8')\n",
    "\n",
    "        if if_tbl_exists(bq_client, table_ref):\n",
    "            bq_client.delete_table(table_ref)\n",
    "\n",
    "        # content = open(query_path, 'r').read()\n",
    "\n",
    "        create_base_feature_set_query = content.format(score_date=score_date,\n",
    "                                                       score_date_delta=score_date_delta,\n",
    "                                                       v_start_date=v_start_date,\n",
    "                                                       v_end_date=v_end_date,\n",
    "                                                       view_name=view_name,\n",
    "                                                       dataset_id=dataset_id,\n",
    "                                                       project_id=project_id,\n",
    "                                                       )\n",
    "        shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "        base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "        base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "        base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "        base_feature_set_view = bq_client.create_table(base_feature_set_view)\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    @component(\n",
    "        base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/vertex_pipelines/kfp-preprocess-slim:latest\",\n",
    "        output_component_file=\"{}_model_preprocess.yaml\".format(SERVICE_TYPE),\n",
    "    )\n",
    "    def preprocess(\n",
    "            account_consl_view: str,\n",
    "            account_bill_view: str,\n",
    "            hs_usage_view: str,\n",
    "            demo_income_view: str,\n",
    "            promo_expiry_view: str,\n",
    "            gpon_copper_view: str,\n",
    "            clckstrm_telus_view: str, \n",
    "            alarmdotcom_app_usage_view: str, \n",
    "            tos_active_bans_view: str, \n",
    "            save_data_path: str,\n",
    "            project_id: str,\n",
    "            dataset_id: str\n",
    "    ):\n",
    "        \n",
    "        from google.cloud import bigquery\n",
    "        import pandas as pd\n",
    "        import gc\n",
    "        import time\n",
    "\n",
    "        client = bigquery.Client(project=project_id)\n",
    "        consl_data_set = f\"{project_id}.{dataset_id}.{account_consl_view}\" \n",
    "\n",
    "        build_df_consl = '''SELECT * FROM `{consl_data_set}`'''.format(consl_data_set=consl_data_set)\n",
    "        df_consl = client.query(build_df_consl).to_dataframe()\n",
    "        print('......base data done')\n",
    "\n",
    "        # product mix\n",
    "        df_mix = df_consl[[\n",
    "            'ban',\n",
    "            'product_mix_all',\n",
    "            'sing_count',\n",
    "            'hsic_count',\n",
    "            'mob_count',\n",
    "            'shs_count',\n",
    "            'ttv_count',\n",
    "            'stv_count',\n",
    "            'diic_count',\n",
    "            'new_c_ind',\n",
    "            'new_sing_ind',\n",
    "            'new_hsic_ind',\n",
    "            'new_ttv_ind',\n",
    "            'new_smhm_ind',\n",
    "            'mnh_ind'\n",
    "        ]]\n",
    "        df_mix = df_mix.drop_duplicates(subset=['ban']).set_index('ban').add_prefix('productMix_')\n",
    "\n",
    "        # df_join\n",
    "        df_join = df_mix.fillna(0)\n",
    "\n",
    "        del df_mix\n",
    "        gc.collect()\n",
    "        print('......product mix done')\n",
    "\n",
    "        bill_data_set = f\"{project_id}.{dataset_id}.{account_bill_view}\" \n",
    "        build_df_bill = '''SELECT * FROM `{bill_data_set}`'''.format(bill_data_set=bill_data_set)\n",
    "\n",
    "        client = bigquery.Client(project=project_id)\n",
    "        df_bill = client.query(build_df_bill).to_dataframe() \n",
    "\n",
    "        df_bill = df_bill.set_index('ban').add_prefix('ffhBill_')\n",
    "\n",
    "        # df_join\n",
    "        df_join = df_join.join(df_bill).fillna(0) \n",
    "        del df_bill\n",
    "        gc.collect()\n",
    "        print('......account bill done')\n",
    "\n",
    "        hs_usage_data_set = f\"{project_id}.{dataset_id}.{hs_usage_view}\" \n",
    "        build_df_hs_usage = '''SELECT * FROM `{hs_usage_data_set}`'''.format(hs_usage_data_set=hs_usage_data_set)\n",
    "\n",
    "        client = bigquery.Client(project=project_id)\n",
    "        df_hs_usage = client.query(build_df_hs_usage).to_dataframe() \n",
    "\n",
    "        df_hs_usage = df_hs_usage.set_index('ban').add_prefix('hsiaUsage_')\n",
    "\n",
    "        # df_join\n",
    "        df_join = df_join.join(df_hs_usage).fillna(0) \n",
    "        del df_hs_usage\n",
    "        gc.collect()\n",
    "        print('......hs usage done')\n",
    "\n",
    "        demo_income_data_set = f\"{project_id}.{dataset_id}.{demo_income_view}\" \n",
    "        build_df_demo_income = '''SELECT * FROM `{demo_income_data_set}`'''.format(\n",
    "            demo_income_data_set=demo_income_data_set)\n",
    "\n",
    "        client = bigquery.Client(project=project_id)\n",
    "        df_income = client.query(build_df_demo_income).to_dataframe()\n",
    "\n",
    "        df_income = df_income.set_index('ban')\n",
    "        df_income['demo_urban_flag'] = df_income.demo_sgname.str.lower().str.contains('urban').fillna(0).astype(int)\n",
    "        df_income['demo_rural_flag'] = df_income.demo_sgname.str.lower().str.contains('rural').fillna(0).astype(int)\n",
    "        df_income['demo_family_flag'] = df_income.demo_lsname.str.lower().str.contains('families').fillna(0).astype(int)\n",
    "        df_income_dummies = pd.get_dummies(df_income[['demo_lsname']])\n",
    "        df_income_dummies.columns = df_income_dummies.columns.str.replace('&', 'and')\n",
    "        df_income_dummies.columns = df_income_dummies.columns.str.replace(' ', '_')\n",
    "        df_income = df_income[['demo_avg_income', 'demo_urban_flag', 'demo_rural_flag', 'demo_family_flag']].join(\n",
    "            df_income_dummies)\n",
    "        df_income.demo_avg_income = df_income.demo_avg_income.astype(float)\n",
    "        df_income.demo_avg_income = df_income.demo_avg_income.fillna(df_income.demo_avg_income.median())\n",
    "        df_group_income = df_income.groupby('ban').agg('mean')\n",
    "        df_group_income = df_group_income.add_prefix('demographics_')\n",
    "        # df_join\n",
    "        df_join = df_join.join(df_group_income.fillna(df_group_income.median()))\n",
    "\n",
    "        del df_group_income\n",
    "        del df_income\n",
    "        gc.collect()\n",
    "        print('......income done')\n",
    "\n",
    "        promo_expiry_data_set = f\"{project_id}.{dataset_id}.{promo_expiry_view}\" \n",
    "        build_df_promo = '''SELECT * FROM `{promo_expiry_data_set}` '''.format(\n",
    "            promo_expiry_data_set=promo_expiry_data_set)\n",
    "\n",
    "        client = bigquery.Client(project=project_id)\n",
    "        df_promo = client.query(build_df_promo).to_dataframe()  \n",
    "\n",
    "        df_promo = df_promo.set_index('ban')\n",
    "        disc_cols = [col for col in df_promo.columns if 'disc' in col]\n",
    "        bill_cols = [col for col in df_promo.columns if 'disc' not in col]\n",
    "\n",
    "        df_join = df_join.join(df_promo[disc_cols].add_prefix('promo_'))\n",
    "        df_join = df_join.join(df_promo[bill_cols].add_prefix('ffhBill_')).fillna(0)\n",
    "\n",
    "        del df_promo\n",
    "        gc.collect()\n",
    "        print('......promo expiry done')\n",
    "\n",
    "        # gpon copper\n",
    "        gpon_copper_data_set = f\"{project_id}.{dataset_id}.{gpon_copper_view}\"\n",
    "        build_df_gpon_copper = '''\n",
    "        SELECT * FROM `{gpon_copper_data_set}` \n",
    "        '''.format(gpon_copper_data_set=gpon_copper_data_set)\n",
    "\n",
    "        client = bigquery.Client(project=project_id)\n",
    "        df_gpon_copper = client.query(build_df_gpon_copper).to_dataframe()\n",
    "\n",
    "        df_gpon_copper = df_gpon_copper.set_index('ban')\n",
    "        df_join = df_join.join(df_gpon_copper.add_prefix('infra_')).fillna(0)\n",
    "        del df_gpon_copper\n",
    "        gc.collect()\n",
    "        print('......gpon copper done')\n",
    "\n",
    "        # clickstream data\n",
    "        clckstrm_telus_data_set = f\"{project_id}.{dataset_id}.{clckstrm_telus_view}\" \n",
    "        build_df_clckstrm_telus = '''SELECT * FROM `{clckstrm_telus_data_set}`'''.format(clckstrm_telus_data_set=clckstrm_telus_data_set)\n",
    "\n",
    "        client = bigquery.Client(project=project_id)\n",
    "        df_clckstrm_telus = client.query(build_df_clckstrm_telus).to_dataframe() \n",
    "\n",
    "        df_clckstrm_telus = df_clckstrm_telus.set_index('ban').add_prefix('clckstrmData_')\n",
    "\n",
    "        # df_join\n",
    "        df_join = df_join.join(df_clckstrm_telus).fillna(0) \n",
    "        del df_clckstrm_telus\n",
    "        gc.collect()\n",
    "        print('......clcktsrm data done')\n",
    "\n",
    "        # alarm.com data\n",
    "        alarmdotcom_app_usage_data_set = f\"{project_id}.{dataset_id}.{alarmdotcom_app_usage_view}\" \n",
    "        build_df_alarmdotcom_app_usage = '''SELECT * FROM `{alarmdotcom_app_usage_data_set}`'''.format(alarmdotcom_app_usage_data_set=alarmdotcom_app_usage_data_set)\n",
    "\n",
    "        client = bigquery.Client(project=project_id)\n",
    "        df_alarmdotcom_app_usage = client.query(build_df_alarmdotcom_app_usage).to_dataframe() \n",
    "\n",
    "        df_alarmdotcom_app_usage = df_alarmdotcom_app_usage.set_index('ban').add_prefix('alarmdotcomAppUsage_')\n",
    "\n",
    "        # df_join\n",
    "        df_join = df_join.join(df_alarmdotcom_app_usage).fillna(0) \n",
    "        del df_alarmdotcom_app_usage\n",
    "        gc.collect()\n",
    "        print('......alarm.com app usage data done')\n",
    "\n",
    "        # tos active bans\n",
    "        tos_active_bans = f\"{project_id}.{dataset_id}.{tos_active_bans_view}\" \n",
    "        build_df_tos_active_data = '''SELECT * FROM `{tos_active_bans}`'''.format(tos_active_bans=tos_active_bans)\n",
    "\n",
    "        client = bigquery.Client(project=project_id)\n",
    "        df_tos_active_data = client.query(build_df_tos_active_data).to_dataframe()  # Make an API request.\n",
    "\n",
    "        # df_join\n",
    "        df_tos_active_data = df_tos_active_data.set_index('ban')\n",
    "        df_join = df_join.join(df_tos_active_data).fillna(0)\n",
    "        del df_tos_active_data\n",
    "        gc.collect()\n",
    "        print('......tos active data done')\n",
    "\n",
    "        df_join.columns = df_join.columns.str.replace(' ', '_')\n",
    "        df_join.columns = df_join.columns.str.replace('-', '_')\n",
    "\n",
    "        df_final = df_join.copy()\n",
    "        del df_join\n",
    "        gc.collect()\n",
    "        print('......df final done')\n",
    "\n",
    "        for f in df_final.columns:\n",
    "            df_final[f] = list(df_final[f])\n",
    "\n",
    "        df_final = df_final.loc[(df_final['tos_ind'] == 0) & (df_final['new_smhm_ind'] == 0)].reset_index()\n",
    "        df_final = df_final.drop(['tos_ind', 'new_smhm_ind'], axis=1) \n",
    "\n",
    "        df_final.to_csv(save_data_path, index=False, compression='gzip') \n",
    "        del df_final\n",
    "        gc.collect()\n",
    "        print(f'......csv saved in {save_data_path}')\n",
    "        time.sleep(300)\n",
    "\n",
    "    @component(\n",
    "        base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/vertex_pipelines/kfp-xgboost-slim:latest\",\n",
    "        output_component_file=\"xgb_batch_prediction.yaml\",\n",
    "    )\n",
    "    def batch_prediction(\n",
    "            project_id: str,\n",
    "            dataset_id: str,\n",
    "            file_bucket: str,\n",
    "            service_type: str,\n",
    "            score_table: str,\n",
    "            score_date_dash: str,\n",
    "            metrics: Output[Metrics],\n",
    "            metricsc: Output[ClassificationMetrics],\n",
    "    ):\n",
    "        import time\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import pickle\n",
    "        from datetime import date\n",
    "        from dateutil.relativedelta import relativedelta\n",
    "        from google.cloud import bigquery\n",
    "        from google.cloud import storage\n",
    "        \n",
    "        MODEL_ID = '5060'\n",
    "        \n",
    "        def if_tbl_exists(bq_client, table_ref):\n",
    "            from google.cloud.exceptions import NotFound\n",
    "            try:\n",
    "                bq_client.get_table(table_ref)\n",
    "                return True\n",
    "            except NotFound:\n",
    "                return False\n",
    "\n",
    "        def upsert_table(project_id, dataset_id, table_id, sql, result):\n",
    "            new_values = ',\\n'.join(result.apply(lambda row: row_format(row), axis=1))\n",
    "            new_sql = sql.format(proj_id=project_id, dataset_id=dataset_id, table_id=table_id,\n",
    "                                 new_values=new_values)\n",
    "            bq_client = bigquery.Client(project=project_id)\n",
    "            code = bq_client.query(new_sql)\n",
    "            time.sleep(5)\n",
    "\n",
    "        def row_format(row):\n",
    "            values = row.values\n",
    "            new_values = \"\"\n",
    "            v = str(values[0]) if not pd.isnull(values[0]) else 'NULL'\n",
    "            if 'str' in str(type(values[0])):\n",
    "                new_values += f\"'{v}'\"\n",
    "            else:\n",
    "                new_values += f\"{v}\"\n",
    "\n",
    "            for i in range(1, len(values)):\n",
    "                v = str(values[i]) if not pd.isnull(values[i]) else 'NULL'\n",
    "                if 'str' in str(type(values[i])):\n",
    "                    new_values += f\",'{v}'\"\n",
    "                else:\n",
    "                    new_values += f\",{v}\"\n",
    "            return '(' + new_values + ')'\n",
    "\n",
    "        def generate_sql_file(ll):\n",
    "            s = 'MERGE INTO `{proj_id}.{dataset_id}.{table_id}` a'\n",
    "            s += \" USING UNNEST(\"\n",
    "            s += \"[struct<\"\n",
    "            for i in range(len(ll) - 1):\n",
    "                v = ll[i]\n",
    "                s += \"{} {},\".format(v[0], v[1])\n",
    "            s += \"{} {}\".format(ll[-1][0], ll[-1][1])\n",
    "            s += \">{new_values}]\"\n",
    "            s += \") b\"\n",
    "            s += \" ON a.ban = b.ban and a.score_date = b.score_date\"\n",
    "            s += \" WHEN MATCHED THEN\"\n",
    "            s += \" UPDATE SET \"\n",
    "            s += \"a.{}=b.{},\".format(ll[0][0], ll[0][0])\n",
    "            for i in range(1, len(ll) - 1):\n",
    "                v = ll[i]\n",
    "                s += \"a.{}=b.{},\".format(v[0], v[0])\n",
    "            s += \"a.{}=b.{}\".format(ll[-1][0], ll[-1][0])\n",
    "            s += \" WHEN NOT MATCHED THEN\"\n",
    "            s += \" INSERT(\"\n",
    "            for i in range(len(ll) - 1):\n",
    "                v = ll[i]\n",
    "                s += \"{},\".format(v[0])\n",
    "            s += \"{})\".format(ll[-1][0])\n",
    "            s += \" VALUES(\"\n",
    "            for i in range(len(ll) - 1):\n",
    "                s += \"b.{},\".format(ll[i][0])\n",
    "            s += \"b.{}\".format(ll[-1][0])\n",
    "            s += \")\"\n",
    "\n",
    "            return s\n",
    "\n",
    "        MODEL_PATH = '{}_xgb_models/'.format(service_type)\n",
    "        df_score = pd.read_csv('gs://{}/{}/{}_score.csv.gz'.format(file_bucket, service_type, service_type), compression='gzip')\n",
    "        df_score.dropna(subset=['ban'], inplace=True)\n",
    "        df_score.reset_index(drop=True, inplace=True)\n",
    "        print('......scoring data loaded:{}'.format(df_score.shape))\n",
    "        time.sleep(10)\n",
    "\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(file_bucket)\n",
    "        blobs = storage_client.list_blobs(file_bucket, prefix='{}{}_models_xgb_'.format(MODEL_PATH, service_type))\n",
    "\n",
    "        model_lists = []\n",
    "        for blob in blobs:\n",
    "            model_lists.append(blob.name)\n",
    "\n",
    "        blob = bucket.blob(model_lists[-1])\n",
    "        blob_in = blob.download_as_string()\n",
    "        model_dict = pickle.loads(blob_in)\n",
    "        model_xgb = model_dict['model']\n",
    "        features = model_dict['features']\n",
    "        print('...... model loaded')\n",
    "        time.sleep(10)\n",
    "\n",
    "        ll = [('ban', 'string'), ('score_date', 'string'), ('model_id', 'string'), ('score', 'float64')]\n",
    "        sql = generate_sql_file(ll)\n",
    "\n",
    "        df_score['ban'] = df_score['ban'].astype(int)\n",
    "        print('.... scoring for {} tos cross sell bans base'.format(len(df_score)))\n",
    "\n",
    "        # get full score to cave into bucket\n",
    "        pred_prob = model_xgb.predict_proba(df_score[features], ntree_limit=model_xgb.best_iteration)[:, 1]\n",
    "        result = pd.DataFrame(columns=['ban', 'score_date', 'model_id', 'score'])\n",
    "        result['score'] = list(pred_prob)\n",
    "        result['score'] = result['score'].fillna(0.0).astype('float64')\n",
    "        result['ban'] = list(df_score['ban'])\n",
    "        result['ban'] = result['ban'].astype('str')\n",
    "        result['score_date'] = score_date_dash\n",
    "        result['model_id'] = MODEL_ID\n",
    "\n",
    "        result.to_csv('gs://{}/ucar/{}_prediction.csv.gz'.format(file_bucket, service_type), compression='gzip',\n",
    "                      index=False)\n",
    "        time.sleep(60)\n",
    "\n",
    "        batch_size = 1000\n",
    "        n_batchs = int(df_score.shape[0] / batch_size) + 1\n",
    "        print('...... will upsert {} batches'.format(n_batchs))\n",
    "\n",
    "        # start batch prediction\n",
    "        all_scores = np.array(result['score'].values)\n",
    "        for i in range(n_batchs):\n",
    "        \n",
    "            s, e = i * batch_size, (i + 1) * batch_size\n",
    "            if e >= df_score.shape[0]:\n",
    "                e = df_score.shape[0]\n",
    "\n",
    "            df_temp = df_score.iloc[s:e]\n",
    "            pred_prob = all_scores[s:e]\n",
    "            batch_result = pd.DataFrame(columns=['ban', 'score_date', 'model_id', 'score'])\n",
    "            batch_result['score'] = list(pred_prob)\n",
    "            batch_result['score'] = batch_result['score'].fillna(0.0).astype('float64')\n",
    "            batch_result['ban'] = list(df_temp['ban'])\n",
    "            batch_result['ban'] = batch_result['ban'].astype('str')\n",
    "            batch_result['score_date'] = score_date_dash\n",
    "            batch_result['model_id'] = MODEL_ID\n",
    "\n",
    "            upsert_table(project_id,\n",
    "                         dataset_id,\n",
    "                         score_table,\n",
    "                         sql,\n",
    "                         batch_result,\n",
    "                         )\n",
    "            if i % 20 == 0:\n",
    "                print('predict for batch {} done'.format(i), end=' ')\n",
    "\n",
    "        time.sleep(120)\n",
    "        \n",
    "        \n",
    "        #-------------------------------------------------------complete upto here----------------------------------------------------------------#\n",
    "\n",
    "\n",
    "    @component(\n",
    "        base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/vertex_pipelines/kfp-xgboost-slim:latest\",\n",
    "        output_component_file=\"xgb_postprocess.yaml\",\n",
    "    )\n",
    "    def postprocess(\n",
    "            project_id: str,\n",
    "            file_bucket: str,\n",
    "            service_type: str,\n",
    "            score_date_dash: str,\n",
    "    ):\n",
    "        import time\n",
    "        import pandas as pd\n",
    "        from google.cloud import bigquery\n",
    "\n",
    "        MODEL_ID = '5060'\n",
    "        file_name = 'gs://{}/ucar/{}_prediction.csv.gz'.format(file_bucket, service_type)\n",
    "        df_orig = pd.read_csv(file_name, compression='gzip')\n",
    "        df_orig.dropna(subset=['ban'], inplace=True)\n",
    "        df_orig.reset_index(drop=True, inplace=True)\n",
    "        df_orig['scoring_date'] = score_date_dash\n",
    "        df_orig.ban = df_orig.ban.astype(int)\n",
    "        df_orig = df_orig.rename(columns={'ban': 'bus_bacct_num', 'score': 'score_num'})\n",
    "        df_orig.score_num = df_orig.score_num.astype(float)\n",
    "        df_orig['decile_grp_num'] = pd.qcut(df_orig['score_num'], q=10, labels=False)\n",
    "        df_orig.decile_grp_num = df_orig.decile_grp_num + 1\n",
    "        df_orig['percentile_pct'] = df_orig.score_num.rank(pct=True)\n",
    "        df_orig['predict_model_nm'] = 'FFH TOS CROSS SELL Model - DIVG'\n",
    "        df_orig['model_type_cd'] = 'FFH'\n",
    "        df_orig['subscriber_no'] = \"\"\n",
    "        df_orig['prod_instnc_resrc_str'] = \"\"\n",
    "        df_orig['service_instnc_id'] = \"\"\n",
    "        df_orig['segment_nm'] = \"\"\n",
    "        df_orig['segment_id'] = \"\"\n",
    "        df_orig['classn_nm'] = \"\"\n",
    "        df_orig['predict_model_id'] = MODEL_ID\n",
    "        df_orig.drop(columns=['model_id', 'score_date'], axis=1, inplace=True)\n",
    "\n",
    "        get_cust_id = \"\"\"\n",
    "        WITH bq_snpsht_max_date AS(\n",
    "        SELECT PARSE_DATE('%Y%m%d', MAX(partition_id)) AS max_date\n",
    "            FROM `cio-datahub-enterprise-pr-183a.ent_cust_cust.INFORMATION_SCHEMA.PARTITIONS` \n",
    "        WHERE table_name = 'bq_prod_instnc_snpsht' \n",
    "            AND partition_id <> '__NULL__'\n",
    "        ),\n",
    "        -- BANs can have multiple Cust ID. Create rank by product type and status, prioritizing ban/cust id with active FFH products\n",
    "        rank_prod_type AS (\n",
    "        SELECT DISTINCT\n",
    "            bacct_bus_bacct_num,\n",
    "            consldt_cust_bus_cust_id AS cust_id,\n",
    "            CASE WHEN pi_prod_instnc_resrc_typ_cd IN ('SING', 'HSIC', 'TTV', 'SMHM', 'STV', 'DIIC') AND pi_prod_instnc_stat_cd = 'A' THEN 1\n",
    "                    WHEN pi_prod_instnc_resrc_typ_cd IN ('SING', 'HSIC', 'TTV', 'SMHM', 'STV', 'DIIC') THEN 2\n",
    "                    WHEN pi_prod_instnc_stat_cd = 'A' THEN 3\n",
    "                    ELSE 4\n",
    "                    END AS prod_rank\n",
    "        FROM `cio-datahub-enterprise-pr-183a.ent_cust_cust.bq_prod_instnc_snpsht`\n",
    "        CROSS JOIN bq_snpsht_max_date\n",
    "        WHERE CAST(prod_instnc_ts AS DATE)=bq_snpsht_max_date.max_date\n",
    "        AND bus_prod_instnc_src_id = 1001\n",
    "        ),\n",
    "        --Rank Cust ID\n",
    "        rank_cust_id AS (\n",
    "        SELECT DISTINCT\n",
    "            bacct_bus_bacct_num,\n",
    "            cust_id,\n",
    "            RANK() OVER(PARTITION BY bacct_bus_bacct_num\n",
    "                            ORDER BY prod_rank,\n",
    "                                        cust_id) AS cust_id_rank               \n",
    "        FROM rank_prod_type\n",
    "        )\n",
    "        --Select best cust id\n",
    "        SELECT bacct_bus_bacct_num,\n",
    "            cust_id\n",
    "        FROM rank_cust_id\n",
    "        WHERE cust_id_rank = 1\n",
    "        \"\"\"\n",
    "\n",
    "        client = bigquery.Client(project=project_id)\n",
    "        df_cust = client.query(get_cust_id).to_dataframe()\n",
    "        df_final = df_orig.set_index('bus_bacct_num').join(df_cust.set_index('bacct_bus_bacct_num')).reset_index()\n",
    "        df_final = df_final.rename(columns={'index': 'bus_bacct_num', 'cust_bus_cust_id': 'cust_id'})\n",
    "        df_final = df_final.sort_values(by=['score_num'], ascending=False)\n",
    "        df_final.to_csv(file_name, compression='gzip', index=False)\n",
    "        time.sleep(300)\n",
    "        \n",
    "    @dsl.pipeline(\n",
    "        # A name for the pipeline.\n",
    "        name=\"{}-xgb-predict-pipeline\".format(SERVICE_TYPE),\n",
    "        description=' pipeline for predict {} model'.format(SERVICE_TYPE)\n",
    "    )\n",
    "    def pipeline(\n",
    "                project_id: str = PROJECT_ID,\n",
    "                region: str = REGION,\n",
    "                resource_bucket: str = RESOURCE_BUCKET,\n",
    "                file_bucket: str = FILE_BUCKET\n",
    "        ):\n",
    "        # ------------- train view ops ---------------\n",
    "        create_input_account_consl_train_view_op = create_input_account_consl_view(\n",
    "            view_name=CONSL_VIEW_NAME,\n",
    "            score_date=SCORE_DATE,\n",
    "            score_date_delta=SCORE_DATE_DELTA,\n",
    "            project_id=PROJECT_ID,\n",
    "            dataset_id=DATASET_ID,\n",
    "            region=REGION,\n",
    "            resource_bucket=RESOURCE_BUCKET,\n",
    "            query_path=ACCOUNT_CONSL_QUERY_PATH,\n",
    "        )\n",
    "        create_input_account_consl_train_view_op.set_memory_limit('16G')\n",
    "        create_input_account_consl_train_view_op.set_cpu_limit('4')\n",
    "\n",
    "        create_input_account_ffh_billing_train_view_op = create_input_account_ffh_billing_view(\n",
    "            v_report_date=SCORE_DATE_DASH,\n",
    "            v_start_date=SCORE_DATE_MINUS_6_MOS_DASH,\n",
    "            v_end_date=SCORE_DATE_LAST_MONTH_END_DASH,\n",
    "            v_bill_year=SCORE_DATE_LAST_MONTH_YEAR,\n",
    "            v_bill_month=SCORE_DATE_LAST_MONTH_MONTH,\n",
    "            view_name=FFH_BILLING_VIEW_NAME,\n",
    "            dataset_id=DATASET_ID,\n",
    "            project_id=PROJECT_ID,\n",
    "            region=REGION,\n",
    "            resource_bucket=RESOURCE_BUCKET,\n",
    "            query_path=ACCOUNT_FFH_BILLING_QUERY_PATH\n",
    "        )\n",
    "\n",
    "        create_input_account_ffh_billing_train_view_op.set_memory_limit('16G')\n",
    "        create_input_account_ffh_billing_train_view_op.set_cpu_limit('4')\n",
    "\n",
    "        create_input_account_hs_usage_train_view_op = create_input_account_hs_usage_view(\n",
    "            v_report_date=SCORE_DATE_DASH,\n",
    "            v_start_date=SCORE_DATE_MINUS_6_MOS_DASH,\n",
    "            v_end_date=SCORE_DATE_LAST_MONTH_END_DASH,\n",
    "            v_bill_year=SCORE_DATE_LAST_MONTH_YEAR,\n",
    "            v_bill_month=SCORE_DATE_LAST_MONTH_MONTH,\n",
    "            view_name=HS_USAGE_VIEW_NAME,\n",
    "            dataset_id=DATASET_ID,\n",
    "            project_id=PROJECT_ID,\n",
    "            region=REGION,\n",
    "            resource_bucket=RESOURCE_BUCKET,\n",
    "            query_path=ACCOUNT_HS_USAGE_QUERY_PATH\n",
    "        )\n",
    "\n",
    "        create_input_account_hs_usage_train_view_op.set_memory_limit('16G')\n",
    "        create_input_account_hs_usage_train_view_op.set_cpu_limit('4')\n",
    "\n",
    "        create_input_account_demo_income_train_view_op = create_input_account_demo_income_view(\n",
    "            score_date=SCORE_DATE,\n",
    "            score_date_delta=SCORE_DATE_DELTA,\n",
    "            view_name=DEMO_INCOME_VIEW_NAME,\n",
    "            dataset_id=DATASET_ID,\n",
    "            project_id=PROJECT_ID,\n",
    "            region=REGION,\n",
    "            resource_bucket=RESOURCE_BUCKET,\n",
    "            query_path=ACCOUNT_DEMO_INCOME_QUERY_PATH\n",
    "        )\n",
    "\n",
    "        create_input_account_demo_income_train_view_op.set_memory_limit('16G')\n",
    "        create_input_account_demo_income_train_view_op.set_cpu_limit('4')\n",
    "\n",
    "        create_input_account_promo_expiry_train_view_op = create_input_account_promo_expiry_view(\n",
    "            score_date=SCORE_DATE,\n",
    "            view_name=PROMO_EXPIRY_VIEW_NAME,\n",
    "            dataset_id=DATASET_ID,\n",
    "            project_id=PROJECT_ID,\n",
    "            region=REGION,\n",
    "            resource_bucket=RESOURCE_BUCKET,\n",
    "            query_path=ACCOUNT_PROMO_EXPIRY_QUERY_PATH\n",
    "        )\n",
    "\n",
    "        create_input_account_promo_expiry_train_view_op.set_memory_limit('16G')\n",
    "        create_input_account_promo_expiry_train_view_op.set_cpu_limit('4')\n",
    "\n",
    "        create_input_account_gpon_copper_train_view_op = create_input_account_gpon_copper_view(\n",
    "            score_date=SCORE_DATE,\n",
    "            score_date_delta=SCORE_DATE_DELTA,\n",
    "            view_name=GPON_COPPER_VIEW_NAME,\n",
    "            dataset_id=DATASET_ID,\n",
    "            project_id=PROJECT_ID,\n",
    "            region=REGION,\n",
    "            resource_bucket=RESOURCE_BUCKET,\n",
    "            query_path=ACCOUNT_GPON_COPPER_QUERY_PATH\n",
    "        )\n",
    "\n",
    "        create_input_account_gpon_copper_train_view_op.set_memory_limit('16G')\n",
    "        create_input_account_gpon_copper_train_view_op.set_cpu_limit('4')\n",
    "\n",
    "        create_input_account_clckstrm_telus_view_op = create_input_account_clckstrm_telus_view(\n",
    "            score_date=SCORE_DATE,\n",
    "            score_date_delta=SCORE_DATE_DELTA,\n",
    "            view_name=CLCKSTRM_TELUS_VIEW_NAME,\n",
    "            dataset_id=DATASET_ID,\n",
    "            project_id=PROJECT_ID,\n",
    "            region=REGION,\n",
    "            resource_bucket=RESOURCE_BUCKET,\n",
    "            query_path=ACCOUNT_CLCKSTRM_TELUS_QUERY_PATH\n",
    "        )\n",
    "\n",
    "        create_input_account_clckstrm_telus_view_op.set_memory_limit('16G')\n",
    "        create_input_account_clckstrm_telus_view_op.set_cpu_limit('4')\n",
    "\n",
    "        create_input_account_alarmdotcom_app_usage_view_op = create_input_account_alarmdotcom_app_usage_view(\n",
    "            score_date=SCORE_DATE,\n",
    "            score_date_delta=SCORE_DATE_DELTA,\n",
    "            view_name=ALARMDOTCOM_APP_USAGE_VIEW_NAME,\n",
    "            dataset_id=DATASET_ID,\n",
    "            project_id=PROJECT_ID,\n",
    "            region=REGION,\n",
    "            resource_bucket=RESOURCE_BUCKET,\n",
    "            query_path=ACCOUNT_ALARMDOTCOM_APP_USAGE_QUERY_PATH\n",
    "        )\n",
    "\n",
    "        create_input_account_alarmdotcom_app_usage_view_op.set_memory_limit('16G')\n",
    "        create_input_account_alarmdotcom_app_usage_view_op.set_cpu_limit('4')\n",
    "\n",
    "        create_input_account_tos_active_bans_view_op = create_input_account_tos_active_bans_view(\n",
    "            score_date=SCORE_DATE,\n",
    "            score_date_delta=SCORE_DATE_DELTA,\n",
    "            v_start_date=SCORE_DATE_LAST_MONTH_START_DASH,\n",
    "            v_end_date=SCORE_DATE_LAST_MONTH_END_DASH,\n",
    "            view_name=TOS_ACTIVE_BANS_VIEW_NAME,\n",
    "            dataset_id=DATASET_ID,\n",
    "            project_id=PROJECT_ID,\n",
    "            region=REGION,\n",
    "            resource_bucket=RESOURCE_BUCKET,\n",
    "            query_path=ACCOUNT_TOS_ACTIVE_BANS_QUERY_PATH\n",
    "        )\n",
    "\n",
    "        create_input_account_tos_active_bans_view_op.set_memory_limit('16G')\n",
    "        create_input_account_tos_active_bans_view_op.set_cpu_limit('4')\n",
    "\n",
    "        # ----- preprocessing train data --------\n",
    "        preprocess_op = preprocess(\n",
    "            account_consl_view=CONSL_VIEW_NAME,\n",
    "            account_bill_view=FFH_BILLING_VIEW_NAME,\n",
    "            hs_usage_view=HS_USAGE_VIEW_NAME,\n",
    "            demo_income_view=DEMO_INCOME_VIEW_NAME,\n",
    "            promo_expiry_view=PROMO_EXPIRY_VIEW_NAME,\n",
    "            gpon_copper_view=GPON_COPPER_VIEW_NAME,\n",
    "            clckstrm_telus_view=CLCKSTRM_TELUS_VIEW_NAME, \n",
    "            alarmdotcom_app_usage_view=ALARMDOTCOM_APP_USAGE_VIEW_NAME, \n",
    "            tos_active_bans_view=TOS_ACTIVE_BANS_VIEW_NAME, \n",
    "            save_data_path='gs://{}/{}/{}_score.csv.gz'.format(FILE_BUCKET, SERVICE_TYPE, SERVICE_TYPE),\n",
    "            project_id=PROJECT_ID,\n",
    "            dataset_id=DATASET_ID\n",
    "        )\n",
    "\n",
    "        preprocess_op.set_memory_limit('128G')\n",
    "        preprocess_op.set_cpu_limit('32')\n",
    "\n",
    "        preprocess_op.after(create_input_account_consl_train_view_op)\n",
    "        preprocess_op.after(create_input_account_ffh_billing_train_view_op)\n",
    "        preprocess_op.after(create_input_account_hs_usage_train_view_op)\n",
    "        preprocess_op.after(create_input_account_demo_income_train_view_op)\n",
    "        preprocess_op.after(create_input_account_promo_expiry_train_view_op)\n",
    "        preprocess_op.after(create_input_account_gpon_copper_train_view_op)\n",
    "        preprocess_op.after(create_input_account_clckstrm_telus_view_op)\n",
    "        preprocess_op.after(create_input_account_alarmdotcom_app_usage_view_op)\n",
    "        preprocess_op.after(create_input_account_tos_active_bans_view_op)\n",
    "\n",
    "        batch_prediction_op = batch_prediction(\n",
    "            project_id=PROJECT_ID,\n",
    "            dataset_id=DATASET_ID,\n",
    "            file_bucket=FILE_BUCKET,\n",
    "            service_type=SERVICE_TYPE,\n",
    "            score_date_dash=SCORE_DATE_DASH,\n",
    "            score_table='bq_tos_cross_sell_score',\n",
    "        )\n",
    "        batch_prediction_op.set_memory_limit('32G')\n",
    "        batch_prediction_op.set_cpu_limit('4')\n",
    "\n",
    "        batch_prediction_op.after(preprocess_op)\n",
    "\n",
    "        postprocessing_op = postprocess(\n",
    "            project_id=PROJECT_ID,\n",
    "            file_bucket=FILE_BUCKET,\n",
    "            service_type=SERVICE_TYPE,\n",
    "            score_date_dash=SCORE_DATE_DASH,\n",
    "        )\n",
    "        postprocessing_op.set_memory_limit('16G')\n",
    "        postprocessing_op.set_cpu_limit('4')\n",
    "\n",
    "        postprocessing_op.after(batch_prediction_op)\n",
    "\n",
    "    return pipeline\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
