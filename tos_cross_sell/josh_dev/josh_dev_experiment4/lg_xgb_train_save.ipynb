{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735a44c1-ab52-493c-aca1-249b5eda7b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import google\n",
    "from google.oauth2 import credentials\n",
    "from google.oauth2 import service_account\n",
    "from google.oauth2.service_account import Credentials\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# build model\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "SERVICE_TYPE = 'tos_crosssell'\n",
    "DATASET_ID = 'tos_crosssell'\n",
    "PROJECT_ID = 'divg-josh-pr-d1cc3a' #mapping['PROJECT_ID']\n",
    "RESOURCE_BUCKET = 'divg-josh-pr-d1cc3a-default' #mapping['resources_bucket']\n",
    "FILE_BUCKET = 'divg-josh-pr-d1cc3a-default' #mapping['gcs_csv_bucket']\n",
    "REGION = 'northamerica-northeast1' #mapping['REGION']\n",
    "MODEL_ID = '9999'\n",
    "FOLDER_NAME = 'xgb_tos_cross_sell_train_deploy'.format(MODEL_ID)\n",
    "QUERIES_PATH = 'vertex_pipelines/' + FOLDER_NAME + '/queries/'\n",
    "\n",
    "scoringDate = date(2022, 6, 30)  # date.today() - relativedelta(days=2)- relativedelta(months=30)\n",
    "valScoringDate = date(2022, 9, 30)  # scoringDate - relativedelta(days=2)\n",
    "\n",
    "# training views\n",
    "# training views\n",
    "CONSL_VIEW_NAME = '{}_pipeline_consl_data_training_bi_layer'.format(SERVICE_TYPE)  # done\n",
    "FFH_BILLING_VIEW_NAME = '{}_pipeline_ffh_billing_data_training_bi_layer'.format(SERVICE_TYPE)  # done\n",
    "HS_USAGE_VIEW_NAME = '{}_pipeline_hs_usage_data_training_bi_layer'.format(SERVICE_TYPE)  # done\n",
    "DEMO_INCOME_VIEW_NAME = '{}_pipeline_demo_income_data_training_bi_layer'.format(SERVICE_TYPE)  # done\n",
    "PROMO_EXPIRY_VIEW_NAME = '{}_pipeline_promo_expiry_data_training_bi_layer'.format(SERVICE_TYPE)  # done\n",
    "TROUBLE_TICKETS_VIEW_NAME = '{}_pipeline_trouble_tickets_data_training_bi_layer'.format(SERVICE_TYPE)  # done\n",
    "GPON_COPPER_VIEW_NAME = '{}_pipeline_gpon_copper_data_training_bi_layer'.format(SERVICE_TYPE)  # done\n",
    "CALL_DATA_VIEW_NAME = '{}_pipeline_call_data_training_bi_layer'.format(SERVICE_TYPE)  # done\n",
    "HSIA_DROPS_VIEW_NAME = '{}_pipeline_hsia_drops_training_bi_layer'.format(SERVICE_TYPE)\n",
    "\n",
    "# validation views\n",
    "CONSL_VIEW_VALIDATION_NAME = '{}_pipeline_consl_data_validation_bi_layer'.format(SERVICE_TYPE)\n",
    "FFH_BILLING_VIEW_VALIDATION_NAME = '{}_pipeline_ffh_billing_data_validation_bi_layer'.format(SERVICE_TYPE)\n",
    "HS_USAGE_VIEW_VALIDATION_NAME = '{}_pipeline_hs_usage_data_validation_bi_layer'.format(SERVICE_TYPE)\n",
    "DEMO_INCOME_VIEW_VALIDATION_NAME = '{}_pipeline_demo_income_data_validation_bi_layer'.format(SERVICE_TYPE)\n",
    "PROMO_EXPIRY_VIEW_VALIDATION_NAME = '{}_pipeline_promo_expiry_data_validation_bi_layer'.format(SERVICE_TYPE)\n",
    "TROUBLE_TICKETS_VIEW_VALIDATION_NAME = '{}_pipeline_trouble_tickets_data_validation_bi_layer'.format(SERVICE_TYPE)\n",
    "GPON_COPPER_VIEW_VALIDATION_NAME = '{}_pipeline_gpon_copper_data_validation_bi_layer'.format(SERVICE_TYPE)\n",
    "CALL_DATA_VIEW_VALIDATION_NAME = '{}_pipeline_call_data_validation_bi_layer'.format(SERVICE_TYPE)\n",
    "HSIA_DROPS_VIEW_VALIDATION_NAME = '{}_pipeline_hsia_drops_validation_bi_layer'.format(SERVICE_TYPE)\n",
    "\n",
    "# training dates\n",
    "SCORE_DATE = scoringDate.strftime('%Y%m%d')  # date.today().strftime('%Y%m%d')\n",
    "SCORE_DATE_DASH = scoringDate.strftime('%Y-%m-%d')\n",
    "SCORE_DATE_MINUS_6_MOS_DASH = ((scoringDate - relativedelta(months=6)).replace(day=1)).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_LAST_MONTH_END_DASH = ((scoringDate.replace(day=1)) - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_LAST_MONTH_YEAR = ((scoringDate.replace(day=1)) - timedelta(days=1)).year\n",
    "SCORE_DATE_LAST_MONTH_MONTH = ((scoringDate.replace(day=1)) - timedelta(days=1)).month\n",
    "\n",
    "# validation dates\n",
    "SCORE_DATE_VAL = valScoringDate.strftime('%Y%m%d')\n",
    "\n",
    "SCORE_DATE_VAL_DASH = valScoringDate.strftime('%Y-%m-%d')\n",
    "SCORE_DATE_VAL_MINUS_6_MOS_DASH = ((valScoringDate - relativedelta(months=6)).replace(day=1)).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_VAL_LAST_MONTH_END_DASH = ((valScoringDate.replace(day=1)) - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_VAL_LAST_MONTH_YEAR = ((valScoringDate.replace(day=1)) - timedelta(days=1)).year\n",
    "SCORE_DATE_VAL_LAST_MONTH_MONTH = ((valScoringDate.replace(day=1)) - timedelta(days=1)).month\n",
    "\n",
    "SCORE_DATE_DELTA = 0\n",
    "SCORE_DATE_VAL_DELTA = 0\n",
    "TICKET_DATE_WINDOW = 30  # Days of ticket data to be queried\n",
    "\n",
    "ACCOUNT_CONSL_QUERY_PATH = QUERIES_PATH + 'create_input_account_consl_query.txt'\n",
    "ACCOUNT_HSIA_DROPS_QUERY_PATH = QUERIES_PATH + 'create_input_account_hsia_drops_query.txt'\n",
    "ACCOUNT_CALL_DATA_QUERY_PATH = QUERIES_PATH + 'create_input_account_call_data_query.txt'\n",
    "ACCOUNT_GPON_COPPER_QUERY_PATH = QUERIES_PATH + 'create_input_account_gpon_copper_query.txt'\n",
    "ACCOUNT_TROUBLE_TICKETS_QUERY_PATH = QUERIES_PATH + 'create_input_account_trouble_tickets_query.txt'\n",
    "ACCOUNT_PROMO_EXPIRY_QUERY_PATH = QUERIES_PATH + 'create_input_account_promo_expiry_query.txt'\n",
    "# ACCOUNT_TV_USAGE_QUERY_PATH = QUERIES_PATH + 'create_input_account_tv_usage_query.txt'\n",
    "ACCOUNT_DEMO_INCOME_QUERY_PATH = QUERIES_PATH + 'create_input_account_demo_income_query.txt'\n",
    "ACCOUNT_HS_USAGE_QUERY_PATH = QUERIES_PATH + 'create_input_account_hs_usage_query.txt'\n",
    "ACCOUNT_FFH_BILLING_QUERY_PATH = QUERIES_PATH + 'create_input_account_ffh_billing_query.txt'\n",
    "\n",
    "def train_and_save_model(\n",
    "            file_bucket: str,\n",
    "            service_type: str,\n",
    "            score_date_dash: str,\n",
    "            score_date_val_dash: str,\n",
    "            project_id: str,\n",
    "            dataset_id: str,\n",
    "            # metrics: Output[Metrics],\n",
    "            # metricsc: Output[ClassificationMetrics],\n",
    "    ):\n",
    "\n",
    "        import gc\n",
    "        import time\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import pickle\n",
    "        from google.cloud import storage\n",
    "        from google.cloud import bigquery\n",
    "        from sklearn.model_selection import train_test_split\n",
    "\n",
    "        def get_lift(prob, y_test, q):\n",
    "            result = pd.DataFrame(columns=['Prob', 'Churn'])\n",
    "            result['Prob'] = prob\n",
    "            result['Churn'] = y_test\n",
    "            # result['Decile'] = pd.qcut(1-result['Prob'], 10, labels = False)\n",
    "            result['Decile'] = pd.qcut(result['Prob'], q, labels=[i for i in range(q, 0, -1)])\n",
    "            add = pd.DataFrame(result.groupby('Decile')['Churn'].mean()).reset_index()\n",
    "            add.columns = ['Decile', 'avg_real_churn_rate']\n",
    "            result = result.merge(add, on='Decile', how='left')\n",
    "            result.sort_values('Decile', ascending=True, inplace=True)\n",
    "            lg = pd.DataFrame(result.groupby('Decile')['Prob'].mean()).reset_index()\n",
    "            lg.columns = ['Decile', 'avg_model_pred_churn_rate']\n",
    "            lg.sort_values('Decile', ascending=False, inplace=True)\n",
    "            lg['avg_churn_rate_total'] = result['Churn'].mean()\n",
    "            lg = lg.merge(add, on='Decile', how='left')\n",
    "            lg['lift'] = lg['avg_real_churn_rate'] / lg['avg_churn_rate_total']\n",
    "\n",
    "            return lg\n",
    "\n",
    "        df_train = pd.read_csv('gs://{}/{}_train.csv.gz'.format(file_bucket, service_type),\n",
    "                               compression='gzip')  # for 2022-08-01\n",
    "        df_test = pd.read_csv('gs://{}/{}_validation.csv.gz'.format(file_bucket, service_type),  # 2022-09-01\n",
    "                              compression='gzip')\n",
    "\n",
    "        sql_train = ''' SELECT * FROM `{}.{}.bq_tos_cross_sell_targets_q3` '''.format(project_id, dataset_id)\n",
    "        sql_test = ''' SELECT * FROM `{}.{}.bq_tos_cross_sell_targets_q4` '''.format(project_id, dataset_id)\n",
    "\n",
    "        def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "            token = os.popen('gcloud auth print-access-token').read()\n",
    "            token = re.sub(f'\\n$', '', token)\n",
    "            credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "            bq_client = bigquery.Client(project=project_id)\n",
    "            if use_local_credential:\n",
    "                bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "            return bq_client\n",
    "        \n",
    "        client = get_gcp_bqclient(project_id)\n",
    "        df_target_train = client.query(sql_train).to_dataframe()\n",
    "        df_target_train = df_target_train.loc[\n",
    "            df_target_train['YEAR_MONTH'] == \"2022-Q3\"]  #'-'.join(score_date_dash.split('-')[:2])]  # score_date_dash = '2022-08-31'\n",
    "\n",
    "        df_target_test = client.query(sql_test).to_dataframe()\n",
    "        df_target_test = df_target_test.loc[\n",
    "            df_target_test['YEAR_MONTH'] == \"2022-Q4\"]  #'-'.join(score_date_val_dash.split('-')[:2])]  # score_date_dash = '2022-09-30'\n",
    "\n",
    "        df_target_test['ban'] = df_target_test['ban'].astype('int64')\n",
    "        df_target_test = df_target_test.groupby('ban').tail(1)\n",
    "        \n",
    "        df_test = df_test.merge(df_target_test[['ban', 'product_crosssell_ind']], on='ban', how='left')\n",
    "        df_test.rename(columns={'product_crosssell_ind': 'target'}, inplace=True)\n",
    "        df_test.dropna(subset=['target'], inplace=True)\n",
    "        df_test['target'] = df_test['target'].astype(int)\n",
    "\n",
    "        df_target_train['ban'] = df_target_train['ban'].astype('int64')\n",
    "        df_target_train = df_target_train.groupby('ban').tail(1)\n",
    "        \n",
    "        df_train = df_train.merge(df_target_train[['ban', 'product_crosssell_ind']], on='ban', how='left')\n",
    "        df_train.rename(columns={'product_crosssell_ind': 'target'}, inplace=True)\n",
    "        df_train.dropna(subset=['target'], inplace=True)\n",
    "        df_train['target'] = df_train['target'].astype(int)\n",
    "\n",
    "        cols_1 = df_train.columns.values\n",
    "        cols_2 = df_test.columns.values\n",
    "        cols = set(cols_1).intersection(set(cols_2))\n",
    "        features = [f for f in cols if f not in ['ban', 'target']]\n",
    "\n",
    "        df_train, df_val = train_test_split(df_train, shuffle=True, test_size=0.3, random_state=42,\n",
    "                                            stratify=df_train['target']\n",
    "                                            )\n",
    "        ban_train = df_train['ban']\n",
    "        X_train = df_train[features]\n",
    "        y_train = np.squeeze(df_train['target'].values)\n",
    "        \n",
    "        ban_val = df_val['ban']\n",
    "        X_val = df_val[features]\n",
    "        y_val = np.squeeze(df_val['target'].values)\n",
    "\n",
    "        ban_test = df_test['ban']\n",
    "        X_test = df_test[features]\n",
    "        y_test = np.squeeze(df_test['target'].values)\n",
    "\n",
    "        del df_train, df_val, df_test\n",
    "        gc.collect()\n",
    "\n",
    "        # build model\n",
    "        import xgboost as xgb\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "\n",
    "        xgb_model = xgb.XGBClassifier(\n",
    "            learning_rate=0.02,\n",
    "            n_estimators=1000,\n",
    "            max_depth=10,\n",
    "            min_child_weight=1,\n",
    "            gamma=0,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            objective='binary:logistic',\n",
    "            nthread=4,\n",
    "            scale_pos_weight=1,\n",
    "            seed=27\n",
    "        )\n",
    "\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        print(' xgb training done ')\n",
    "\n",
    "        y_pred = xgb_model.predict_proba(X_val, ntree_limit=xgb_model.best_iteration)[:, 1]\n",
    "        y_pred_label = (y_pred > 0.5).astype(int)\n",
    "        auc = roc_auc_score(y_val, y_pred_label)\n",
    "        # metrics.log_metric(\"AUC\", auc)\n",
    "\n",
    "        pred_prb = xgb_model.predict_proba(X_test, ntree_limit=xgb_model.best_iteration)[:, 1]\n",
    "        \n",
    "        #join ban, X_test and pred_prb and print to csv\n",
    "        \n",
    "        lg = get_lift(pred_prb, y_test, 10)\n",
    "\n",
    "        # save the model in GCS\n",
    "        from datetime import datetime\n",
    "        models_dict = {}\n",
    "        create_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        models_dict['create_time'] = create_time\n",
    "        models_dict['model'] = xgb_model\n",
    "        models_dict['features'] = features\n",
    "        lg.to_csv('gs://{}/lift_on_scoring_data_{}.csv'.format(file_bucket, create_time, index=False))\n",
    "        print(\"....lift_to_csv done\")\n",
    "\n",
    "        with open('model_dict.pkl', 'wb') as handle:\n",
    "            pickle.dump(models_dict, handle)\n",
    "        handle.close()\n",
    "\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(file_bucket)\n",
    "\n",
    "        MODEL_PATH = '{}_xgb_models/'.format(service_type)\n",
    "        blob = bucket.blob(MODEL_PATH)\n",
    "        if not blob.exists(storage_client):\n",
    "            blob.upload_from_string('')\n",
    "\n",
    "        model_name_onbkt = '{}{}_models_xgb_{}'.format(MODEL_PATH, service_type, models_dict['create_time'])\n",
    "        blob = bucket.blob(model_name_onbkt)\n",
    "        blob.upload_from_filename('model_dict.pkl')\n",
    "        time.sleep(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff55ff0-0398-4c08-b54a-d1260b2cb5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(\n",
    "        project_id: str = PROJECT_ID,\n",
    "        region: str = REGION,\n",
    "        resource_bucket: str = RESOURCE_BUCKET,\n",
    "        file_bucket: str = FILE_BUCKET,\n",
    "):\n",
    "\n",
    "    train_and_save_model_op = train_and_save_model(file_bucket=FILE_BUCKET,\n",
    "                                                       service_type=SERVICE_TYPE,\n",
    "                                                       score_date_dash=SCORE_DATE_DASH,\n",
    "                                                       score_date_val_dash=SCORE_DATE_VAL_DASH,\n",
    "                                                       project_id=PROJECT_ID,\n",
    "                                                       dataset_id=DATASET_ID,\n",
    "                                                       )\n",
    "    \n",
    "    train_and_save_model_op\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e012d54-6850-4cd3-a30d-167f00bb0adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(project_id = PROJECT_ID, region = REGION, resource_bucket = RESOURCE_BUCKET, file_bucket = FILE_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c390f0-aa89-4d6f-89ac-39f98c7a7844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b2d583-51fb-4b76-afd9-ab856b9f6bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757998a7-f860-4855-8300-7d45befc3245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a770a00-cb34-41ae-a609-b9a2cbc5671f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7a09f1-0cf4-4701-9a90-a7799bf9d249",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d344f507-6c0e-4e30-845a-9862ea9f56aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(\n",
    "        project_id: str = PROJECT_ID,\n",
    "        region: str = REGION,\n",
    "        resource_bucket: str = RESOURCE_BUCKET,\n",
    "        file_bucket: str = FILE_BUCKET,\n",
    "):\n",
    "\n",
    "    train_and_save_model_op = train_and_save_model(file_bucket=FILE_BUCKET,\n",
    "                                                       service_type=SERVICE_TYPE,\n",
    "                                                       score_date_dash=SCORE_DATE_DASH,\n",
    "                                                       score_date_val_dash=SCORE_DATE_VAL_DASH,\n",
    "                                                       project_id=PROJECT_ID,\n",
    "                                                       dataset_id=DATASET_ID,\n",
    "                                                       )\n",
    "    \n",
    "    train_and_save_model_op\n",
    "\n",
    "    return pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
