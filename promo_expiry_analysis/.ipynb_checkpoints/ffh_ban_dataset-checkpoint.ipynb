{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4cd1cdb-60e0-44e4-80cb-342ebc873624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kfp import dsl\n",
    "# from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output, OutputPath, ClassificationMetrics,\n",
    "#                         Metrics, component)\n",
    "import os\n",
    "import re\n",
    "import google\n",
    "from google.oauth2 import credentials\n",
    "from google.oauth2 import service_account\n",
    "from google.oauth2.service_account import Credentials\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "SERVICE_TYPE = 'tos_cross_sell'\n",
    "DATASET_ID = 'tos_cross_sell'\n",
    "PROJECT_ID = 'divg-josh-pr-d1cc3a' #mapping['PROJECT_ID']\n",
    "RESOURCE_BUCKET = 'divg-josh-pr-d1cc3a-default' #mapping['resources_bucket']\n",
    "FILE_BUCKET = 'divg-josh-pr-d1cc3a-default' #mapping['gcs_csv_bucket']\n",
    "REGION = 'northamerica-northeast1' #mapping['REGION']\n",
    "MODEL_ID = '5060'\n",
    "FOLDER_NAME = 'xgb_tos_cross_sell_train_deploy'.format(MODEL_ID)\n",
    "QUERIES_PATH = 'queries/'\n",
    "\n",
    "FILENAME = \"ffh_ban_dataset_feb_2023\" \n",
    "\n",
    "scoringDate = date(2023, 2, 1)  # date.today() - relativedelta(days=2)- relativedelta(months=30)\n",
    "valScoringDate = date(2023, 3, 1)  # scoringDate - relativedelta(days=2)\n",
    "\n",
    "# training views\n",
    "CONSL_VIEW_NAME = '{}_pipeline_consl_data_training_bi_layer'.format(SERVICE_TYPE)  # done\n",
    "FFH_BILLING_VIEW_NAME = '{}_pipeline_ffh_billing_data_training_bi_layer'.format(SERVICE_TYPE)  # done\n",
    "HS_USAGE_VIEW_NAME = '{}_pipeline_hs_usage_data_training_bi_layer'.format(SERVICE_TYPE)  # done\n",
    "DEMO_INCOME_VIEW_NAME = '{}_pipeline_demo_income_data_training_bi_layer'.format(SERVICE_TYPE)  # done\n",
    "PROMO_EXPIRY_VIEW_NAME = '{}_pipeline_promo_expiry_data_training_bi_layer'.format(SERVICE_TYPE)  # done\n",
    "TROUBLE_TICKETS_VIEW_NAME = '{}_pipeline_trouble_tickets_data_training_bi_layer'.format(SERVICE_TYPE)  # done\n",
    "GPON_COPPER_VIEW_NAME = '{}_pipeline_gpon_copper_data_training_bi_layer'.format(SERVICE_TYPE)  # done\n",
    "CALL_DATA_VIEW_NAME = '{}_pipeline_call_data_training_bi_layer'.format(SERVICE_TYPE)  # done\n",
    "HSIA_DROPS_VIEW_NAME = '{}_pipeline_hsia_drops_training_bi_layer'.format(SERVICE_TYPE)\n",
    "CLCKSTRM_TELUS_VIEW_NAME = '{}_pipeline_clckstrm_telus_training_bi_layer'.format(SERVICE_TYPE)\n",
    "ALARMDOTCOM_APP_USAGE_VIEW_NAME = '{}_pipeline_alarmdotcom_app_usage_training_bi_layer'.format(SERVICE_TYPE)\n",
    "TOS_ACTIVE_BANS_VIEW_NAME = '{}_pipeline_tos_active_bans_training_bi_layer'.format(SERVICE_TYPE) \n",
    "\n",
    "# validation views\n",
    "CONSL_VIEW_VALIDATION_NAME = '{}_pipeline_consl_data_validation_bi_layer'.format(SERVICE_TYPE)\n",
    "FFH_BILLING_VIEW_VALIDATION_NAME = '{}_pipeline_ffh_billing_data_validation_bi_layer'.format(SERVICE_TYPE)\n",
    "HS_USAGE_VIEW_VALIDATION_NAME = '{}_pipeline_hs_usage_data_validation_bi_layer'.format(SERVICE_TYPE)\n",
    "DEMO_INCOME_VIEW_VALIDATION_NAME = '{}_pipeline_demo_income_data_validation_bi_layer'.format(SERVICE_TYPE)\n",
    "PROMO_EXPIRY_VIEW_VALIDATION_NAME = '{}_pipeline_promo_expiry_data_validation_bi_layer'.format(SERVICE_TYPE)\n",
    "TROUBLE_TICKETS_VIEW_VALIDATION_NAME = '{}_pipeline_trouble_tickets_data_validation_bi_layer'.format(SERVICE_TYPE)\n",
    "GPON_COPPER_VIEW_VALIDATION_NAME = '{}_pipeline_gpon_copper_data_validation_bi_layer'.format(SERVICE_TYPE)\n",
    "CALL_DATA_VIEW_VALIDATION_NAME = '{}_pipeline_call_data_validation_bi_layer'.format(SERVICE_TYPE)\n",
    "HSIA_DROPS_VIEW_VALIDATION_NAME = '{}_pipeline_hsia_drops_validation_bi_layer'.format(SERVICE_TYPE)\n",
    "CLCKSTRM_TELUS_VIEW_VALIDATION_NAME = '{}_pipeline_clckstrm_telus_validation_bi_layer'.format(SERVICE_TYPE)\n",
    "ALARMDOTCOM_APP_USAGE_VIEW_VALIDATION_NAME = '{}_pipeline_alarmdotcom_app_usage_validation_bi_layer'.format(SERVICE_TYPE)\n",
    "TOS_ACTIVE_BANS_VALIDATION_VIEW_NAME = '{}_pipeline_tos_active_bans_validation_bi_layer'.format(SERVICE_TYPE) \n",
    "\n",
    "# training dates\n",
    "SCORE_DATE = scoringDate.strftime('%Y%m%d')  # date.today().strftime('%Y%m%d')\n",
    "SCORE_DATE_DASH = scoringDate.strftime('%Y-%m-%d')\n",
    "SCORE_DATE_MINUS_6_MOS_DASH = ((scoringDate - relativedelta(months=6)).replace(day=1)).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_LAST_MONTH_START_DASH = (scoringDate.replace(day=1) - timedelta(days=1)).replace(day=1).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_LAST_MONTH_END_DASH = ((scoringDate.replace(day=1)) - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_LAST_MONTH_YEAR = ((scoringDate.replace(day=1)) - timedelta(days=1)).year\n",
    "SCORE_DATE_LAST_MONTH_MONTH = ((scoringDate.replace(day=1)) - timedelta(days=1)).month\n",
    "\n",
    "# validation dates\n",
    "SCORE_DATE_VAL = valScoringDate.strftime('%Y%m%d')\n",
    "SCORE_DATE_VAL_DASH = valScoringDate.strftime('%Y-%m-%d')\n",
    "SCORE_DATE_VAL_MINUS_6_MOS_DASH = ((valScoringDate - relativedelta(months=6)).replace(day=1)).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_VAL_LAST_MONTH_START_DASH = (valScoringDate.replace(day=1) - timedelta(days=1)).replace(day=1).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_VAL_LAST_MONTH_END_DASH = ((valScoringDate.replace(day=1)) - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_VAL_LAST_MONTH_YEAR = ((valScoringDate.replace(day=1)) - timedelta(days=1)).year\n",
    "SCORE_DATE_VAL_LAST_MONTH_MONTH = ((valScoringDate.replace(day=1)) - timedelta(days=1)).month\n",
    "\n",
    "SCORE_DATE_DELTA = 0\n",
    "SCORE_DATE_VAL_DELTA = 0\n",
    "TICKET_DATE_WINDOW = 30  # Days of ticket data to be queried\n",
    "\n",
    "ACCOUNT_CONSL_QUERY_PATH = QUERIES_PATH + 'create_input_account_consl_query.txt'\n",
    "ACCOUNT_HSIA_DROPS_QUERY_PATH = QUERIES_PATH + 'create_input_account_hsia_drops_query.txt'\n",
    "ACCOUNT_CALL_DATA_QUERY_PATH = QUERIES_PATH + 'create_input_account_call_data_query.txt'\n",
    "ACCOUNT_GPON_COPPER_QUERY_PATH = QUERIES_PATH + 'create_input_account_gpon_copper_query.txt'\n",
    "ACCOUNT_TROUBLE_TICKETS_QUERY_PATH = QUERIES_PATH + 'create_input_account_trouble_tickets_query.txt'\n",
    "ACCOUNT_PROMO_EXPIRY_QUERY_PATH = QUERIES_PATH + 'create_input_account_promo_expiry_query.txt'\n",
    "# ACCOUNT_TV_USAGE_QUERY_PATH = QUERIES_PATH + 'create_input_account_tv_usage_query.txt'\n",
    "ACCOUNT_DEMO_INCOME_QUERY_PATH = QUERIES_PATH + 'create_input_account_demo_income_query.txt'\n",
    "ACCOUNT_HS_USAGE_QUERY_PATH = QUERIES_PATH + 'create_input_account_hs_usage_query.txt'\n",
    "ACCOUNT_FFH_BILLING_QUERY_PATH = QUERIES_PATH + 'create_input_account_ffh_billing_query.txt'\n",
    "ACCOUNT_CLCKSTRM_TELUS_QUERY_PATH = QUERIES_PATH + 'create_input_account_clckstrm_telus_query.txt'\n",
    "ACCOUNT_ALARMDOTCOM_APP_USAGE_QUERY_PATH = QUERIES_PATH + 'create_input_account_alarmdotcom_app_usage_query.txt'\n",
    "ACCOUNT_TOS_ACTIVE_BANS_QUERY_PATH = QUERIES_PATH + 'create_input_account_tos_active_bans_query.txt'\n",
    "\n",
    "def create_input_account_consl_view(view_name: str,\n",
    "                                    score_date: str,\n",
    "                                    score_date_delta: str,\n",
    "                                    project_id: str,\n",
    "                                    dataset_id: str,\n",
    "                                    region: str,\n",
    "                                    resource_bucket: str,\n",
    "                                    query_path: str,\n",
    "                                    ):\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "\n",
    "    def if_tbl_exists(client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    bq_client = get_gcp_bqclient(project_id)\n",
    "    dataset = bq_client.dataset(dataset_id)\n",
    "    table_ref = dataset.table(view_name)\n",
    "\n",
    "    # load query from .txt file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(resource_bucket)\n",
    "    blob = bucket.get_blob(query_path)\n",
    "    content = blob.download_as_string()\n",
    "    content = str(content, 'utf-8')\n",
    "\n",
    "    if if_tbl_exists(bq_client, table_ref):\n",
    "        bq_client.delete_table(table_ref)\n",
    "\n",
    "    # content = open(query_path, 'r').read()\n",
    "\n",
    "    create_base_feature_set_query = content.format(score_date=score_date,\n",
    "                                                   score_date_delta=score_date_delta,\n",
    "                                                   view_name=view_name,\n",
    "                                                   dataset_id=dataset_id,\n",
    "                                                   project_id=project_id,\n",
    "                                                   )\n",
    "    shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "    base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "    base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "    base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "    base_feature_set_view = bq_client.create_table(base_feature_set_view)\n",
    "\n",
    "def create_input_account_ffh_billing_view(view_name: str,\n",
    "                                              v_report_date: str,\n",
    "                                              v_start_date: str,\n",
    "                                              v_end_date: str,\n",
    "                                              v_bill_year: str,\n",
    "                                              v_bill_month: str,\n",
    "                                              dataset_id: str,\n",
    "                                              project_id: str,\n",
    "                                              region: str,\n",
    "                                              resource_bucket: str,\n",
    "                                              query_path: str\n",
    "                                              ):\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    bq_client = get_gcp_bqclient(project_id)\n",
    "    dataset = bq_client.dataset(dataset_id)\n",
    "    table_ref = dataset.table(view_name)\n",
    "\n",
    "    # load query from .txt file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(resource_bucket)\n",
    "    blob = bucket.get_blob(query_path)\n",
    "    content = blob.download_as_string()\n",
    "    content = str(content, 'utf-8')\n",
    "\n",
    "    def if_tbl_exists(client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "\n",
    "    if if_tbl_exists(bq_client, table_ref):\n",
    "        bq_client.delete_table(table_ref)\n",
    "\n",
    "    create_base_feature_set_query = content.format(v_report_date=v_report_date,\n",
    "                                                   v_start_date=v_start_date,\n",
    "                                                   v_end_date=v_end_date,\n",
    "                                                   v_bill_year=v_bill_year,\n",
    "                                                   v_bill_month=v_bill_month,\n",
    "                                                   )\n",
    "\n",
    "    shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "    base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "    base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "    base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "    base_feature_set_view = bq_client.create_table(base_feature_set_view)\n",
    "\n",
    "def create_input_account_hs_usage_view(view_name: str,\n",
    "                                       v_report_date: str,\n",
    "                                       v_start_date: str,\n",
    "                                       v_end_date: str,\n",
    "                                       v_bill_year: str,\n",
    "                                       v_bill_month: str,\n",
    "                                       dataset_id: str,\n",
    "                                       project_id: str,\n",
    "                                       region: str,\n",
    "                                       resource_bucket: str,\n",
    "                                       query_path: str\n",
    "                                       ):\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    bq_client = get_gcp_bqclient(project_id)\n",
    "    dataset = bq_client.dataset(dataset_id)\n",
    "    table_ref = dataset.table(view_name)\n",
    "\n",
    "    # load query from .txt file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(resource_bucket)\n",
    "    blob = bucket.get_blob(query_path)\n",
    "    content = blob.download_as_string()\n",
    "    content = str(content, 'utf-8')\n",
    "\n",
    "    def if_tbl_exists(client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "\n",
    "    if if_tbl_exists(bq_client, table_ref):\n",
    "        bq_client.delete_table(table_ref)\n",
    "\n",
    "    create_base_feature_set_query = content.format(v_report_date=v_report_date,\n",
    "                                                   v_start_date=v_start_date,\n",
    "                                                   v_end_date=v_end_date,\n",
    "                                                   v_bill_year=v_bill_year,\n",
    "                                                   v_bill_month=v_bill_month,\n",
    "                                                   )\n",
    "\n",
    "    shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "    base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "    base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "    base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "    base_feature_set_view = bq_client.create_table(base_feature_set_view)\n",
    "\n",
    "def create_input_account_demo_income_view(view_name: str,\n",
    "                                          score_date: str,\n",
    "                                          score_date_delta: str,\n",
    "                                          dataset_id: str,\n",
    "                                          project_id: str,\n",
    "                                          region: str,\n",
    "                                          resource_bucket: str,\n",
    "                                          query_path: str\n",
    "                                          ):\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    bq_client = get_gcp_bqclient(project_id)\n",
    "    dataset = bq_client.dataset(dataset_id)\n",
    "    table_ref = dataset.table(view_name)\n",
    "\n",
    "    # load query from .txt file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(resource_bucket)\n",
    "    blob = bucket.get_blob(query_path)\n",
    "    content = blob.download_as_string()\n",
    "    content = str(content, 'utf-8')\n",
    "\n",
    "    def if_tbl_exists(client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "\n",
    "    if if_tbl_exists(bq_client, table_ref):\n",
    "        bq_client.delete_table(table_ref)\n",
    "\n",
    "    create_base_feature_set_query = content.format(score_date=score_date,\n",
    "                                                   score_date_delta=score_date_delta,\n",
    "                                                   project_id=project_id,\n",
    "                                                   dataset_id='common_dataset',\n",
    "                                                   )\n",
    "\n",
    "    shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "    base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "    base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "    base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "    base_feature_set_view = bq_client.create_table(base_feature_set_view)\n",
    "\n",
    "def create_input_account_promo_expiry_view(view_name: str,\n",
    "                                           score_date: str,\n",
    "                                           dataset_id: str,\n",
    "                                           project_id: str,\n",
    "                                           region: str,\n",
    "                                           resource_bucket: str,\n",
    "                                           query_path: str\n",
    "                                           ):\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    bq_client = get_gcp_bqclient(project_id)\n",
    "    dataset = bq_client.dataset(dataset_id)\n",
    "    table_ref = dataset.table(view_name)\n",
    "\n",
    "    # load query from .txt file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(resource_bucket)\n",
    "    blob = bucket.get_blob(query_path)\n",
    "    content = blob.download_as_string()\n",
    "    content = str(content, 'utf-8')\n",
    "\n",
    "    def if_tbl_exists(client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "\n",
    "    if if_tbl_exists(bq_client, table_ref):\n",
    "        bq_client.delete_table(table_ref)\n",
    "\n",
    "    create_base_feature_set_query = content.format(score_date=score_date)\n",
    "\n",
    "    create_base_feature_set_query = create_base_feature_set_query.replace('{', '{{')\n",
    "    create_base_feature_set_query = create_base_feature_set_query.replace('}', '}}')\n",
    "\n",
    "    shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "    base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "    base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "    base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "    base_feature_set_view = bq_client.create_table(base_feature_set_view)\n",
    "\n",
    "def create_input_account_trouble_tickets_view(view_name: str,\n",
    "                                              score_date: str,\n",
    "                                              score_date_delta: str,\n",
    "                                              trouble_tickets_window: str,\n",
    "                                              dataset_id: str,\n",
    "                                              project_id: str,\n",
    "                                              region: str,\n",
    "                                              resource_bucket: str,\n",
    "                                              query_path: str\n",
    "                                              ):\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    bq_client = get_gcp_bqclient(project_id)\n",
    "    dataset = bq_client.dataset(dataset_id)\n",
    "    table_ref = dataset.table(view_name)\n",
    "\n",
    "    # load query from .txt file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(resource_bucket)\n",
    "    blob = bucket.get_blob(query_path)\n",
    "    content = blob.download_as_string()\n",
    "    content = str(content, 'utf-8')\n",
    "\n",
    "    def if_tbl_exists(client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "\n",
    "    if if_tbl_exists(bq_client, table_ref):\n",
    "        bq_client.delete_table(table_ref)\n",
    "\n",
    "    create_base_feature_set_query = content.format(score_date=score_date,\n",
    "                                                   score_date_delta=score_date_delta,\n",
    "                                                   trouble_tickets_window=trouble_tickets_window\n",
    "                                                   )\n",
    "\n",
    "    shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "    base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "    base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "    base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "    base_feature_set_view = bq_client.create_table(base_feature_set_view)\n",
    "\n",
    "def create_input_account_gpon_copper_view(view_name: str,\n",
    "                                          score_date: str,\n",
    "                                          score_date_delta: str,\n",
    "                                          dataset_id: str,\n",
    "                                          project_id: str,\n",
    "                                          region: str,\n",
    "                                          resource_bucket: str,\n",
    "                                          query_path: str\n",
    "                                          ):\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    bq_client = get_gcp_bqclient(project_id)\n",
    "    dataset = bq_client.dataset(dataset_id)\n",
    "    table_ref = dataset.table(view_name)\n",
    "\n",
    "    # load query from .txt file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(resource_bucket)\n",
    "    blob = bucket.get_blob(query_path)\n",
    "    content = blob.download_as_string()\n",
    "    content = str(content, 'utf-8')\n",
    "\n",
    "    def if_tbl_exists(client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "\n",
    "    if if_tbl_exists(bq_client, table_ref):\n",
    "        bq_client.delete_table(table_ref)\n",
    "\n",
    "    create_base_feature_set_query = content.format(score_date=score_date,\n",
    "                                                   score_date_delta=score_date_delta,\n",
    "                                                   )\n",
    "\n",
    "    shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "    base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "    base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "    base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "    base_feature_set_view = bq_client.create_table(base_feature_set_view)\n",
    "\n",
    "def create_input_account_call_data_view(view_name: str,\n",
    "                                        score_date: str,\n",
    "                                        score_date_delta: str,\n",
    "                                        dataset_id: str,\n",
    "                                        project_id: str,\n",
    "                                        region: str,\n",
    "                                        resource_bucket: str,\n",
    "                                        query_path: str\n",
    "                                        ):\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    bq_client = get_gcp_bqclient(project_id)\n",
    "    dataset = bq_client.dataset(dataset_id)\n",
    "    table_ref = dataset.table(view_name)\n",
    "\n",
    "    # load query from .txt file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(resource_bucket)\n",
    "    blob = bucket.get_blob(query_path)\n",
    "    content = blob.download_as_string()\n",
    "    content = str(content, 'utf-8')\n",
    "\n",
    "    def if_tbl_exists(client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "\n",
    "    if if_tbl_exists(bq_client, table_ref):\n",
    "        bq_client.delete_table(table_ref)\n",
    "\n",
    "    create_base_feature_set_query = content.format(score_date=score_date,\n",
    "                                                   score_date_delta=score_date_delta,\n",
    "                                                   )\n",
    "\n",
    "    shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "    base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "    base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "    base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "    base_feature_set_view = bq_client.create_table(base_feature_set_view)\n",
    "\n",
    "def create_input_account_hsia_drops_view(view_name: str,\n",
    "                                         score_date: str,\n",
    "                                         score_date_delta: str,\n",
    "                                         dataset_id: str,\n",
    "                                         project_id: str,\n",
    "                                         region: str,\n",
    "                                         resource_bucket: str,\n",
    "                                         query_path: str\n",
    "                                         ):\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    bq_client = get_gcp_bqclient(project_id)\n",
    "    dataset = bq_client.dataset(dataset_id)\n",
    "    table_ref = dataset.table(view_name)\n",
    "\n",
    "    # load query from .txt file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(resource_bucket)\n",
    "    blob = bucket.get_blob(query_path)\n",
    "    content = blob.download_as_string()\n",
    "    content = str(content, 'utf-8')\n",
    "\n",
    "    def if_tbl_exists(client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "\n",
    "    if if_tbl_exists(bq_client, table_ref):\n",
    "        bq_client.delete_table(table_ref)\n",
    "\n",
    "    create_base_feature_set_query = content.format(score_date=score_date,\n",
    "                                                   score_date_delta=score_date_delta,\n",
    "                                                   project_id=project_id,\n",
    "                                                   dataset_id='common_dataset',\n",
    "                                                   )\n",
    "\n",
    "    shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "    base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "    base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "    base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "    base_feature_set_view = bq_client.create_table(base_feature_set_view)\n",
    "    \n",
    "def create_input_account_clckstrm_telus_view(view_name: str,\n",
    "                                    score_date: str,\n",
    "                                    score_date_delta: str,\n",
    "                                    project_id: str,\n",
    "                                    dataset_id: str,\n",
    "                                    region: str,\n",
    "                                    resource_bucket: str,\n",
    "                                    query_path: str,\n",
    "                                    ):\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "\n",
    "    def if_tbl_exists(client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    bq_client = get_gcp_bqclient(project_id)\n",
    "    dataset = bq_client.dataset(dataset_id)\n",
    "    table_ref = dataset.table(view_name)\n",
    "\n",
    "    # load query from .txt file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(resource_bucket)\n",
    "    blob = bucket.get_blob(query_path)\n",
    "    content = blob.download_as_string()\n",
    "    content = str(content, 'utf-8')\n",
    "\n",
    "    if if_tbl_exists(bq_client, table_ref):\n",
    "        bq_client.delete_table(table_ref)\n",
    "\n",
    "    # content = open(query_path, 'r').read()\n",
    "\n",
    "    create_base_feature_set_query = content.format(score_date=score_date,\n",
    "                                                   score_date_delta=score_date_delta,\n",
    "                                                   view_name=view_name,\n",
    "                                                   dataset_id=dataset_id,\n",
    "                                                   project_id=project_id,\n",
    "                                                   )\n",
    "    shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "    base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "    base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "    base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "    base_feature_set_view = bq_client.create_table(base_feature_set_view)\n",
    "    \n",
    "def create_input_account_alarmdotcom_app_usage_view(view_name: str,\n",
    "                                    score_date: str,\n",
    "                                    score_date_delta: str,\n",
    "                                    project_id: str,\n",
    "                                    dataset_id: str,\n",
    "                                    region: str,\n",
    "                                    resource_bucket: str,\n",
    "                                    query_path: str,\n",
    "                                    ):\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "\n",
    "    def if_tbl_exists(client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    bq_client = get_gcp_bqclient(project_id)\n",
    "    dataset = bq_client.dataset(dataset_id)\n",
    "    table_ref = dataset.table(view_name)\n",
    "\n",
    "    # load query from .txt file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(resource_bucket)\n",
    "    blob = bucket.get_blob(query_path)\n",
    "    content = blob.download_as_string()\n",
    "    content = str(content, 'utf-8')\n",
    "\n",
    "    if if_tbl_exists(bq_client, table_ref):\n",
    "        bq_client.delete_table(table_ref)\n",
    "\n",
    "    # content = open(query_path, 'r').read()\n",
    "\n",
    "    create_base_feature_set_query = content.format(score_date=score_date,\n",
    "                                                   score_date_delta=score_date_delta,\n",
    "                                                   view_name=view_name,\n",
    "                                                   dataset_id=dataset_id,\n",
    "                                                   project_id=project_id,\n",
    "                                                   )\n",
    "    shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "    base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "    base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "    base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "    base_feature_set_view = bq_client.create_table(base_feature_set_view)\n",
    "\n",
    "def create_input_account_tos_active_bans_view(view_name: str,\n",
    "                                    score_date: str,\n",
    "                                    score_date_delta: str,\n",
    "                                    v_start_date: str,\n",
    "                                    v_end_date: str,\n",
    "                                    project_id: str,\n",
    "                                    dataset_id: str,\n",
    "                                    region: str,\n",
    "                                    resource_bucket: str,\n",
    "                                    query_path: str,\n",
    "                                    ):\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "\n",
    "    def if_tbl_exists(client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    bq_client = get_gcp_bqclient(project_id)\n",
    "    dataset = bq_client.dataset(dataset_id)\n",
    "    table_ref = dataset.table(view_name)\n",
    "\n",
    "    # load query from .txt file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(resource_bucket)\n",
    "    blob = bucket.get_blob(query_path)\n",
    "    content = blob.download_as_string()\n",
    "    content = str(content, 'utf-8')\n",
    "\n",
    "    if if_tbl_exists(bq_client, table_ref):\n",
    "        bq_client.delete_table(table_ref)\n",
    "\n",
    "    # content = open(query_path, 'r').read()\n",
    "\n",
    "    create_base_feature_set_query = content.format(score_date=score_date,\n",
    "                                                   score_date_delta=score_date_delta,\n",
    "                                                   v_start_date=v_start_date,\n",
    "                                                   v_end_date=v_end_date,\n",
    "                                                   view_name=view_name,\n",
    "                                                   dataset_id=dataset_id,\n",
    "                                                   project_id=project_id,\n",
    "                                                   )\n",
    "    shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "    base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "    base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "    base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "    base_feature_set_view = bq_client.create_table(base_feature_set_view)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def preprocess(\n",
    "        account_consl_view: str,\n",
    "        account_bill_view: str,\n",
    "        hs_usage_view: str,\n",
    "        demo_income_view: str,\n",
    "        promo_expiry_view: str,\n",
    "        trouble_tickets_view: str,\n",
    "        gpon_copper_view: str,\n",
    "        call_data_view: str,\n",
    "        hsia_drops_view: str,\n",
    "        clckstrm_telus_view: str, \n",
    "        alarmdotcom_app_usage_view: str, \n",
    "        tos_active_bans_view: str, \n",
    "        save_data_path: str,\n",
    "        project_id: str,\n",
    "        dataset_id: str\n",
    "):\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    import pandas as pd\n",
    "    import gc\n",
    "    import time\n",
    "    \n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    client = get_gcp_bqclient(project_id)\n",
    "\n",
    "    consl_data_set = f\"{project_id}.{dataset_id}.{account_consl_view}\" #`divg-churn-analysis-pr-7e40f6.divg_churn_analysis_pr_dataset.internet_churn_pipeline_consl_data_training_bi_layer`\n",
    "\n",
    "    build_df_consl = '''SELECT * FROM `{consl_data_set}`'''.format(consl_data_set=consl_data_set)\n",
    "    df_consl = client.query(build_df_consl).to_dataframe() #df_consl is the dataframe that contains consolidated input data (dataset #1)\n",
    "    print('......base data done')\n",
    "\n",
    "    # product mix\n",
    "    df_mix = df_consl[[\n",
    "        'ban',\n",
    "        'product_mix_all',\n",
    "        'sing_count',\n",
    "        'hsic_count',\n",
    "        'mob_count',\n",
    "        'shs_count',\n",
    "        'ttv_count',\n",
    "        'stv_count',\n",
    "        'diic_count',\n",
    "        'new_c_ind',\n",
    "        'new_sing_ind',\n",
    "        'new_hsic_ind',\n",
    "        'new_ttv_ind',\n",
    "        'new_smhm_ind',\n",
    "        'mnh_ind'\n",
    "    ]]\n",
    "    df_mix = df_mix.drop_duplicates(subset=['ban']).set_index('ban').add_prefix('productMix_')\n",
    "\n",
    "    # df_join\n",
    "    df_join = df_mix.fillna(0)\n",
    "\n",
    "    del df_mix\n",
    "    gc.collect()\n",
    "    print('......product mix done')\n",
    "\n",
    "    bill_data_set = f\"{project_id}.{dataset_id}.{account_bill_view}\" #`divg-churn-analysis-pr-7e40f6.divg_churn_analysis_pr_dataset.internet_churn_pipeline_ffh_billing_data_training_bi_layer`\n",
    "    build_df_bill = '''SELECT * FROM `{bill_data_set}`'''.format(bill_data_set=bill_data_set)\n",
    "\n",
    "    client = get_gcp_bqclient(project_id)\n",
    "    df_bill = client.query(build_df_bill).to_dataframe() #df_bill is the dataframe that contains billing input data (dataset #2)\n",
    "\n",
    "    df_bill = df_bill.set_index('ban').add_prefix('ffhBill_')\n",
    "\n",
    "    # df_join\n",
    "    df_join = df_join.join(df_bill).fillna(0) #join datasets #1 and #2 = dataset 1+2\n",
    "    del df_bill\n",
    "    gc.collect()\n",
    "    print('......account bill done')\n",
    "\n",
    "    hs_usage_data_set = f\"{project_id}.{dataset_id}.{hs_usage_view}\" #`divg-churn-analysis-pr-7e40f6.divg_churn_analysis_pr_dataset.internet_churn_pipeline_hs_usage_data_training_bi_layer`\n",
    "    build_df_hs_usage = '''SELECT * FROM `{hs_usage_data_set}`'''.format(hs_usage_data_set=hs_usage_data_set)\n",
    "\n",
    "    client = get_gcp_bqclient(project_id)\n",
    "    df_hs_usage = client.query(build_df_hs_usage).to_dataframe() #df_hs_usage is the dataframe that contains internet usage input data (dataset #3)\n",
    "\n",
    "    df_hs_usage = df_hs_usage.set_index('ban').add_prefix('hsiaUsage_')\n",
    "\n",
    "    # df_join\n",
    "    df_join = df_join.join(df_hs_usage).fillna(0) #join datasets#1+2 and #3\n",
    "    del df_hs_usage\n",
    "    gc.collect()\n",
    "    print('......hs usage done')\n",
    "\n",
    "    demo_income_data_set = f\"{project_id}.{dataset_id}.{demo_income_view}\" #`divg-churn-analysis-pr-7e40f6.divg_churn_analysis_pr_dataset.internet_churn_pipeline_demo_income_data_training_bi_layer`\n",
    "    build_df_demo_income = '''SELECT * FROM `{demo_income_data_set}`'''.format(\n",
    "        demo_income_data_set=demo_income_data_set)\n",
    "    \n",
    "    client = get_gcp_bqclient(project_id)\n",
    "    df_income = client.query(build_df_demo_income).to_dataframe()\n",
    "\n",
    "    df_income = df_income.set_index('ban')\n",
    "    df_income['demo_urban_flag'] = df_income.demo_sgname.str.lower().str.contains('urban').fillna(0).astype(int)\n",
    "    df_income['demo_rural_flag'] = df_income.demo_sgname.str.lower().str.contains('rural').fillna(0).astype(int)\n",
    "    df_income['demo_family_flag'] = df_income.demo_lsname.str.lower().str.contains('families').fillna(0).astype(int)\n",
    "    df_income_dummies = pd.get_dummies(df_income[['demo_lsname']])\n",
    "    df_income_dummies.columns = df_income_dummies.columns.str.replace('&', 'and')\n",
    "    df_income_dummies.columns = df_income_dummies.columns.str.replace(' ', '_')\n",
    "    df_income = df_income[['demo_avg_income', 'demo_urban_flag', 'demo_rural_flag', 'demo_family_flag']].join(\n",
    "        df_income_dummies)\n",
    "    df_income.demo_avg_income = df_income.demo_avg_income.astype(float)\n",
    "    df_income.demo_avg_income = df_income.demo_avg_income.fillna(df_income.demo_avg_income.median())\n",
    "    df_group_income = df_income.groupby('ban').agg('mean')\n",
    "    df_group_income = df_group_income.add_prefix('demographics_')\n",
    "    # df_join\n",
    "    df_join = df_join.join(df_group_income.fillna(df_group_income.median()))\n",
    "\n",
    "    del df_group_income\n",
    "    del df_income\n",
    "    gc.collect()\n",
    "    print('......income done')\n",
    "\n",
    "    promo_expiry_data_set = f\"{project_id}.{dataset_id}.{promo_expiry_view}\" #`divg-churn-analysis-pr-7e40f6.divg_churn_analysis_pr_dataset.internet_churn_pipeline_promo_expiry_data_training_bi_layer`\n",
    "    build_df_promo = '''SELECT * FROM `{promo_expiry_data_set}` '''.format(\n",
    "        promo_expiry_data_set=promo_expiry_data_set)\n",
    "    \n",
    "    client = get_gcp_bqclient(project_id)\n",
    "    df_promo = client.query(build_df_promo).to_dataframe()  # Make an API request.\n",
    "\n",
    "    df_promo = df_promo.set_index('ban')\n",
    "    disc_cols = [col for col in df_promo.columns if 'disc' in col]\n",
    "    bill_cols = [col for col in df_promo.columns if 'disc' not in col]\n",
    "\n",
    "    df_join = df_join.join(df_promo[disc_cols].add_prefix('promo_'))\n",
    "    df_join = df_join.join(df_promo[bill_cols].add_prefix('ffhBill_')).fillna(0)\n",
    "\n",
    "    del df_promo\n",
    "    gc.collect()\n",
    "    print('......promo expiry done')\n",
    "\n",
    "#     trouble_tickets_data_set = f\"{project_id}.{dataset_id}.{trouble_tickets_view}\" #`_churn_pipeline_trouble_tickets_data_training_bi_layer`\n",
    "#     build_df_trouble_tickets = '''\n",
    "#     SELECT * FROM `{trouble_tickets_data_set}`\n",
    "#     '''.format(trouble_tickets_data_set=trouble_tickets_data_set)\n",
    "    \n",
    "#     client = get_gcp_bqclient(project_id)\n",
    "#     df_trouble_tickets = client.query(build_df_trouble_tickets).to_dataframe()\n",
    "\n",
    "#     df_trouble_tickets = df_trouble_tickets.set_index('ban')\n",
    "#     df_join = df_join.join(df_trouble_tickets.add_prefix('troubleTickets_')).fillna(0)\n",
    "#     del df_trouble_tickets\n",
    "#     gc.collect()\n",
    "#     print('......trouble tickets done')\n",
    "    \n",
    "    # gpon copper\n",
    "    gpon_copper_data_set = f\"{project_id}.{dataset_id}.{gpon_copper_view}\"\n",
    "    build_df_gpon_copper = '''\n",
    "    SELECT * FROM `{gpon_copper_data_set}` \n",
    "    '''.format(gpon_copper_data_set=gpon_copper_data_set)\n",
    "    \n",
    "    client = get_gcp_bqclient(project_id)\n",
    "    df_gpon_copper = client.query(build_df_gpon_copper).to_dataframe()\n",
    "\n",
    "    df_gpon_copper = df_gpon_copper.set_index('ban')\n",
    "    df_join = df_join.join(df_gpon_copper.add_prefix('infra_')).fillna(0)\n",
    "    del df_gpon_copper\n",
    "    gc.collect()\n",
    "    print('......gpon copper done')\n",
    "    \n",
    "    # call data\n",
    "    call_data_set = f\"{project_id}.{dataset_id}.{call_data_view}\"\n",
    "    build_df_call_data = '''SELECT * FROM `{call_data_set}`'''.format(call_data_set=call_data_set)\n",
    "    \n",
    "    client = get_gcp_bqclient(project_id)\n",
    "    df_call_data = client.query(build_df_call_data).to_dataframe()  # Make an API request.\n",
    "\n",
    "    # df_join\n",
    "    df_call_data = df_call_data.set_index('ban')\n",
    "    df_join = df_join.join(df_call_data.add_prefix('callCentre_')).fillna(0)\n",
    "    del df_call_data\n",
    "    gc.collect()\n",
    "    print('......call data done')\n",
    "    \n",
    "#     # hsia drops\n",
    "#     hsia_drops_data_set = f\"{project_id}.{dataset_id}.{hsia_drops_view}\" #`_churn_pipeline_hsia_drops_training_bi_layer`\n",
    "#     build_df_hsia_drops = '''\n",
    "#     SELECT * FROM `{hsia_drops_data_set}` \n",
    "#     '''.format(hsia_drops_data_set=hsia_drops_data_set)\n",
    "    \n",
    "#     client = get_gcp_bqclient(project_id)\n",
    "#     df_hsia_drops = client.query(build_df_hsia_drops).to_dataframe()\n",
    "\n",
    "#     df_hsia_drops = df_hsia_drops.set_index('ban')\n",
    "#     df_join = df_join.join(df_hsia_drops.add_prefix('hsiaDrops_')).fillna(0)\n",
    "#     del df_hsia_drops\n",
    "#     gc.collect()\n",
    "#     print('......hsia drop done')\n",
    "\n",
    "    # clickstream data\n",
    "    clckstrm_telus_data_set = f\"{project_id}.{dataset_id}.{clckstrm_telus_view}\" \n",
    "    build_df_clckstrm_telus = '''SELECT * FROM `{clckstrm_telus_data_set}`'''.format(clckstrm_telus_data_set=clckstrm_telus_data_set)\n",
    "\n",
    "    client = get_gcp_bqclient(project_id)\n",
    "    df_clckstrm_telus = client.query(build_df_clckstrm_telus).to_dataframe() \n",
    "\n",
    "    df_clckstrm_telus = df_clckstrm_telus.set_index('ban').add_prefix('clckstrmData_')\n",
    "\n",
    "    # df_join\n",
    "    df_join = df_join.join(df_clckstrm_telus).fillna(0) \n",
    "    del df_clckstrm_telus\n",
    "    gc.collect()\n",
    "    print('......clcktsrm data done')\n",
    "    \n",
    "    # alarm.com data\n",
    "    alarmdotcom_app_usage_data_set = f\"{project_id}.{dataset_id}.{alarmdotcom_app_usage_view}\" \n",
    "    build_df_alarmdotcom_app_usage = '''SELECT * FROM `{alarmdotcom_app_usage_data_set}`'''.format(alarmdotcom_app_usage_data_set=alarmdotcom_app_usage_data_set)\n",
    "\n",
    "    client = get_gcp_bqclient(project_id)\n",
    "    df_alarmdotcom_app_usage = client.query(build_df_alarmdotcom_app_usage).to_dataframe() \n",
    "\n",
    "    df_alarmdotcom_app_usage = df_alarmdotcom_app_usage.set_index('ban').add_prefix('alarmdotcomAppUsage_')\n",
    "\n",
    "    # df_join\n",
    "    df_join = df_join.join(df_alarmdotcom_app_usage).fillna(0) \n",
    "    del df_alarmdotcom_app_usage\n",
    "    gc.collect()\n",
    "    print('......alarm.com app usage data done')\n",
    "    \n",
    "    # tos active bans\n",
    "    tos_active_bans = f\"{project_id}.{dataset_id}.{tos_active_bans_view}\" \n",
    "    build_df_tos_active_data = '''SELECT * FROM `{tos_active_bans}`'''.format(tos_active_bans=tos_active_bans)\n",
    "    \n",
    "    client = get_gcp_bqclient(project_id)\n",
    "    df_tos_active_data = client.query(build_df_tos_active_data).to_dataframe()  # Make an API request.\n",
    "\n",
    "    # df_join\n",
    "    df_tos_active_data = df_tos_active_data.set_index('ban')\n",
    "    df_join = df_join.join(df_tos_active_data).fillna(0)\n",
    "    del df_tos_active_data\n",
    "    gc.collect()\n",
    "    print('......tos active data done')\n",
    "    \n",
    "    df_join.columns = df_join.columns.str.replace(' ', '_')\n",
    "    df_join.columns = df_join.columns.str.replace('-', '_')\n",
    "\n",
    "    df_final = df_join.copy()\n",
    "    del df_join\n",
    "    gc.collect()\n",
    "    print('......df final done')\n",
    "\n",
    "    for f in df_final.columns:\n",
    "        df_final[f] = list(df_final[f])\n",
    "\n",
    "#     df_final = df_final.reset_index()\n",
    "    \n",
    "    df_final.to_csv(save_data_path, index=True) \n",
    "    del df_final\n",
    "    gc.collect()\n",
    "    print(f'......csv saved in {save_data_path}')\n",
    "    time.sleep(300)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "def pipeline(\n",
    "            project_id: str = PROJECT_ID,\n",
    "            region: str = REGION,\n",
    "            resource_bucket: str = RESOURCE_BUCKET,\n",
    "            file_bucket: str = FILE_BUCKET\n",
    "    ):\n",
    "        # ------------- train view ops ---------------\n",
    "        #timeframe: snapshot as of scoring date, and look at 30 days for new activations\n",
    "        create_input_account_consl_train_view_op = create_input_account_consl_view(\n",
    "            view_name=CONSL_VIEW_NAME,\n",
    "            score_date=SCORE_DATE,\n",
    "            score_date_delta=SCORE_DATE_DELTA,\n",
    "            project_id=PROJECT_ID,\n",
    "            dataset_id=DATASET_ID,\n",
    "            region=REGION,\n",
    "            resource_bucket=RESOURCE_BUCKET,\n",
    "            query_path=ACCOUNT_CONSL_QUERY_PATH,\n",
    "        )\n",
    "\n",
    "        #timeframe: last 6 months invoice amounts ('SING','HSIC','TTV','SMHM')\n",
    "        create_input_account_ffh_billing_train_view_op = create_input_account_ffh_billing_view(\n",
    "            v_report_date=SCORE_DATE_DASH,\n",
    "            v_start_date=SCORE_DATE_MINUS_6_MOS_DASH,\n",
    "            v_end_date=SCORE_DATE_LAST_MONTH_END_DASH,\n",
    "            v_bill_year=SCORE_DATE_LAST_MONTH_YEAR,\n",
    "            v_bill_month=SCORE_DATE_LAST_MONTH_MONTH,\n",
    "            view_name=FFH_BILLING_VIEW_NAME,\n",
    "            dataset_id=DATASET_ID,\n",
    "            project_id=PROJECT_ID,\n",
    "            region=REGION,\n",
    "            resource_bucket=RESOURCE_BUCKET,\n",
    "            query_path=ACCOUNT_FFH_BILLING_QUERY_PATH\n",
    "        )\n",
    " \n",
    "        #timeframe: last 6 months internet usage by each account by month\n",
    "        create_input_account_hs_usage_train_view_op = create_input_account_hs_usage_view(\n",
    "            v_report_date=SCORE_DATE_DASH,\n",
    "            v_start_date=SCORE_DATE_MINUS_6_MOS_DASH,\n",
    "            v_end_date=SCORE_DATE_LAST_MONTH_END_DASH,\n",
    "            v_bill_year=SCORE_DATE_LAST_MONTH_YEAR,\n",
    "            v_bill_month=SCORE_DATE_LAST_MONTH_MONTH,\n",
    "            view_name=HS_USAGE_VIEW_NAME,\n",
    "            dataset_id=DATASET_ID,\n",
    "            project_id=PROJECT_ID,\n",
    "            region=REGION,\n",
    "            resource_bucket=RESOURCE_BUCKET,\n",
    "            query_path=ACCOUNT_HS_USAGE_QUERY_PATH\n",
    "        )\n",
    "\n",
    "        #timeframe: latest demographic avg income, social group name, life stage group names for each account (matching by postal code) \n",
    "        create_input_account_demo_income_train_view_op = create_input_account_demo_income_view(\n",
    "            score_date=SCORE_DATE,\n",
    "            score_date_delta=SCORE_DATE_DELTA,\n",
    "            view_name=DEMO_INCOME_VIEW_NAME,\n",
    "            dataset_id=DATASET_ID,\n",
    "            project_id=PROJECT_ID,\n",
    "            region=REGION,\n",
    "            resource_bucket=RESOURCE_BUCKET,\n",
    "            query_path=ACCOUNT_DEMO_INCOME_QUERY_PATH\n",
    "        )\n",
    "\n",
    "        #timeframe: last month, next month [total charge, total credit, discount] amts and discount expiry for ('SING', 'HSIC', 'TTV', 'SMHM', 'OTHER')\n",
    "        create_input_account_promo_expiry_train_view_op = create_input_account_promo_expiry_view(\n",
    "            score_date=SCORE_DATE,\n",
    "            view_name=PROMO_EXPIRY_VIEW_NAME,\n",
    "            dataset_id=DATASET_ID,\n",
    "            project_id=PROJECT_ID,\n",
    "            region=REGION,\n",
    "            resource_bucket=RESOURCE_BUCKET,\n",
    "            query_path=ACCOUNT_PROMO_EXPIRY_QUERY_PATH\n",
    "        )\n",
    "\n",
    "        #timeframe: # tickets opened, [TV, HSIA, MAIL, PHONE] affected, days it took to close and resolve tickets in the last 1 month\n",
    "        create_input_account_trouble_tickets_train_view_op = create_input_account_trouble_tickets_view(\n",
    "            score_date=SCORE_DATE,\n",
    "            score_date_delta=SCORE_DATE_DELTA,\n",
    "            trouble_tickets_window=TICKET_DATE_WINDOW,\n",
    "            view_name=TROUBLE_TICKETS_VIEW_NAME,\n",
    "            dataset_id=DATASET_ID,\n",
    "            project_id=PROJECT_ID,\n",
    "            region=REGION,\n",
    "            resource_bucket=RESOURCE_BUCKET,\n",
    "            query_path=ACCOUNT_TROUBLE_TICKETS_QUERY_PATH\n",
    "        )\n",
    "\n",
    "        #timeframe: LATEST snapshot of the number of copper and fibre services \n",
    "        create_input_account_gpon_copper_train_view_op = create_input_account_gpon_copper_view(\n",
    "            score_date=SCORE_DATE,\n",
    "            score_date_delta=SCORE_DATE_DELTA,\n",
    "            view_name=GPON_COPPER_VIEW_NAME,\n",
    "            dataset_id=DATASET_ID,\n",
    "            project_id=PROJECT_ID,\n",
    "            region=REGION,\n",
    "            resource_bucket=RESOURCE_BUCKET,\n",
    "            query_path=ACCOUNT_GPON_COPPER_QUERY_PATH\n",
    "        )\n",
    "\n",
    "        #timeframe: LAST 7 DAYS call data (talk time, hold time, agent count, escalation count\n",
    "        create_input_account_call_data_train_view_op = create_input_account_call_data_view(\n",
    "            score_date=SCORE_DATE,\n",
    "            score_date_delta=SCORE_DATE_DELTA,\n",
    "            view_name=CALL_DATA_VIEW_NAME,\n",
    "            dataset_id=DATASET_ID,\n",
    "            project_id=PROJECT_ID,\n",
    "            region=REGION,\n",
    "            resource_bucket=RESOURCE_BUCKET,\n",
    "            query_path=ACCOUNT_CALL_DATA_QUERY_PATH\n",
    "        )\n",
    "\n",
    "        #timeframe: LAST 30 DAYS internet speed drop count\n",
    "        create_input_account_hsia_drops_train_view_op = create_input_account_hsia_drops_view(\n",
    "            score_date=SCORE_DATE,\n",
    "            score_date_delta=SCORE_DATE_DELTA,\n",
    "            view_name=HSIA_DROPS_VIEW_NAME,\n",
    "            dataset_id=DATASET_ID,\n",
    "            project_id=PROJECT_ID,\n",
    "            region=REGION,\n",
    "            resource_bucket=RESOURCE_BUCKET,\n",
    "            query_path=ACCOUNT_HSIA_DROPS_QUERY_PATH\n",
    "        )\n",
    "        \n",
    "        create_input_account_clckstrm_telus_view_op = create_input_account_clckstrm_telus_view(\n",
    "            score_date=SCORE_DATE,\n",
    "            score_date_delta=SCORE_DATE_DELTA,\n",
    "            view_name=CLCKSTRM_TELUS_VIEW_NAME,\n",
    "            dataset_id=DATASET_ID,\n",
    "            project_id=PROJECT_ID,\n",
    "            region=REGION,\n",
    "            resource_bucket=RESOURCE_BUCKET,\n",
    "            query_path=ACCOUNT_CLCKSTRM_TELUS_QUERY_PATH\n",
    "        )\n",
    "        \n",
    "        create_input_account_alarmdotcom_app_usage_view_op = create_input_account_alarmdotcom_app_usage_view(\n",
    "            score_date=SCORE_DATE,\n",
    "            score_date_delta=SCORE_DATE_DELTA,\n",
    "            view_name=ALARMDOTCOM_APP_USAGE_VIEW_NAME,\n",
    "            dataset_id=DATASET_ID,\n",
    "            project_id=PROJECT_ID,\n",
    "            region=REGION,\n",
    "            resource_bucket=RESOURCE_BUCKET,\n",
    "            query_path=ACCOUNT_ALARMDOTCOM_APP_USAGE_QUERY_PATH\n",
    "        )\n",
    "        \n",
    "        create_input_account_tos_active_bans_view_op = create_input_account_tos_active_bans_view(\n",
    "            score_date=SCORE_DATE,\n",
    "            score_date_delta=SCORE_DATE_DELTA,\n",
    "            v_start_date=SCORE_DATE_LAST_MONTH_START_DASH,\n",
    "            v_end_date=SCORE_DATE_LAST_MONTH_END_DASH,\n",
    "            view_name=TOS_ACTIVE_BANS_VIEW_NAME,\n",
    "            dataset_id=DATASET_ID,\n",
    "            project_id=PROJECT_ID,\n",
    "            region=REGION,\n",
    "            resource_bucket=RESOURCE_BUCKET,\n",
    "            query_path=ACCOUNT_TOS_ACTIVE_BANS_QUERY_PATH\n",
    "        )\n",
    "\n",
    "        # ----- preprocessing train data --------\n",
    "        preprocess_train_op = preprocess(\n",
    "            account_consl_view=CONSL_VIEW_NAME,\n",
    "            account_bill_view=FFH_BILLING_VIEW_NAME,\n",
    "            hs_usage_view=HS_USAGE_VIEW_NAME,\n",
    "            demo_income_view=DEMO_INCOME_VIEW_NAME,\n",
    "            promo_expiry_view=PROMO_EXPIRY_VIEW_NAME,\n",
    "            trouble_tickets_view=TROUBLE_TICKETS_VIEW_NAME,\n",
    "            gpon_copper_view=GPON_COPPER_VIEW_NAME,\n",
    "            call_data_view=CALL_DATA_VIEW_NAME,\n",
    "            hsia_drops_view=HSIA_DROPS_VIEW_NAME,\n",
    "            clckstrm_telus_view=CLCKSTRM_TELUS_VIEW_NAME, \n",
    "            alarmdotcom_app_usage_view=ALARMDOTCOM_APP_USAGE_VIEW_NAME, \n",
    "            tos_active_bans_view=TOS_ACTIVE_BANS_VIEW_NAME, \n",
    "            save_data_path='gs://{}/downloads/{}.csv'.format(FILE_BUCKET, FILENAME),\n",
    "            project_id=PROJECT_ID,\n",
    "            dataset_id=DATASET_ID\n",
    "        )\n",
    "\n",
    "        create_input_account_consl_train_view_op\n",
    "        create_input_account_ffh_billing_train_view_op\n",
    "        create_input_account_hs_usage_train_view_op\n",
    "        create_input_account_demo_income_train_view_op\n",
    "        create_input_account_promo_expiry_train_view_op\n",
    "        create_input_account_trouble_tickets_train_view_op\n",
    "        create_input_account_gpon_copper_train_view_op\n",
    "        create_input_account_call_data_train_view_op\n",
    "        create_input_account_hsia_drops_train_view_op\n",
    "        create_input_account_clckstrm_telus_view_op\n",
    "        create_input_account_alarmdotcom_app_usage_view_op\n",
    "        create_input_account_tos_active_bans_view_op\n",
    "        \n",
    "        preprocess_train_op\n",
    "        \n",
    "        return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "341921cf-3d04-4c28-8ea3-73b41913ad63",
   "metadata": {},
   "outputs": [
    {
     "ename": "Forbidden",
     "evalue": "403 DELETE https://bigquery.googleapis.com/bigquery/v2/projects/divg-josh-pr-d1cc3a/datasets/tos_cross_sell/tables/tos_cross_sell_pipeline_consl_data_training_bi_layer?prettyPrint=false: Access Denied: Table divg-josh-pr-d1cc3a:tos_cross_sell.tos_cross_sell_pipeline_consl_data_training_bi_layer: Permission bigquery.tables.delete denied on table divg-josh-pr-d1cc3a:tos_cross_sell.tos_cross_sell_pipeline_consl_data_training_bi_layer (or it may not exist).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPROJECT_ID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregion\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mREGION\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource_bucket\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mRESOURCE_BUCKET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_bucket\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mFILE_BUCKET\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(project_id, region, resource_bucket, file_bucket)\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpipeline\u001b[39m(\n\u001b[1;32m   1071\u001b[0m             project_id: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m PROJECT_ID,\n\u001b[1;32m   1072\u001b[0m             region: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m REGION,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1076\u001b[0m         \u001b[38;5;66;03m# ------------- train view ops ---------------\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m         \u001b[38;5;66;03m#timeframe: snapshot as of scoring date, and look at 30 days for new activations\u001b[39;00m\n\u001b[0;32m-> 1078\u001b[0m         create_input_account_consl_train_view_op \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_input_account_consl_view\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m            \u001b[49m\u001b[43mview_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONSL_VIEW_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscore_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSCORE_DATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscore_date_delta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSCORE_DATE_DELTA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproject_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPROJECT_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdataset_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATASET_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m            \u001b[49m\u001b[43mregion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mREGION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresource_bucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRESOURCE_BUCKET\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m            \u001b[49m\u001b[43mquery_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mACCOUNT_CONSL_QUERY_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1089\u001b[0m         \u001b[38;5;66;03m#timeframe: last 6 months invoice amounts ('SING','HSIC','TTV','SMHM')\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m         create_input_account_ffh_billing_train_view_op \u001b[38;5;241m=\u001b[39m create_input_account_ffh_billing_view(\n\u001b[1;32m   1091\u001b[0m             v_report_date\u001b[38;5;241m=\u001b[39mSCORE_DATE_DASH,\n\u001b[1;32m   1092\u001b[0m             v_start_date\u001b[38;5;241m=\u001b[39mSCORE_DATE_MINUS_6_MOS_DASH,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1101\u001b[0m             query_path\u001b[38;5;241m=\u001b[39mACCOUNT_FFH_BILLING_QUERY_PATH\n\u001b[1;32m   1102\u001b[0m         )\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mcreate_input_account_consl_view\u001b[0;34m(view_name, score_date, score_date_delta, project_id, dataset_id, region, resource_bucket, query_path)\u001b[0m\n\u001b[1;32m    134\u001b[0m content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(content, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m if_tbl_exists(bq_client, table_ref):\n\u001b[0;32m--> 137\u001b[0m     \u001b[43mbq_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelete_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# content = open(query_path, 'r').read()\u001b[39;00m\n\u001b[1;32m    141\u001b[0m create_base_feature_set_query \u001b[38;5;241m=\u001b[39m content\u001b[38;5;241m.\u001b[39mformat(score_date\u001b[38;5;241m=\u001b[39mscore_date,\n\u001b[1;32m    142\u001b[0m                                                score_date_delta\u001b[38;5;241m=\u001b[39mscore_date_delta,\n\u001b[1;32m    143\u001b[0m                                                view_name\u001b[38;5;241m=\u001b[39mview_name,\n\u001b[1;32m    144\u001b[0m                                                dataset_id\u001b[38;5;241m=\u001b[39mdataset_id,\n\u001b[1;32m    145\u001b[0m                                                project_id\u001b[38;5;241m=\u001b[39mproject_id,\n\u001b[1;32m    146\u001b[0m                                                )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/google/cloud/bigquery/client.py:1794\u001b[0m, in \u001b[0;36mClient.delete_table\u001b[0;34m(self, table, retry, timeout, not_found_ok)\u001b[0m\n\u001b[1;32m   1792\u001b[0m     path \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39mpath\n\u001b[1;32m   1793\u001b[0m     span_attributes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m: path}\n\u001b[0;32m-> 1794\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1795\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspan_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBigQuery.deleteTable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspan_attributes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspan_attributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1798\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDELETE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1799\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1800\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1801\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1802\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core_exceptions\u001b[38;5;241m.\u001b[39mNotFound:\n\u001b[1;32m   1803\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m not_found_ok:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/google/cloud/bigquery/client.py:759\u001b[0m, in \u001b[0;36mClient._call_api\u001b[0;34m(self, retry, span_name, span_attributes, job_ref, headers, **kwargs)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    756\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m create_span(\n\u001b[1;32m    757\u001b[0m         name\u001b[38;5;241m=\u001b[39mspan_name, attributes\u001b[38;5;241m=\u001b[39mspan_attributes, client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, job_ref\u001b[38;5;241m=\u001b[39mjob_ref\n\u001b[1;32m    758\u001b[0m     ):\n\u001b[0;32m--> 759\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m call()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/google/api_core/retry.py:283\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    280\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    282\u001b[0m )\n\u001b[0;32m--> 283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_deadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/google/api_core/retry.py:190\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, deadline, on_error)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/google/cloud/_http/__init__.py:494\u001b[0m, in \u001b[0;36mJSONConnection.api_request\u001b[0;34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout, extra_api_info)\u001b[0m\n\u001b[1;32m    482\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    483\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    484\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m     extra_api_info\u001b[38;5;241m=\u001b[39mextra_api_info,\n\u001b[1;32m    491\u001b[0m )\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 494\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_http_response(response)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expect_json \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent:\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "\u001b[0;31mForbidden\u001b[0m: 403 DELETE https://bigquery.googleapis.com/bigquery/v2/projects/divg-josh-pr-d1cc3a/datasets/tos_cross_sell/tables/tos_cross_sell_pipeline_consl_data_training_bi_layer?prettyPrint=false: Access Denied: Table divg-josh-pr-d1cc3a:tos_cross_sell.tos_cross_sell_pipeline_consl_data_training_bi_layer: Permission bigquery.tables.delete denied on table divg-josh-pr-d1cc3a:tos_cross_sell.tos_cross_sell_pipeline_consl_data_training_bi_layer (or it may not exist)."
     ]
    }
   ],
   "source": [
    "pipeline(project_id = PROJECT_ID, region = REGION, resource_bucket = RESOURCE_BUCKET, file_bucket = FILE_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19031f2a-1c49-4ce2-848d-1240bbb9a54a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
