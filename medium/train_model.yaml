name: Model training
inputs:
- {name: dataset_id, type: String}
- {name: file_bucket, type: String}
- {name: save_path, type: String}
outputs:
- {name: metrics, type: Metrics}
- {name: metricsc, type: ClassificationMetrics}
- {name: accuracy, type: Float}
- {name: f1_score, type: Float}
- {name: roc_auc, type: Float}
- {name: X_y_val_index, type: JsonArray}
implementation:
  container:
    image: northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/wb-platform/pipelines/kubeflow-pycaret:latest
    command:
    - sh
    - -c
    - |2

      if ! [ -x "$(command -v pip)" ]; then
          python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
      fi

      PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.11' && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp -d)
      printf "%s" "$0" > "$program_path/ephemeral_component.py"
      python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
    - "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing\
      \ import *\n\ndef model_training(\n            dataset_id: str,\n          \
      \  file_bucket: str, \n            save_path: str,\n            metrics: Output[Metrics],\n\
      \            metricsc: Output[ClassificationMetrics]\n    ) -> NamedTuple(\n\
      \        \"Outputs\",\n        [\n            (\"accuracy\", float),  # Return\
      \ parameters\n            (\"f1_score\", float),\n            (\"roc_auc\",\
      \ float), \n            (\"X_y_val_index\", list), \n        ],\n    ):\n  \
      \  # Import Libraries\n    import pandas as pd\n    import numpy as np\n   \
      \ import matplotlib.pyplot as plt\n    import seaborn as sns\n    from sklearn.datasets\
      \ import load_breast_cancer\n    from sklearn.model_selection import train_test_split\n\
      \    import xgboost as xgb\n    from sklearn.metrics import roc_auc_score, accuracy_score,\
      \ precision_score, recall_score, f1_score, roc_curve, confusion_matrix\n\n \
      \   # Read csv that was saved in 'import_data' component\n    df = pd.read_csv(save_path)\
      \  \n\n    # X and y\n    y = np.squeeze(df['target'].values)\n    X = df.drop(columns='target')\n\
      \n    # Create the training and test sets\n    X_train, X_test, y_train, y_test\
      \ = train_test_split(X, y, test_size=0.3, random_state=123)\n\n    # Reserve\
      \ some samples for final validation\n    X_test, X_val, y_test, y_val = train_test_split(X_test,\
      \ y_test, test_size=0.2, random_state=123)\n\n    # Instantiate the XGB Classifier:\
      \ xgb_model\n    xgb_model = xgb.XGBClassifier(\n        learning_rate=0.01,\n\
      \        n_estimators=100,\n        max_depth=8,\n        min_child_weight=1,\n\
      \        max_delta_step=1, \n        colsample_bytree=0.9,\n        subsample=0.9,\n\
      \        objective='binary:logistic',\n        nthread=4,\n        scale_pos_weight=1,\
      \ \n        eval_metric='auc', \n        base_score=0.5\n    )\n\n    # Fit\
      \ the classifier to the training set\n    xgb_model.fit(X_train, y_train)\n\n\
      \    # Predict based on X_test\n    y_pred = xgb_model.predict(X_test)\n   \
      \ y_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n\n    # Model accuracy\
      \ \n    accuracy = accuracy_score(y_test, y_pred)\n    print(\"Accuracy:\",\
      \ accuracy)\n\n    # Precision & Recall \n    precision = precision_score(y_test,\
      \ y_pred)\n    recall = recall_score(y_test, y_pred)\n\n    # F1 Score \n  \
      \  f1_score = f1_score(y_test, y_pred)\n    print(\"F1 Score:\", f1_score)\n\
      \n    # ROC AUC Score\n    roc_auc = roc_auc_score(y_test, y_pred_proba)\n \
      \   print(\"ROC AUC Score:\", roc_auc)\n\n    # Log eval metrics\n    metrics.log_metric(\"\
      Model\", \"XGBClassifier\")\n    metrics.log_metric(\"Size\", df.shape[0])\n\
      \    metrics.log_metric(\"Accuracy\",(accuracy * 100.0))\n    metrics.log_metric(\"\
      AUC\", roc_auc)\n    metrics.log_metric(\"F1_Score\", f1_score)\n\n    # Compute\
      \ fpr, tpr, thresholds for the ROC Curve\n    fpr, tpr, thresholds = roc_curve(\n\
      \        y_true=y_test, y_score=y_pred_proba, pos_label=True\n    )\n\n    #\
      \ Log classification metrics\n    metricsc.log_roc_curve(fpr.tolist(), tpr.tolist(),\
      \ thresholds.tolist())\n    metricsc.log_confusion_matrix(['Malignant', 'Benign'],\
      \ confusion_matrix(y_test, y_pred).tolist())\n\n    return (accuracy, f1_score,\
      \ roc_auc, list(X_val.index))\n\n"
    args:
    - --executor_input
    - {executorInput: null}
    - --function_to_execute
    - model_training
