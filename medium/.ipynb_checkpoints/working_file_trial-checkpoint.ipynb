{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41505112-1c95-4043-8f88-44631ff9faec",
   "metadata": {},
   "source": [
    "### 2) Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366a6af1-520a-4d3a-bfd8-2962221e82ec",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bee86f4-677b-40dd-9356-638e301032ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output, OutputPath, ClassificationMetrics,\n",
    "                        Metrics, component)\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from typing import NamedTuple\n",
    "import google\n",
    "from google.oauth2 import credentials\n",
    "from google.oauth2 import service_account\n",
    "from google.oauth2.service_account import Credentials\n",
    "from google.cloud import storage\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google_cloud_pipeline_components.v1.batch_predict_job import \\\n",
    "    ModelBatchPredictOp as batch_prediction_op\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037517c9-3db0-4817-85d6-b669c5d79a65",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccbf37f-831e-42e6-ae6c-e8c35d42d218",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import Dataset and Save in GCS\n",
    "@component(\n",
    "    base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/wb-platform/pipelines/kubeflow-pycaret:latest\",\n",
    "    output_component_file=\"import_data.yaml\",\n",
    ")\n",
    "def import_data(\n",
    "            dataset_id: str,\n",
    "            file_bucket: str, \n",
    "    ) -> NamedTuple(\n",
    "        \"Outputs\", \n",
    "        [\n",
    "         (\"save_path\", str), \n",
    "         (\"col_list\", list)\n",
    "        ]\n",
    "    ):\n",
    "    # Import Libraries\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "    # import the entire dataset into 'data'\n",
    "    data = load_breast_cancer() \n",
    "    \n",
    "    # save the data in df, including the targets\n",
    "    df = pd.DataFrame(data = data.data, columns = data.feature_names) \n",
    "    df['target'] = pd.Series(data.target) \n",
    "    \n",
    "    # save df in cloud storage \n",
    "    save_path = f'gs://{file_bucket}/{dataset_id}/{dataset_id}_data.csv'\n",
    "    df.to_csv(save_path, index=True) \n",
    "    \n",
    "    print(f'{dataset_id}_data.csv saved in {save_path}')\n",
    "    \n",
    "    col_list = list([col for col in df.columns if col != \"target\"])\n",
    "    \n",
    "    return (save_path, col_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eae71d1-7ffc-4216-9c57-75f4c2e5886d",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fa0316-f72d-4f90-83fc-7951ac672199",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load Dataset and Train Model\n",
    "@component(\n",
    "    base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/wb-platform/pipelines/kubeflow-pycaret:latest\",\n",
    "    output_component_file=\"train_model.yaml\",\n",
    ")\n",
    "def model_training(\n",
    "            dataset_id: str,\n",
    "            file_bucket: str, \n",
    "            save_path: str,\n",
    "            model: Output[Model],\n",
    "            metrics: Output[Metrics],\n",
    "            metricsc: Output[ClassificationMetrics], \n",
    "            col_list: list \n",
    "    ) -> NamedTuple(\n",
    "        \"Outputs\",\n",
    "        [\n",
    "            (\"accuracy\", float),  # Return parameters\n",
    "            (\"f1_score\", float),\n",
    "            (\"roc_auc\", float), \n",
    "            (\"X_y_val_index\", list), \n",
    "            (\"model_location\", str)\n",
    "        ],\n",
    "    ):\n",
    "    # Import Libraries\n",
    "    import gc\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import xgboost as xgb\n",
    "    import pickle\n",
    "    import logging\n",
    "    from google.cloud import storage\n",
    "    from google.cloud import bigquery\n",
    "    from sklearn.datasets import load_breast_cancer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, roc_curve, confusion_matrix\n",
    "    \n",
    "    # Read csv that was saved in 'import_data' component\n",
    "    df = pd.read_csv(save_path)  \n",
    "\n",
    "    # X and y\n",
    "    y = np.squeeze(df['target'].values)\n",
    "    X = df.drop(columns='target')\n",
    "    \n",
    "    # Create the training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)\n",
    "    \n",
    "    # Reserve some samples for final validation\n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.2, random_state=123)\n",
    "    \n",
    "    # export X_val and y_val to GCS for scoring\n",
    "    X_val.to_csv(f'gs://{file_bucket}/{dataset_id}/{dataset_id}_X_val.csv')\n",
    "    \n",
    "    y_val_df = pd.DataFrame(y_val, columns = ['target']) \n",
    "    y_val_df.to_csv(f'gs://{file_bucket}/{dataset_id}/{dataset_id}_y_val.csv')\n",
    "\n",
    "    # Instantiate the XGB Classifier: xgb_model\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        learning_rate=0.01,\n",
    "        n_estimators=100,\n",
    "        max_depth=8,\n",
    "        min_child_weight=1,\n",
    "        max_delta_step=1, \n",
    "        colsample_bytree=0.9,\n",
    "        subsample=0.9,\n",
    "        objective='binary:logistic',\n",
    "        nthread=4,\n",
    "        scale_pos_weight=1, \n",
    "        eval_metric='auc', \n",
    "        base_score=0.5\n",
    "    )\n",
    "\n",
    "    # Fit the classifier to the training set\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict based on X_test\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "    y_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Model accuracy \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    \n",
    "    # Precision & Recall \n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    \n",
    "    # F1 Score \n",
    "    f1_score = f1_score(y_test, y_pred)\n",
    "    print(\"F1 Score:\", f1_score)\n",
    "\n",
    "    # ROC AUC Score\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    print(\"ROC AUC Score:\", roc_auc)\n",
    "\n",
    "    # Log eval metrics\n",
    "    metrics.log_metric(\"Model\", \"XGBClassifier\")\n",
    "    metrics.log_metric(\"Size\", df.shape[0])\n",
    "    metrics.log_metric(\"Accuracy\",accuracy)\n",
    "    metrics.log_metric(\"AUC\", roc_auc)\n",
    "    metrics.log_metric(\"Precision\", precision) \n",
    "    metrics.log_metric(\"Recall\", recall) \n",
    "    metrics.log_metric(\"F1_Score\", f1_score)\n",
    "\n",
    "    # Compute fpr, tpr, thresholds for the ROC Curve\n",
    "    fpr, tpr, thresholds = roc_curve(\n",
    "        y_true=y_test, y_score=y_pred_proba, pos_label=True\n",
    "    )\n",
    "    \n",
    "    # Log classification metrics\n",
    "    metricsc.log_roc_curve(fpr.tolist(), tpr.tolist(), thresholds.tolist())\n",
    "    metricsc.log_confusion_matrix(['Malignant', 'Benign'], confusion_matrix(y_test, y_pred).tolist())\n",
    "\n",
    "    # added to model_training component: save model artifacts in GCS bucket\n",
    "    model_artifacts = {}\n",
    "    create_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    model_artifacts['create_time'] = create_time\n",
    "    model_artifacts['model'] = xgb_model\n",
    "    model_artifacts['col_list'] = col_list\n",
    "    \n",
    "    # create and write model_artifacts.pkl\n",
    "    with open('model_artifacts.pkl', 'wb') as pkl_file:\n",
    "        pickle.dump(model_artifacts, pkl_file)\n",
    "\n",
    "        # Use the 'pickle.dump()' method to serialize and store the 'model_artifacts' data\n",
    "        pickle.dump(model_artifacts, pkl_file)\n",
    "\n",
    "    # create a gcs bucket instance\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(file_bucket)\n",
    "    \n",
    "    # define the folder path where the models will be saved. create one if not found. \n",
    "    model_path = 'breast_cancer_models/'\n",
    "    blob = bucket.blob(model_path)\n",
    "    if not blob.exists(storage_client):\n",
    "        blob.upload_from_string('')\n",
    "    \n",
    "    # set model name and upload 'model_artifacts.pkl' to the folder in gcs bucket \n",
    "    model_name = 'breast_cancer_models_{}'.format(model_artifacts['create_time'])\n",
    "    model_location = f'{model_path}{model_name}'\n",
    "    blob = bucket.blob(model_location)\n",
    "    blob.upload_from_filename('model_artifacts.pkl')\n",
    "    \n",
    "    print(f\"Model artifacts loaded to GCS Bucket: {model_location}\")\n",
    "    \n",
    "#     model.metadata['accuracy'] = accuracy\n",
    "#     model.metadata['precision'] = precision\n",
    "#     model.metadata['recall'] = recall\n",
    "#     model.metadata['f1_score'] = f1_score\n",
    "#     model.metadata['auc'] = roc_auc\n",
    "    \n",
    "    model.uri = f'gs://{file_bucket}/{model_location}'\n",
    "    \n",
    "#     # Log additional model details \n",
    "#     with open(model.path, 'w') as output_file:\n",
    "#         output_file.write(f'You can enter additional model details here')\n",
    "#     output_file.close()\n",
    "    \n",
    "    time.sleep(120)\n",
    "\n",
    "    return (accuracy, f1_score, roc_auc, list(X_val.index), model_location)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d56ddc-47bc-4c45-a87d-941cc493ffd4",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597f14a3-5144-4ef7-ba77-11a8d46583f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model performance\n",
    "@component(\n",
    "    base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/wb-platform/pipelines/kubeflow-pycaret:latest\",\n",
    "    output_component_file=\"model_evaluation.yaml\",\n",
    ")\n",
    "def model_evaluation(\n",
    "            accuracy: float, \n",
    "            f1_score: float, \n",
    "            roc_auc: float, \n",
    "            accuracy_threshold: float, \n",
    "            f1_score_threshold: float, \n",
    "            roc_auc_threshold: float\n",
    "            ) -> NamedTuple(\n",
    "                \"Output\", [(\"result\", str)]\n",
    "            ):\n",
    "    \n",
    "    # Set checker to True\n",
    "    checker = True\n",
    "    \n",
    "    # Set checker to False if any of the eval metrics is below threshold\n",
    "    if accuracy < accuracy_threshold: \n",
    "        checker = False \n",
    "    if f1_score < f1_score_threshold: \n",
    "        checker = False \n",
    "    if roc_auc < roc_auc_threshold: \n",
    "        checker = False \n",
    "        \n",
    "    # if checker == True, return \"Pass\", otherwise return \"Fail\"\n",
    "    if checker == True: \n",
    "        return (\"Pass\",) \n",
    "    else: \n",
    "        return (\"Fail\",)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccbb67d-36f5-4964-b7ee-c9fe0bbc9d88",
   "metadata": {},
   "source": [
    "### Upload model to Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcabf08a-b7fb-4fb0-aade-4e966e5d6b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.v2.dsl import (Model, Input, Output, component)\n",
    "\n",
    "# Component for uploading model to Vertex Model Registry\n",
    "@component(\n",
    "# Uploads model\n",
    "    base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/wb-platform/pipelines/kubeflow-pycaret:latest\",\n",
    "    output_component_file=\"model-upload.yaml\",\n",
    ")\n",
    "\n",
    "def upload_model_to_mr(\n",
    "    project_id: str,\n",
    "    model: Input[Model],\n",
    "    vertex_model: Output[Model],\n",
    "    region: str,\n",
    "    model_name: str,\n",
    "    prediction_image: str,\n",
    "    col_list: list, \n",
    "    result: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Upload model to Vertex Model Registry.\n",
    "    Args:\n",
    "        project_id (str): project id for where this pipeline is being run\n",
    "        model (Input[Model]): model passed in from training component. Must have path specified in model.uri\n",
    "        region (str): region for where the query will be run\n",
    "        model_name (str): name of model to be stored\n",
    "        prediction_image (str): prediction image uri\n",
    "        col_list (str): string of list of columns in serving data\n",
    "    Returns:\n",
    "        vertex_model (Output[Model]): Model saved in Vertex AI\n",
    "    \"\"\"\n",
    "\n",
    "    from google.cloud import aiplatform\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "\n",
    "    aiplatform.init(project=project_id, location=region)\n",
    "    \n",
    "    ## check if prediction image is custom or not\n",
    "    if prediction_image.startswith('northamerica-northeast1-docker'):\n",
    "        # custom: must set ports\n",
    "        health_route = \"/ping\"\n",
    "        predict_route = \"/predict\"\n",
    "        serving_container_ports = [7080]\n",
    "    else:\n",
    "        # Google pre-built\n",
    "        health_route = None\n",
    "        predict_route = None\n",
    "        serving_container_ports = None\n",
    "\n",
    "    if result == \"Pass\": \n",
    "\n",
    "        ## check for existing models\n",
    "        # if model exists, update the version\n",
    "        try:\n",
    "            model_uid = aiplatform.Model.list(\n",
    "                filter=f'display_name={model_name}', \n",
    "                order_by=\"update_time\",\n",
    "                location=region)[-1].resource_name\n",
    "\n",
    "            uploaded_model = aiplatform.Model.upload(\n",
    "                display_name = model_name, \n",
    "                artifact_uri = os.path.dirname(model.uri),\n",
    "                serving_container_image_uri = prediction_image,\n",
    "                serving_container_environment_variables =  {\"COL_LIST\":str(col_list)}, # remove for posting\n",
    "                parent_model = model_uid,\n",
    "                is_default_version = True\n",
    "            )\n",
    "        # if model does not already exist, upload a new model\n",
    "        except:\n",
    "            uploaded_model = aiplatform.Model.upload(\n",
    "                display_name = model_name,\n",
    "                artifact_uri = os.path.dirname(model.uri),\n",
    "                serving_container_image_uri=prediction_image,\n",
    "                serving_container_environment_variables =  {\"COL_LIST\":str(col_list)}, # remove for posting\n",
    "            )\n",
    "\n",
    "        vertex_model.uri = uploaded_model.resource_name\n",
    "        vertex_model.version_create_time = datetime.now()\n",
    "        vertex_model.version_description = \"breast cancer model\" \n",
    "    \n",
    "    else: \n",
    "        \n",
    "        print(\"Training performance is not satisfactory. Upload to the Model Registry revoked.\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad498db7-a73e-44c2-b670-5018b0000052",
   "metadata": {},
   "source": [
    "### Load Model to Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5185bc-1de1-4f42-a3b7-c52c361c3f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import (Artifact, Output, Input, HTML, component)\n",
    "\n",
    "# Load Custom Model Component: load in most recent version of your model to run batch predictions with\n",
    "@component(\n",
    "    base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/wb-platform/pipelines/kubeflow-pycaret:latest\",\n",
    "    output_component_file=\"load_model.yaml\"\n",
    ")\n",
    "# this model returns a model artifact that will be passed on to Batch Predictions\n",
    "def load_model(\n",
    "                project_id: str, \n",
    "                region: str, \n",
    "                model_name: str, \n",
    "                model: Output[Artifact]):\n",
    "    \n",
    "    from google.cloud import aiplatform\n",
    "    \n",
    "    model_uid = aiplatform.Model.list(\n",
    "                                    filter=f'display_name={model_name}', \n",
    "                                    order_by=\"update_time\",\n",
    "                                    location=region)[-1].resource_name\n",
    "    model.uri = model_uid\n",
    "    model.metadata['resourceName'] = model_uid\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a005d834-3043-4a3c-8fb7-f50046ba4187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import (Artifact, Output, Input, HTML, component)\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ModelOutput:\n",
    "    model_uri: str\n",
    "\n",
    "# Load Custom Model Component: load in most recent version of your model to run batch predictions with\n",
    "@component(\n",
    "    base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/wb-platform/pipelines/kubeflow-pycaret:latest\",\n",
    "    output_component_file=\"load_model.yaml\"\n",
    ")\n",
    "# this model returns a model artifact that will be passed on to Batch Predictions\n",
    "def load_model(\n",
    "                project_id: str, \n",
    "                region: str, \n",
    "                model_name: str, \n",
    "                model: Output[Artifact]\n",
    "                ) -> ModelOutput:\n",
    "\n",
    "    @dataclass\n",
    "    class ModelOutput:\n",
    "        model_uri: str\n",
    "\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    model_uid = aiplatform.Model.list(\n",
    "                                    filter=f'display_name={model_name}', \n",
    "                                    order_by=\"update_time\",\n",
    "                                    location=region)[-1].resource_name\n",
    "    model.uri = model_uid\n",
    "    model.metadata['resourceName'] = model_uid\n",
    "    model_uri = model.uri\n",
    "\n",
    "    return ModelOutput(model_uri=str(model_uri))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d27b51-a266-4a51-a7b1-bc46f91eb490",
   "metadata": {},
   "source": [
    "### Batch Prediction - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4310c02a-b2b5-4ae1-ac5a-d93bc810166c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output, OutputPath, ClassificationMetrics,\n",
    "                        Metrics, component, HTML)\n",
    "@component(\n",
    "    base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/wb-platform/pipelines/kubeflow-pycaret:latest\",\n",
    "    output_component_file=\"batch_prediction.yaml\",\n",
    ")\n",
    "def batch_prediction(\n",
    "        project_id: str,\n",
    "        dataset_id: str,\n",
    "        file_bucket: str,\n",
    "        val_index: list, \n",
    "        save_path: str, \n",
    "        model: Input[Model],\n",
    "        metrics: Output[Metrics],\n",
    "        metricsc: Output[ClassificationMetrics],\n",
    "):\n",
    "    import time\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    from datetime import date\n",
    "    from dateutil.relativedelta import relativedelta\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "    from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, roc_curve, confusion_matrix\n",
    "    \n",
    "    # Read csv that was saved in 'import_data' component\n",
    "    df = pd.read_csv(save_path)  \n",
    "\n",
    "    # X and y\n",
    "    X = df.drop(columns='target')\n",
    "    y = df['target']\n",
    "    \n",
    "    X_val = X.loc[val_index] \n",
    "    y_val = np.squeeze(y.iloc[val_index].values) \n",
    "\n",
    "    time.sleep(10)\n",
    "    \n",
    "    print(str(model.uri))\n",
    "    print(str(model.location))\n",
    "\n",
    "    model_path = 'breast_cancer_models/'\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(file_bucket)\n",
    "    blobs = storage_client.list_blobs(file_bucket, prefix='{}breast_cancer_models'.format(model_path))\n",
    "\n",
    "    model_lists = []\n",
    "    for blob in blobs:\n",
    "        model_lists.append(blob.name)\n",
    "\n",
    "    blob = bucket.blob(model_lists[-1])\n",
    "    blob_in = blob.download_as_string()\n",
    "    model_dict = pickle.loads(blob_in)\n",
    "    model_xgb = model_dict['model']\n",
    "    features = model_dict['col_list']\n",
    "    print('...... model loaded')\n",
    "    time.sleep(10)\n",
    "\n",
    "    # get full score to cave into bucket\n",
    "    y_pred = model_xgb.predict(X_val)\n",
    "    y_pred_proba = model_xgb.predict_proba(X_val)[:, 1] \n",
    "    \n",
    "    result = pd.DataFrame(columns=['index', 'y_pred_proba', 'y_pred', 'y_val'])\n",
    "    result['index'] = pd.Series(X_val.index.to_list())\n",
    "    # result['index'] = result['index'].astype('int64')\n",
    "    result['y_pred_proba'] = y_pred_proba\n",
    "    # result['y_pred_proba'] = result['y_pred_proba'].fillna(0.0).astype('float64')\n",
    "    result['y_pred'] = y_pred\n",
    "    result['y_test'] = y_val\n",
    "\n",
    "    result.to_csv('gs://{}/breast_cancer/model_validation.csv'.format(file_bucket), index=True)\n",
    "\n",
    "    # Model accuracy \n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    \n",
    "    # Precision & Recall \n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)\n",
    "    \n",
    "    # F1 Score \n",
    "    f1_score = f1_score(y_val, y_pred)\n",
    "    print(\"F1 Score:\", f1_score)\n",
    "\n",
    "    # ROC AUC Score\n",
    "    roc_auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    print(\"ROC AUC Score:\", roc_auc)\n",
    "\n",
    "    # Log eval metrics\n",
    "    metrics.log_metric(\"Model\", \"XGBClassifier\")\n",
    "    metrics.log_metric(\"Size\", X_val.shape[0])\n",
    "    metrics.log_metric(\"Accuracy\", accuracy)\n",
    "    metrics.log_metric(\"AUC\", roc_auc)\n",
    "    metrics.log_metric(\"Precision\", precision) \n",
    "    metrics.log_metric(\"Recall\", recall) \n",
    "    metrics.log_metric(\"F1_Score\", f1_score)\n",
    "    \n",
    "    time.sleep(60)\n",
    "    print(f\"Batch prediction for {X_val.shape[0]} samples completed\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddfb2e3-e717-4972-bde2-8b8b403f4296",
   "metadata": {},
   "source": [
    "### Batch Prediction - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eace8980-70b6-40f2-b5c0-efc506bf9c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output, OutputPath, ClassificationMetrics,\n",
    "                        Metrics, component, HTML)\n",
    "@component(\n",
    "    base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/wb-platform/pipelines/kubeflow-pycaret:latest\",\n",
    "    output_component_file=\"batch_prediction.yaml\",\n",
    ")\n",
    "def batch_prediction(\n",
    "        region: str, \n",
    "        project_id: str,\n",
    "        dataset_id: str,\n",
    "        file_bucket: str,\n",
    "        val_index: list, \n",
    "        save_path: str, \n",
    "        model_name: str, \n",
    "        model: Output[Artifact],\n",
    "        metrics: Output[Metrics],\n",
    "        metricsc: Output[ClassificationMetrics],\n",
    "):\n",
    "    import time\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    from datetime import date\n",
    "    from dateutil.relativedelta import relativedelta\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "    from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, roc_curve, confusion_matrix\n",
    "    from google.cloud import aiplatform\n",
    "    \n",
    "    model_uid = aiplatform.Model.list(\n",
    "                                    filter=f'display_name={model_name}', \n",
    "                                    order_by=\"update_time\",\n",
    "                                    location=region)[-1].resource_name\n",
    "    model.uri = model_uid\n",
    "    model.metadata['resourceName'] = model_uid\n",
    "    \n",
    "    # Read csv that was saved in 'import_data' component\n",
    "    df = pd.read_csv(save_path)  \n",
    "\n",
    "    # X and y\n",
    "    X = df.drop(columns='target')\n",
    "    y = df['target']\n",
    "    \n",
    "    X_val = X.loc[val_index] \n",
    "    y_val = np.squeeze(y.iloc[val_index].values) \n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "    model_path = 'breast_cancer_models/'\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(file_bucket)\n",
    "    blobs = storage_client.list_blobs(file_bucket, prefix='{}breast_cancer_models'.format(model_path))\n",
    "\n",
    "    model_lists = []\n",
    "    for blob in blobs:\n",
    "        model_lists.append(blob.name)\n",
    "\n",
    "    blob = bucket.blob(model_lists[-1])\n",
    "    blob_in = blob.download_as_string()\n",
    "    model_dict = pickle.loads(blob_in)\n",
    "    model_xgb = model_dict['model']\n",
    "    features = model_dict['col_list']\n",
    "    print('...... model loaded')\n",
    "    time.sleep(10)\n",
    "\n",
    "    # get full score to cave into bucket\n",
    "    y_pred = model_xgb.predict(X_val)\n",
    "    y_pred_proba = model_xgb.predict_proba(X_val)[:, 1] \n",
    "    \n",
    "    result = pd.DataFrame(columns=['index', 'y_pred_proba', 'y_pred', 'y_val'])\n",
    "    result['index'] = pd.Series(X_val.index.to_list())\n",
    "    # result['index'] = result['index'].astype('int64')\n",
    "    result['y_pred_proba'] = y_pred_proba\n",
    "    # result['y_pred_proba'] = result['y_pred_proba'].fillna(0.0).astype('float64')\n",
    "    result['y_pred'] = y_pred\n",
    "    result['y_test'] = y_val\n",
    "\n",
    "    result.to_csv('gs://{}/breast_cancer/model_validation.csv'.format(file_bucket), index=True)\n",
    "\n",
    "    # Model accuracy \n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    \n",
    "    # Precision & Recall \n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)\n",
    "    \n",
    "    # F1 Score \n",
    "    f1_score = f1_score(y_val, y_pred)\n",
    "    print(\"F1 Score:\", f1_score)\n",
    "\n",
    "    # ROC AUC Score\n",
    "    roc_auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    print(\"ROC AUC Score:\", roc_auc)\n",
    "\n",
    "    # Log eval metrics\n",
    "    metrics.log_metric(\"Model\", \"XGBClassifier\")\n",
    "    metrics.log_metric(\"Size\", X_val.shape[0])\n",
    "    metrics.log_metric(\"Accuracy\", accuracy)\n",
    "    metrics.log_metric(\"AUC\", roc_auc)\n",
    "    metrics.log_metric(\"Precision\", precision) \n",
    "    metrics.log_metric(\"Recall\", recall) \n",
    "    metrics.log_metric(\"F1_Score\", f1_score)\n",
    "    \n",
    "    time.sleep(60)\n",
    "    print(f\"Batch prediction for {X_val.shape[0]} samples completed\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f0d746-8bfd-4665-aade-2549917a0096",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import pickle\n",
    "# from datetime import date\n",
    "# from dateutil.relativedelta import relativedelta\n",
    "# from google.cloud import bigquery\n",
    "# from google.cloud import storage\n",
    "\n",
    "# file_bucket = FILE_BUCKET\n",
    "# dataset_id = DATASET_ID \n",
    "\n",
    "# save_path = f'gs://{file_bucket}/{dataset_id}/{dataset_id}_data.csv'\n",
    "# val_index = [22, 138, 192, 190, 260, 498, 157, 309, 454, 166, 202, 488, 33, 480, 205, 345, 334, 175, 520, 399, 511, 24, 400, 49, 74, 230, 557, 327, 43, 436, 456, 287, 209, 410, 326]\n",
    "\n",
    "# # Read csv that was saved in 'import_data' component\n",
    "# df = pd.read_csv(save_path)  \n",
    "\n",
    "# # X and y\n",
    "# X = df.drop(columns='target')\n",
    "# y = df['target']\n",
    "\n",
    "# X_val = X.loc[val_index] \n",
    "# y_val = np.squeeze(y.iloc[val_index].values) \n",
    "\n",
    "# result = pd.DataFrame(columns=['index', 'y_pred_proba', 'y_pred', 'y_val'])\n",
    "# result['index'] = X_val.index\n",
    "# result['index'] = result['index'].astype(int)\n",
    "\n",
    "\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece8b567-6c9e-4409-a834-bcae43b99fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag cell with parameters\n",
    "PROJECT_ID =  'divg-josh-pr-d1cc3a'\n",
    "BUCKET_NAME='divg-josh-pr-d1cc3a-default'\n",
    "DATASET_ID = 'breast_cancer'\n",
    "RESOURCE_BUCKET = 'divg-josh-pr-d1cc3a-default'\n",
    "FILE_BUCKET = 'divg-josh-pr-d1cc3a-default'\n",
    "MODEL_ID = '5070'\n",
    "REGION = 'northamerica-northeast1'\n",
    "MODEL_NAME = 'breast_cancer'\n",
    "PREDICTION_IMAGE = 'northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/wb-platform/pipelines/kubeflow-pycaret:latest'\n",
    "\n",
    "# batch predictions parameters\n",
    "GCS_SOURCE_INPUT_URI = f'gs://{FILE_BUCKET}/{DATASET_ID}/{DATASET_ID}_X_val.csv'\n",
    "GCS_DESTINATION_OUTPUT_URI = f'gs://{FILE_BUCKET}/{DATASET_ID}/'\n",
    "BATCH_PREDICTIONS_DISPLAY_NAME = 'breast_cancer_batch_predictions'\n",
    "INSTANCES_FORMAT = 'csv'\n",
    "PREDICTIONS_FORMAT = 'csv'\n",
    "MACHINE_TYPE = 'n1-standard-2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca60f16-73dd-4f72-85d6-4faaa2039e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports\n",
    "from kfp.v2 import compiler\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "@dsl.pipeline(\n",
    "    name='breast-cancer-pipeline', \n",
    "    description='breast-cancer-pipeline'\n",
    "    )\n",
    "def pipeline(\n",
    "        dataset_id: str = DATASET_ID, \n",
    "        file_bucket: str = FILE_BUCKET, \n",
    "        region: str = REGION\n",
    "    ):\n",
    "    \n",
    "    import google.oauth2.credentials\n",
    "    token = !gcloud auth print-access-token\n",
    "    token_str = token[0]\n",
    "    \n",
    "    # ----- create training set --------\n",
    "    import_data_op = import_data(dataset_id=dataset_id,\n",
    "                          file_bucket=file_bucket)\n",
    "    \n",
    "    model_training_op = model_training(dataset_id=dataset_id,\n",
    "                          file_bucket=file_bucket, \n",
    "                          save_path=import_data_op.outputs['save_path'], \n",
    "                          col_list = import_data_op.outputs[\"col_list\"])\n",
    "    \n",
    "    model_evaluation_op=  model_evaluation(\n",
    "                          accuracy=model_training_op.outputs[\"accuracy\"], \n",
    "                          f1_score=model_training_op.outputs[\"f1_score\"], \n",
    "                          roc_auc=model_training_op.outputs[\"roc_auc\"], \n",
    "                          accuracy_threshold=0.95, \n",
    "                          f1_score_threshold=0.95, \n",
    "                          roc_auc_threshold=0.95\n",
    "                          )\n",
    "    \n",
    "    upload_model_to_mr_op = upload_model_to_mr(\n",
    "                        project_id = PROJECT_ID,\n",
    "                        region = REGION,\n",
    "                        model = model_training_op.outputs[\"model\"],\n",
    "                        model_name = MODEL_NAME,\n",
    "                        prediction_image = PREDICTION_IMAGE,\n",
    "                        col_list = import_data_op.outputs[\"col_list\"], \n",
    "                        result = model_evaluation_op.outputs['result'])\n",
    "    \n",
    "    load_model_op = load_model(\n",
    "                        project_id= PROJECT_ID, \n",
    "                        region= REGION, \n",
    "                        model_name= MODEL_NAME)\n",
    "    \n",
    "#     batch_prediction_op = batch_prediction(\n",
    "#                         project_id = PROJECT_ID,\n",
    "#                         dataset_id = DATASET_ID,\n",
    "#                         file_bucket = FILE_BUCKET, \n",
    "#                         val_index = model_training_op.outputs['X_y_val_index'], \n",
    "#                         save_path = import_data_op.outputs['save_path'], \n",
    "#                         model = load_model_op.outputs[\"model\"])\n",
    "\n",
    "    batch_prediction = batch_prediction_op(\n",
    "                                            project = PROJECT_ID,\n",
    "                                            location = REGION,\n",
    "                                            model = load_model_op.output,\n",
    "                                            job_display_name = BATCH_PREDICTIONS_DISPLAY_NAME,\n",
    "                                            gcs_source_uris = GCS_SOURCE_INPUT_URI, \n",
    "                                            gcs_destination_output_uri_prefix = GCS_DESTINATION_OUTPUT_URI, \n",
    "                                            instances_format = INSTANCES_FORMAT,\n",
    "                                            predictions_format = PREDICTIONS_FORMAT,\n",
    "                                            machine_type = MACHINE_TYPE,\n",
    "                                            starting_replica_count=20,\n",
    "                                            max_replica_count=30\n",
    "                                        )\n",
    "\n",
    "    model_training_op.after(import_data_op)\n",
    "    model_evaluation_op.after(model_training_op)\n",
    "    upload_model_to_mr_op.after(model_evaluation_op)\n",
    "    load_model_op.after(upload_model_to_mr_op)\n",
    "    batch_prediction.after(load_model_op)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee492c4-c526-497a-ba7d-cd4d163da6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.oauth2.credentials\n",
    "import json\n",
    "\n",
    "token = !gcloud auth print-access-token\n",
    "CREDENTIALS = google.oauth2.credentials.Credentials(token[0])\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "   pipeline_func=pipeline, package_path=\"pipeline.json\"\n",
    ")\n",
    "\n",
    "job = pipeline_jobs.PipelineJob(\n",
    "   display_name='breast-cancer-pipeline',\n",
    "   template_path=\"pipeline.json\",\n",
    "   credentials = CREDENTIALS,\n",
    "   pipeline_root = f\"gs://{FILE_BUCKET}\",\n",
    "   location=REGION,\n",
    "   enable_caching=False # I encourage you to enable caching when testing as it will reduce resource use\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f824254c-27f6-4aef-aa42-3911eaca0901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88857d8-9b3c-4473-b64d-60bf34bc7575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af52f31-2d56-40e5-9d23-42124f5c988b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474ae135-771b-44ea-aba0-2b90e958b652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113878d5-705d-4adf-9851-9b1b03a264bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
