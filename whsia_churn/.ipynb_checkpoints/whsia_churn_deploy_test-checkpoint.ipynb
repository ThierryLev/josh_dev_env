{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68441085-a84f-4681-9cc6-2980bd456c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def main(mapping):\n",
    "    print(mapping)\n",
    "    from kfp import dsl\n",
    "    from kfp.v2.dsl import component\n",
    "    from datetime import date\n",
    "    from dateutil.relativedelta import relativedelta\n",
    "\n",
    "    SERVICE_TYPE = 'whsia-churn'\n",
    "    DATASET_ID = 'whsia_churn_dataset'\n",
    "    PROJECT_ID = mapping['PROJECT_ID']\n",
    "    RESOURCE_BUCKET = mapping['resources_bucket']\n",
    "    FILE_BUCKET = mapping['gcs_csv_bucket']\n",
    "    REGION = mapping['REGION']\n",
    "    FOLDER_NAME = 'whsia_churn_deploy'\n",
    "    TABLE_ID = 'bq_whsia_churn_score'\n",
    "    QUERIES_PATH = 'vertex_pipelines/' + FOLDER_NAME + '/queries/'\n",
    "\n",
    "    QUERY_DATE = (date.today() - relativedelta(days=1)).strftime('%Y-%m-%d')\n",
    "    wHSIA_QUERY_VIEW_NAME = 'whsia_query_path_view'\n",
    "    wHSIA_QUERY_PATH = QUERIES_PATH + 'create_input_account_active_whsia_bans_query.txt'\n",
    "    TARGET_TABLE_REF = '{}.{}.{}'.format(PROJECT_ID, DATASET_ID, TABLE_ID)\n",
    "    \n",
    "    @component(\n",
    "        base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/vertex_pipelines/kfp-preprocess-slim:latest\",\n",
    "        output_component_file=\"whsia_churn_model_bans_list.yaml\",\n",
    "    )\n",
    "    def create_wHSIA_view(view_name: str,\n",
    "                        query_date: str,\n",
    "                        project_id: str,\n",
    "                        dataset_id: str,\n",
    "                        region: str,\n",
    "                        resource_bucket: str,\n",
    "                        query_path: str,\n",
    "                        ):\n",
    "\n",
    "        from google.cloud import bigquery\n",
    "        from google.cloud import storage\n",
    "\n",
    "        def if_tbl_exists(client, table_ref):\n",
    "            from google.cloud.exceptions import NotFound\n",
    "            try:\n",
    "                client.get_table(table_ref)\n",
    "                return True\n",
    "            except NotFound:\n",
    "                return False\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        dataset = bq_client.dataset(dataset_id)\n",
    "        table_ref = dataset.table(view_name)\n",
    "\n",
    "        # load query from .txt file\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(resource_bucket)\n",
    "        blob = bucket.get_blob(query_path)\n",
    "        content = blob.download_as_string()\n",
    "        content = str(content, 'utf-8')\n",
    "\n",
    "        if if_tbl_exists(bq_client, table_ref):\n",
    "            bq_client.delete_table(table_ref)\n",
    "\n",
    "        create_wHSIA_query = content.format(query_date=query_date)\n",
    "        shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "        base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "        base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "        base_feature_set_view.view_query = create_wHSIA_query\n",
    "        base_feature_set_view = bq_client.create_table(base_feature_set_view)\n",
    "\n",
    "    @component(\n",
    "        base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/vertex_pipelines/kfp-preprocess-slim:latest\",\n",
    "        output_component_file=\"whsia_churn_process.yaml\",\n",
    "    )\n",
    "    def wHSIA_processing(wHSIA_view: str,\n",
    "                       project_id: str,\n",
    "                       dataset_id: str,\n",
    "                       table_id: str,\n",
    "                       query_date: str,\n",
    "                       file_bucket: str,\n",
    "                       ):\n",
    "\n",
    "        from google.cloud import bigquery\n",
    "        import pandas as pd\n",
    "        import time\n",
    "        \n",
    "        def upsert_table(project_id, dataset_id, table_id, sql, result):\n",
    "            new_values = ',\\n'.join(result.apply(lambda row: row_format(row), axis=1))\n",
    "            new_sql = sql.format(proj_id=project_id, dataset_id=dataset_id, table_id=table_id,\n",
    "                                 new_values=new_values)\n",
    "            bq_client = bigquery.Client(project=project_id)\n",
    "            code = bq_client.query(new_sql)\n",
    "            time.sleep(5)\n",
    "\n",
    "        def row_format(row):\n",
    "            values = row.values\n",
    "            new_values = \"\"\n",
    "            v = str(values[0]) if not pd.isnull(values[0]) else 'NULL'\n",
    "            if 'str' in str(type(values[0])):\n",
    "                new_values += f\"'{v}'\"\n",
    "            else:\n",
    "                new_values += f\"{v}\"\n",
    "\n",
    "            for i in range(1, len(values)):\n",
    "                v = str(values[i]) if not pd.isnull(values[i]) else 'NULL'\n",
    "                if 'str' in str(type(values[i])):\n",
    "                    new_values += f\",'{v}'\"\n",
    "                else:\n",
    "                    new_values += f\",{v}\"\n",
    "            return '(' + new_values + ')'\n",
    "\n",
    "        def generate_sql_file(ll):\n",
    "            s = 'MERGE INTO `{proj_id}.{dataset_id}.{table_id}` a'\n",
    "            s += \" USING UNNEST(\"\n",
    "            s += \"[struct<\"\n",
    "            for i in range(len(ll) - 1):\n",
    "                v = ll[i]\n",
    "                s += \"{} {},\".format(v[0], v[1])\n",
    "            s += \"{} {}\".format(ll[-1][0], ll[-1][1])\n",
    "            s += \">{new_values}]\"\n",
    "            s += \") b\"\n",
    "            s += \" ON a.score_date = b.score_date and a.ban = b.ban\"\n",
    "            s += \" WHEN MATCHED THEN\"\n",
    "            s += \" UPDATE SET \"\n",
    "            s += \"a.{}=b.{},\".format(ll[0][0], ll[0][0])\n",
    "            for i in range(1, len(ll) - 1):\n",
    "                v = ll[i]\n",
    "                s += \"a.{}=b.{},\".format(v[0], v[0])\n",
    "            s += \"a.{}=b.{}\".format(ll[-1][0], ll[-1][0])\n",
    "            s += \" WHEN NOT MATCHED THEN\"\n",
    "            s += \" INSERT(\"\n",
    "            for i in range(len(ll) - 1):\n",
    "                v = ll[i]\n",
    "                s += \"{},\".format(v[0])\n",
    "            s += \"{})\".format(ll[-1][0])\n",
    "            s += \" VALUES(\"\n",
    "            for i in range(len(ll) - 1):\n",
    "                s += \"b.{},\".format(ll[i][0])\n",
    "            s += \"b.{}\".format(ll[-1][0])\n",
    "            s += \")\"\n",
    "\n",
    "            return s \n",
    "\n",
    "        MODEL_ID = '5070'\n",
    "        client = bigquery.Client(project=project_id)\n",
    "        wHSIA_data = f\"{project_id}.{dataset_id}.{wHSIA_view}\"\n",
    "\n",
    "        wHSIA = '''SELECT * FROM `{wHSIA_data}`'''.format(wHSIA_data=wHSIA_data)\n",
    "        df_wHSIA = client.query(wHSIA).to_dataframe()\n",
    "        cols = ['ban', 'score_date', 'model_id', 'score']\n",
    "        df_wHSIA = df_wHSIA[cols]\n",
    "        print('......wHSIA table generated with {} samples'.format(df_wHSIA.shape[0]))\n",
    "\n",
    "        # save current results to bucket for UCAR inputs\n",
    "        file_name = 'gs://{}/ucar/wHSIA_churn.csv.gz'.format(file_bucket)\n",
    "        results = df_wHSIA\n",
    "        results.to_csv(file_name, compression='gzip', index=False)\n",
    "\n",
    "        ll = [('ban', 'string'), ('score_date', 'string'), ('model_id', 'string'), ('score', 'float')]\n",
    "        sql = generate_sql_file(ll)\n",
    "\n",
    "        batch_size = 2000\n",
    "        n_batchs = int(df_wHSIA.shape[0] / batch_size) + 1\n",
    "        print('...... will upsert {} batches'.format(n_batchs))\n",
    "        df_wHSIA['ban'] = df_wHSIA['ban'].astype(str)\n",
    "        df_wHSIA['model_id'] = df_wHSIA['model_id'].astype(str)\n",
    "        df_wHSIA['score_date'] = df_wHSIA['score_date'].astype(str)\n",
    "\n",
    "        for i in range(n_batchs):\n",
    "            s, e = i * batch_size, (i + 1) * batch_size\n",
    "            if e >= df_wHSIA.shape[0]:\n",
    "                e = df_wHSIA.shape[0]\n",
    "\n",
    "            df_temp = df_wHSIA.iloc[s:e]\n",
    "\n",
    "            upsert_table(project_id,\n",
    "                         dataset_id,\n",
    "                         table_id,\n",
    "                         sql,\n",
    "                         df_temp,\n",
    "                         )\n",
    "            if i % 20 == 0:\n",
    "                print('predict for batch {} done'.format(i), end=' ')\n",
    "        \n",
    "        time.sleep(120)\n",
    "        \n",
    "    @dsl.pipeline(\n",
    "        # A name for the pipeline.\n",
    "        name=\"whsia-churn-base-table\",\n",
    "        description='pipeline for whsia churn - part 1'\n",
    "    )\n",
    "    def pipeline(\n",
    "            project_id: str = PROJECT_ID,\n",
    "            region: str = REGION,\n",
    "            resource_bucket: str = RESOURCE_BUCKET,\n",
    "            file_bucket: str = FILE_BUCKET\n",
    "    ):\n",
    "    \n",
    "        # -------------  create ops ---------------\n",
    "        create_wHSIA_view_op = create_wHSIA_view(\n",
    "            view_name=wHSIA_QUERY_VIEW_NAME,\n",
    "            query_date=QUERY_DATE,\n",
    "            project_id=PROJECT_ID,\n",
    "            dataset_id=DATASET_ID,\n",
    "            region=REGION,\n",
    "            resource_bucket=RESOURCE_BUCKET,\n",
    "            query_path=wHSIA_QUERY_PATH,\n",
    "        )        \n",
    "        create_wHSIA_view_op.set_memory_limit('32G')\n",
    "        create_wHSIA_view_op.set_cpu_limit('4')\n",
    "\n",
    "        wHSIA_processing_op = wHSIA_processing(\n",
    "            wHSIA_view=wHSIA_QUERY_VIEW_NAME,\n",
    "            project_id=PROJECT_ID,\n",
    "            dataset_id=DATASET_ID,\n",
    "            table_id=TABLE_ID,\n",
    "            query_date=QUERY_DATE,\n",
    "            file_bucket=FILE_BUCKET,\n",
    "        )\n",
    "        wHSIA_processing_op.set_memory_limit('32G')\n",
    "        wHSIA_processing_op.set_cpu_limit('4')\n",
    "        wHSIA_processing_op.after(create_wHSIA_op)\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e88733f-abb5-4c12-b0a3-ec4c7d711000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "SERVICE_TYPE = 'whsia-churn'\n",
    "DATASET_ID = 'whsia_churn_dataset'\n",
    "PROJECT_ID = 'project_id'\n",
    "FOLDER_NAME = 'whsia_churn_deploy'\n",
    "TABLE_ID = 'bq_whsia_churn_score'\n",
    "QUERIES_PATH = 'vertex_pipelines/' + FOLDER_NAME + '/queries/'\n",
    "\n",
    "QUERY_DATE = (date.today() - relativedelta(days=1)).strftime('%Y-%m-%d')\n",
    "wHSIA_QUERY_PATH_NAME = 'whsia_query_path_view'\n",
    "wHSIA_QUERY_PATH = QUERIES_PATH + 'wHSIA.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9250aef3-390a-4df3-b6b9-2164986cb00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-03-23'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QUERY_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af8b817b-1184-4b67-91ab-1ae000f151ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wHSIA_QUERY_PATH_view'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wHSIA_QUERY_PATH_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "016bf0eb-9f4d-4ca3-934e-db2b6f00f0b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vertex_pipelines/whsia_churn_deploy/queries/wHSIA.txt'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wHSIA_QUERY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a4b122-fb66-4475-9df9-8d8ef25dfe36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
