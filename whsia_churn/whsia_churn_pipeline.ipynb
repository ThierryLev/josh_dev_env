{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be7e9a6c-4023-4b64-9384-e36f25c2cba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import google\n",
    "from google.oauth2 import credentials\n",
    "from google.oauth2 import service_account\n",
    "from google.oauth2.service_account import Credentials\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "SERVICE_TYPE = 'whsia-churn'\n",
    "PROJECT_ID = 'divg-josh-pr-d1cc3a'\n",
    "DATASET_ID = 'whsia_churn_dataset'\n",
    "TABLE_ID = 'bq_whsia_churn_score'\n",
    "RESOURCE_BUCKET = 'divg-josh-pr-d1cc3a-default'\n",
    "FILE_BUCKET = 'divg-josh-pr-d1cc3a-default'\n",
    "REGION = 'northamerica-northeast1'\n",
    "FOLDER_NAME = 'whsia_churn_deploy'\n",
    "QUERIES_PATH = 'vertex_pipelines/' + FOLDER_NAME + '/queries/'\n",
    "\n",
    "QUERY_DATE = (date.today() - relativedelta(days=1)).strftime('%Y-%m-%d')\n",
    "wHSIA_QUERY_VIEW_NAME = 'whsia_query_path_view'\n",
    "wHSIA_QUERY_PATH = QUERIES_PATH + 'whsia_bans_query.txt'\n",
    "TARGET_TABLE_REF = '{}.{}.{}'.format(PROJECT_ID, DATASET_ID, TABLE_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91e50e94-24da-4f48-9850-df59b3893e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wHSIA_view(view_name: str,\n",
    "                    query_date: str,\n",
    "                    project_id: str,\n",
    "                    dataset_id: str,\n",
    "                    region: str,\n",
    "                    resource_bucket: str,\n",
    "                    query_path: str,\n",
    "                    ):\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "\n",
    "    def if_tbl_exists(client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    bq_client = get_gcp_bqclient(project_id)\n",
    "    dataset = bq_client.dataset(dataset_id)\n",
    "    table_ref = dataset.table(view_name)\n",
    "\n",
    "    # load query from .txt file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(resource_bucket)\n",
    "    blob = bucket.get_blob(query_path)\n",
    "    content = blob.download_as_string()\n",
    "    content = str(content, 'utf-8')\n",
    "\n",
    "    if if_tbl_exists(bq_client, table_ref):\n",
    "        bq_client.delete_table(table_ref)\n",
    "\n",
    "    create_wHSIA_query = content.format(query_date=query_date)\n",
    "    shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "    base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "    base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "    base_feature_set_view.view_query = create_wHSIA_query\n",
    "    base_feature_set_view = bq_client.create_table(base_feature_set_view)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3afc18b2-b809-44e8-9e4e-061af70d9f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wHSIA_processing(wHSIA_view: str,\n",
    "                   project_id: str,\n",
    "                   dataset_id: str,\n",
    "                   table_id: str,\n",
    "                   query_date: str,\n",
    "                   file_bucket: str,\n",
    "                   ):\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import time\n",
    "\n",
    "    def upsert_table(project_id, dataset_id, table_id, sql, result):\n",
    "        new_values = ',\\n'.join(result.apply(lambda row: row_format(row), axis=1))\n",
    "        \n",
    "        new_sql = sql.format(proj_id=project_id, dataset_id=dataset_id, table_id=table_id,\n",
    "                             new_values=new_values)\n",
    "        \n",
    "        def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "            token = os.popen('gcloud auth print-access-token').read()\n",
    "            token = re.sub(f'\\n$', '', token)\n",
    "            credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "            bq_client = bigquery.Client(project=project_id)\n",
    "            if use_local_credential:\n",
    "                bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "            return bq_client\n",
    "\n",
    "        # bq_client = bigquery.Client(project=project_id)\n",
    "        bq_client = get_gcp_bqclient(project_id)\n",
    "        \n",
    "        code = bq_client.query(new_sql)\n",
    "        time.sleep(5)\n",
    "\n",
    "    def row_format(row):\n",
    "        values = row.values\n",
    "        new_values = \"\"\n",
    "        v = str(values[0]) if not pd.isnull(values[0]) else 'NULL'\n",
    "        if 'str' in str(type(values[0])):\n",
    "            new_values += f\"'{v}'\"\n",
    "        else:\n",
    "            new_values += f\"{v}\"\n",
    "\n",
    "        for i in range(1, len(values)):\n",
    "            v = str(values[i]) if not pd.isnull(values[i]) else 'NULL'\n",
    "            if 'str' in str(type(values[i])):\n",
    "                new_values += f\",'{v}'\"\n",
    "            else:\n",
    "                new_values += f\",{v}\"\n",
    "        return '(' + new_values + ')'\n",
    "\n",
    "    def generate_sql_file(ll):\n",
    "        s = 'MERGE INTO `{proj_id}.{dataset_id}.{table_id}` a'\n",
    "        s += \" USING UNNEST(\"\n",
    "        s += \"[struct<\"\n",
    "        for i in range(len(ll) - 1):\n",
    "            v = ll[i]\n",
    "            s += \"{} {},\".format(v[0], v[1])\n",
    "        s += \"{} {}\".format(ll[-1][0], ll[-1][1])\n",
    "        s += \">{new_values}]\"\n",
    "        s += \") b\"\n",
    "        s += \" ON a.score_date = b.score_date and a.ban = b.ban\"\n",
    "        s += \" WHEN MATCHED THEN\"\n",
    "        s += \" UPDATE SET \"\n",
    "        s += \"a.{}=b.{},\".format(ll[0][0], ll[0][0])\n",
    "        for i in range(1, len(ll) - 1):\n",
    "            v = ll[i]\n",
    "            s += \"a.{}=b.{},\".format(v[0], v[0])\n",
    "        s += \"a.{}=b.{}\".format(ll[-1][0], ll[-1][0])\n",
    "        s += \" WHEN NOT MATCHED THEN\"\n",
    "        s += \" INSERT(\"\n",
    "        for i in range(len(ll) - 1):\n",
    "            v = ll[i]\n",
    "            s += \"{},\".format(v[0])\n",
    "        s += \"{})\".format(ll[-1][0])\n",
    "        s += \" VALUES(\"\n",
    "        for i in range(len(ll) - 1):\n",
    "            s += \"b.{},\".format(ll[i][0])\n",
    "        s += \"b.{}\".format(ll[-1][0])\n",
    "        s += \")\"\n",
    "\n",
    "        return s \n",
    "\n",
    "    MODEL_ID = '5070'\n",
    "    \n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    bq_client = get_gcp_bqclient(project_id)\n",
    "    \n",
    "    wHSIA_data = f\"{project_id}.{dataset_id}.{wHSIA_view}\"\n",
    "\n",
    "    wHSIA = '''SELECT * FROM `{wHSIA_data}`'''.format(wHSIA_data=wHSIA_data)\n",
    "    df_wHSIA = bq_client.query(wHSIA).to_dataframe()\n",
    "    cols = ['ban', 'score_date', 'model_id', 'score']\n",
    "    df_wHSIA = df_wHSIA[cols]\n",
    "    print('......wHSIA table generated with {} samples'.format(df_wHSIA.shape[0]))\n",
    "\n",
    "    # save current results to bucket for UCAR inputs\n",
    "    file_name = 'gs://{}/ucar/wHSIA_churn.csv.gz'.format(file_bucket)\n",
    "    results = df_wHSIA\n",
    "    results.to_csv(file_name, compression='gzip', index=False)\n",
    "\n",
    "    ll = [('ban', 'string'), ('score_date', 'string'), ('model_id', 'string'), ('score', 'float64')]\n",
    "    sql = generate_sql_file(ll)\n",
    "\n",
    "    batch_size = 2000\n",
    "    n_batchs = int(df_wHSIA.shape[0] / batch_size) + 1\n",
    "    print('...... will upsert {} batches'.format(n_batchs))\n",
    "    df_wHSIA['ban'] = df_wHSIA['ban'].astype(str)\n",
    "    df_wHSIA['model_id'] = df_wHSIA['model_id'].astype(str)\n",
    "    df_wHSIA['score_date'] = df_wHSIA['score_date'].astype(str)\n",
    "    \n",
    "    print(df_wHSIA.head())\n",
    "    print(df_wHSIA.shape)\n",
    "\n",
    "    # all_scores = np.array(df_wHSIA['score'].values)\n",
    "    for i in range(n_batchs):\n",
    "        s, e = i * batch_size, (i + 1) * batch_size\n",
    "        if e >= df_wHSIA.shape[0]:\n",
    "            e = df_wHSIA.shape[0]\n",
    "\n",
    "        df_temp = df_wHSIA.iloc[s:e]\n",
    "#         pred_prob = all_scores[s:e]\n",
    "        \n",
    "#         batch_result = pd.DataFrame(columns=['ban', 'score_date', 'model_id', 'score'])\n",
    "#         batch_result['score'] = list(pred_prob)\n",
    "#         batch_result['score'] = batch_result['score'].fillna(0.0).astype('float64')\n",
    "#         batch_result['ban'] = list(df_temp['ban'])\n",
    "#         batch_result['ban'] = batch_result['ban'].astype('str')\n",
    "#         batch_result['score_date'] = score_date_dash\n",
    "#         batch_result['model_id'] = MODEL_ID\n",
    "        \n",
    "        upsert_table(project_id,\n",
    "                     dataset_id,\n",
    "                     table_id,\n",
    "                     sql,\n",
    "                     df_temp,\n",
    "                     )\n",
    "        if i % 20 == 0:\n",
    "            print('predict for batch {} done'.format(i), end=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "225bd8c9-b0d1-4749-a3c9-d70e75b40375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(\n",
    "        project_id: str = PROJECT_ID,\n",
    "        region: str = REGION,\n",
    "        resource_bucket: str = RESOURCE_BUCKET\n",
    "):\n",
    "\n",
    "    # -------------  create ops ---------------\n",
    "    create_wHSIA_view_op = create_wHSIA_view(\n",
    "        view_name=wHSIA_QUERY_VIEW_NAME,\n",
    "        query_date=QUERY_DATE,\n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID,\n",
    "        region=REGION,\n",
    "        resource_bucket=RESOURCE_BUCKET,\n",
    "        query_path=wHSIA_QUERY_PATH\n",
    "    )        \n",
    "\n",
    "    wHSIA_processing_op = wHSIA_processing(\n",
    "        wHSIA_view=wHSIA_QUERY_VIEW_NAME,\n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID,\n",
    "        table_id=TABLE_ID,\n",
    "        query_date=QUERY_DATE,\n",
    "        file_bucket=FILE_BUCKET\n",
    "    )\n",
    "    \n",
    "    create_wHSIA_view_op\n",
    "    wHSIA_processing_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24db610d-08c0-43f4-af09-5533c91eaecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......wHSIA table generated with 10000 samples\n",
      "...... will upsert 6 batches\n",
      "         ban  score_date model_id     score\n",
      "0  214240481  2023-03-15     5060  0.188538\n",
      "1  605221661  2023-03-15     5060  0.191681\n",
      "2  603041261  2023-03-15     5060  0.192536\n",
      "3  604181301  2023-03-15     5060  0.192536\n",
      "4  603936692  2023-03-15     5060  0.188538\n",
      "(10000, 4)\n",
      "predict for batch 0 done "
     ]
    }
   ],
   "source": [
    "pipeline(project_id = PROJECT_ID, region = REGION, resource_bucket = RESOURCE_BUCKET, file_bucket = FILE_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a18f7c-dc53-44f3-91f8-2cf993290c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
