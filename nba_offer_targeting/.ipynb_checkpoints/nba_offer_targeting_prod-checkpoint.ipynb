{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02186b6a-7ff0-47bc-80ed-e8d1411e28fc",
   "metadata": {},
   "source": [
    "### COMPLETED & READY\n",
    "\n",
    "### 1.2.3. bq_import_tbl_to_df (KFP component)\n",
    "\n",
    "this component is used to import a bigquery table and convert to df, then write to gcs as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64f0d9c-a62f-44c3-afc4-5120d034ea07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import pandas as pd\n",
    "from kfp import dsl\n",
    "# from kfp.v2.dsl import (Model, Input, component)\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,HTML,\n",
    "                        OutputPath, ClassificationMetrics, Metrics, component)\n",
    "from typing import NamedTuple\n",
    "# Create Training Dataset for training pipeline\n",
    "\n",
    "@component(\n",
    "    base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/kfp-pycaret-slim:latest\",\n",
    "    output_component_file=\"bq_import_tbl_to_df.yaml\",\n",
    ")\n",
    "def bq_import_tbl_to_df(project_id: str\n",
    "              , dataset_id: str\n",
    "              , table_id: str\n",
    "              , region: str\n",
    "              , save_data_path: str\n",
    "              , token: str \n",
    "              ):\n",
    " \n",
    "    from google.cloud import bigquery\n",
    "    import logging\n",
    "    from datetime import datetime\n",
    "    \n",
    "    #### For wb\n",
    "    import google.oauth2.credentials\n",
    "    CREDENTIALS = google.oauth2.credentials.Credentials(token)\n",
    "    \n",
    "    client = bigquery.Client(project=project_id, credentials=CREDENTIALS)\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    \n",
    "#     #### For prod \n",
    "#     client = bigquery.Client(project=project_id)\n",
    "#     job_config = bigquery.QueryJobConfig()\n",
    "    \n",
    "    # Change dataset / table + sp table name to version in bi-layer\n",
    "    query =\\\n",
    "        f'''\n",
    "            SELECT * FROM `{project_id}.{dataset_id}.{table_id}`\n",
    "        '''\n",
    "    \n",
    "    df = client.query(query, job_config=job_config).to_dataframe()\n",
    "    \n",
    "    df.to_csv(save_data_path, index=True) \n",
    "\n",
    "    col_list = list([col for col in df.columns])\n",
    "    \n",
    "    return (col_list,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9c2606-f4e0-4ee4-83e9-70b5add491f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bq_import_tbl_to_df(project_id: str\n",
    "              , dataset_id: str\n",
    "              , table_id: str\n",
    "              , region: str\n",
    "              , save_data_path: str\n",
    "              , token: str \n",
    "              ): \n",
    " \n",
    "    from google.cloud import bigquery\n",
    "    import logging\n",
    "    from datetime import datetime\n",
    "    \n",
    "    #### For wb\n",
    "    import google.oauth2.credentials\n",
    "    CREDENTIALS = google.oauth2.credentials.Credentials(token)\n",
    "    \n",
    "    client = bigquery.Client(project=project_id, credentials=CREDENTIALS)\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    \n",
    "#     #### For prod \n",
    "#     client = bigquery.Client(project=project_id)\n",
    "#     job_config = bigquery.QueryJobConfig()\n",
    "    \n",
    "    # Change dataset / table + sp table name to version in bi-layer\n",
    "    query =\\\n",
    "        f'''\n",
    "            SELECT * FROM `{project_id}.{dataset_id}.{table_id}`\n",
    "        '''\n",
    "    \n",
    "    df = client.query(query, job_config=job_config).to_dataframe()\n",
    "    \n",
    "    df.to_csv(save_data_path, index=False) \n",
    "\n",
    "#     col_list = list([col for col in df.columns])\n",
    "    \n",
    "#     return (col_list,)\n",
    "\n",
    "    print(f'csv successfully written to {save_data_path}')\n",
    "\n",
    "\n",
    "import google.oauth2.credentials\n",
    "token = !gcloud auth print-access-token\n",
    "token_str = token[0]\n",
    "\n",
    "project_id = \"divg-groovyhoon-pr-d2eab4\"\n",
    "dataset_id = \"nba_offer_targeting\" \n",
    "table_id = \"bq_irpc_digital_1p_base\"\n",
    "region = \"northamerica-northeast1\"\n",
    "save_data_path = f'gs://divg-groovyhoon-pr-d2eab4-default/uploads/{table_id}.csv'\n",
    "token = token_str\n",
    "\n",
    "bq_import_tbl_to_df(project_id, dataset_id, table_id, region, save_data_path, token)\n",
    "\n",
    "\n",
    "project_id = \"divg-groovyhoon-pr-d2eab4\"\n",
    "dataset_id = \"nba_offer_targeting\" \n",
    "table_id = \"irpc_offer_1p_plans\"\n",
    "region = \"northamerica-northeast1\"\n",
    "save_data_path = f'gs://divg-groovyhoon-pr-d2eab4-default/uploads/{table_id}.csv'\n",
    "token = token_str\n",
    "\n",
    "bq_import_tbl_to_df(project_id, dataset_id, table_id, region, save_data_path, token)\n",
    "\n",
    "\n",
    "project_id = \"divg-groovyhoon-pr-d2eab4\"\n",
    "dataset_id = \"nba_offer_targeting\" \n",
    "table_id = \"irpc_offer_prices\"\n",
    "region = \"northamerica-northeast1\"\n",
    "save_data_path = f'gs://divg-groovyhoon-pr-d2eab4-default/uploads/{table_id}.csv'\n",
    "token = token_str\n",
    "\n",
    "bq_import_tbl_to_df(project_id, dataset_id, table_id, region, save_data_path, token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd2fa7a-b358-4022-81c4-c3d8caccb361",
   "metadata": {},
   "source": [
    "### 4. offer_attachment \n",
    "- read the above 3 csv's (bq_irpc_digital_1p_base.csv, irpc_offer_1p_plans.csv, irpc_offer_prices.csv \n",
    "- read bq_irpc_digital_1p_base and store in df\n",
    "- convert irpc_offer_1p_plans and irpc_offer_prices to lists and dictionaries\n",
    "\t- list_hsia_speed_1p \n",
    "\t- dict_hsia_details_1p \n",
    "- attach offers 1, 2, 3 to each customer based on current speed, price, and max provisioned speed eligibility (using the lists and dictionaries created)\n",
    "- write df_base with offers attached to gcs as a csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401aac4a-de15-44b5-b7ae-455f113be780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kfp\n",
    "# import pandas as pd\n",
    "# from kfp import dsl\n",
    "# # from kfp.v2.dsl import (Model, Input, component)\n",
    "# from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,HTML,\n",
    "#                         OutputPath, ClassificationMetrics, Metrics, component)\n",
    "# from typing import NamedTuple\n",
    "\n",
    "# @component(\n",
    "#     base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/kfp-pycaret-slim:latest\",\n",
    "#     output_component_file=\"bq_irpc_offers.yaml\",\n",
    "# )\n",
    "def offer_attachment(irpc_base_csv: str\n",
    "                   , irpc_offer_plans_csv: str \n",
    "                   , irpc_offer_prices_csv: str\n",
    "                   , channel: str\n",
    "                   , save_data_path: str\n",
    "                  ): \n",
    "    import pandas as pd \n",
    "    import numpy as np \n",
    "\n",
    "    def convert_df_to_list_dict(df, \n",
    "                                channel: str = 'digital' # digital or casa\n",
    "                                ): \n",
    "\n",
    "        import re\n",
    "        import logging \n",
    "        from datetime import datetime\n",
    "\n",
    "        # Change dataset / table + sp table name to version in bi-layer\n",
    "        if channel == 'digital': \n",
    "            df = df\n",
    "        elif channel == 'casa': \n",
    "            df = df.iloc[:4]\n",
    "        else: \n",
    "            print(\"a parameter 'channel' can only be either 'digital' or 'casa'\"\"\")\n",
    "\n",
    "        # create list_hsia_speed = [250, 500, 1000, 1500, 3000]\n",
    "        list_hsia_speed = df.columns[3:]\n",
    "        list_hsia_speed = [int(re.search(r'\\d+', speed).group()) for speed in list_hsia_speed]\n",
    "\n",
    "        # create dictionary of hsia_prices \n",
    "        # Convert DataFrame to a dictionary\n",
    "        dict_hsia_details = df.to_dict(orient='list')\n",
    "\n",
    "        return list_hsia_speed, dict_hsia_details\n",
    "    \n",
    "    def find_irpc_offers(list_hsia_speed: str, \n",
    "                         dict_hsia_plans_details: str, \n",
    "                         dict_hsia_prices_details: str,\n",
    "                         provisioned_hs_speed_numeric,\n",
    "                         hs_max_speed_numeric, \n",
    "                         total_charges, \n",
    "                         offer_1_ind = True\n",
    "                         ):\n",
    "\n",
    "        import pandas as pd\n",
    "        import numpy as np \n",
    "\n",
    "        if provisioned_hs_speed_numeric is None and hs_max_speed_numeric is None and total_charges is None: \n",
    "            return [None, None, None, None]\n",
    "\n",
    "        ### 1. Find the smallest number in hsia_speed that is greater than provisioned_hs_speed_numeric\n",
    "\n",
    "        # exception: if the current hsia_speed >= 150 AND hsia_speed < 500, then change the value to 499 so that it bypasses 250 \n",
    "        ## Internet 150 customers should not be shown Internet 250 tier, as they are upsped in the backend to speeds of 300mbps ##\n",
    "        if provisioned_hs_speed_numeric >= 150 and provisioned_hs_speed_numeric < 500: \n",
    "            provisioned_hs_speed_numeric = 499\n",
    "        \n",
    "        try: \n",
    "\n",
    "            # store minimum hsia speed available greater than the current speed in 'hsia_speed'\n",
    "            if offer_1_ind == True: \n",
    "                hsia_speed = np.min([spd for spd in list_hsia_speed if spd >= provisioned_hs_speed_numeric])\n",
    "            elif provisioned_hs_speed_numeric == 3000: \n",
    "                hsia_speed = np.min([spd for spd in list_hsia_speed if spd >= provisioned_hs_speed_numeric])\n",
    "            else: \n",
    "                hsia_speed = np.min([spd for spd in list_hsia_speed if spd > provisioned_hs_speed_numeric])\n",
    "        \n",
    "        except ValueError as ve:\n",
    "            # This block will be executed if a ValueError occurs\n",
    "            print(f\"list_hsia_speed: {list_hsia_speed}\") \n",
    "            print(\" \") \n",
    "            print(f\"provisioned_hs_speed_numeric: {provisioned_hs_speed_numeric}\") \n",
    "            print(f\"hsia_speed: {hsia_speed}\")\n",
    "        \n",
    "        ### 2. Find the hsia_speed in list_hsia_speed that the customer is eligible for\n",
    "\n",
    "        list_elig_hsia_speed = [spd for spd in list_hsia_speed if spd >= hsia_speed and spd <= hs_max_speed_numeric] \n",
    "\n",
    "        ### 3. Find the smallest number in dict_hsia_prices_details[hsia_speed] that is greater than total_charges --> 753\n",
    "        # print(f'offer internet speed {hsia_speed}: {dict_hsia_prices_details[hsia_speed]}')\n",
    "        \n",
    "        # print(f'list_hsia_speed: {list_hsia_speed}')\n",
    "        # print(f'dict_hsia_plans_details: {dict_hsia_plans_details}') \n",
    "        # print(f'dict_hsia_prices_details: {dict_hsia_prices_details}') \n",
    "        # print(f'hsia_speed: {hsia_speed}') \n",
    "        \n",
    "        list_hsia_price = dict_hsia_prices_details[f'internet_{hsia_speed}']\n",
    "\n",
    "        if len([price for price in list_hsia_price if price > total_charges]) > 0: \n",
    "            if offer_1_ind == True: \n",
    "                hsia_price = np.min([price for price in list_hsia_price if price >= total_charges])\n",
    "            else: \n",
    "                hsia_price = np.min([price for price in list_hsia_price if price > total_charges])\n",
    "\n",
    "            if hsia_price in list_hsia_price:\n",
    "                plan_idx = list_hsia_price.index(hsia_price)    \n",
    "\n",
    "            ### 4. Call the plan name by hsia_plans[provisioned_hs_speed_numeric]==250][plan_idx==4] --> \"1P: Tier 3 (Internet 250)\"\n",
    "\n",
    "            hsia_plan_name = dict_hsia_plans_details[f'internet_{hsia_speed}'][plan_idx]\n",
    "\n",
    "            return [hsia_speed, hs_max_speed_numeric, hsia_price, hsia_plan_name] \n",
    "\n",
    "        else: \n",
    "\n",
    "            return [None, None, None, None]\n",
    "    \n",
    "    # read the above 3 csv's (bq_irpc_digital_1p_base.csv, irpc_offer_1p_plans.csv, irpc_offer_prices.csv\n",
    "    # read bq_irpc_digital_1p_base and store in df\n",
    "    df_base = pd.read_csv(irpc_base_csv)\n",
    "    df_plans = pd.read_csv(irpc_offer_plans_csv)\n",
    "    df_prices = pd.read_csv(irpc_offer_prices_csv)\n",
    "    \n",
    "    # convert irpc_offer_1p_plans and irpc_offer_prices to lists and dictionaries\n",
    "    # - list_hsia_speed\n",
    "    # - dict_hsia_details\n",
    "    list_hsia_speed, dict_hsia_plans_details = convert_df_to_list_dict(df_plans, channel)\n",
    "    list_hsia_speed, dict_hsia_prices_details = convert_df_to_list_dict(df_prices, channel)\n",
    "    \n",
    "    offer_1_list, offer_2_list, offer_3_list = [] , [], [] \n",
    "\n",
    "    for idx, row in df_base.iterrows():\n",
    "        \n",
    "        provisioned_hs_speed_numeric, hs_max_speed_numeric, total_charges = row['provisioned_hs_speed_numeric'], row['hs_max_speed_numeric'], row['total_charges']\n",
    "\n",
    "        offer_1_hsia_speed, offer_1_hs_max_speed_numeric, offer_1_hsia_price, offer_1_hsia_plan_name = find_irpc_offers(list_hsia_speed, \n",
    "                                                                                                                        dict_hsia_plans_details, \n",
    "                                                                                                                        dict_hsia_prices_details, \n",
    "                                                                                                                        provisioned_hs_speed_numeric, \n",
    "                                                                                                                        hs_max_speed_numeric, \n",
    "                                                                                                                        total_charges, \n",
    "                                                                                                                        True) \n",
    "        \n",
    "        offer_2_hsia_speed, offer_2_hs_max_speed_numeric, offer_2_hsia_price, offer_2_hsia_plan_name = find_irpc_offers(list_hsia_speed, \n",
    "                                                                                                                        dict_hsia_plans_details,\n",
    "                                                                                                                        dict_hsia_prices_details,\n",
    "                                                                                                                        offer_1_hsia_speed,\n",
    "                                                                                                                        offer_1_hs_max_speed_numeric,\n",
    "                                                                                                                        offer_1_hsia_price,\n",
    "                                                                                                                        False) \n",
    "        \n",
    "        offer_3_hsia_speed, offer_3_hs_max_speed_numeric, offer_3_hsia_price, offer_3_hsia_plan_name = find_irpc_offers(list_hsia_speed,\n",
    "                                                                                                                        dict_hsia_plans_details,\n",
    "                                                                                                                        dict_hsia_prices_details,\n",
    "                                                                                                                        offer_2_hsia_speed,\n",
    "                                                                                                                        offer_2_hs_max_speed_numeric,\n",
    "                                                                                                                        offer_2_hsia_price,\n",
    "                                                                                                                        False) \n",
    "        \n",
    "        offer_1_list.append(offer_1_hsia_plan_name)\n",
    "        offer_2_list.append(offer_2_hsia_plan_name)\n",
    "        offer_3_list.append(offer_3_hsia_plan_name)\n",
    "\n",
    "    df_base['promo_seg1'] = offer_1_list\n",
    "    df_base['promo_seg2'] = offer_2_list\n",
    "    df_base['promo_seg3'] = offer_3_list\n",
    "\n",
    "    df_base.to_csv(save_data_path)\n",
    "    \n",
    "    print(f'csv successfully written to {save_data_path}')\n",
    "    \n",
    "irpc_base_csv = 'gs://divg-groovyhoon-pr-d2eab4-default/uploads/bq_irpc_digital_1p_base.csv'\n",
    "irpc_offer_plans_csv = 'gs://divg-groovyhoon-pr-d2eab4-default/uploads/irpc_offer_1p_plans.csv'\n",
    "irpc_offer_prices_csv = 'gs://divg-groovyhoon-pr-d2eab4-default/uploads/irpc_offer_prices.csv'\n",
    "    \n",
    "offer_attachment(irpc_base_csv, irpc_offer_plans_csv, irpc_offer_prices_csv, 'digital', 'gs://divg-groovyhoon-pr-d2eab4-default/uploads/irpc_digital_1p_base_with_offers.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72783e98-440e-4856-bc9d-38f29f16fd5b",
   "metadata": {},
   "source": [
    "### 5. postprocess\n",
    "\n",
    "- read one of { irpc_digital_1p_base_with_offers.csv,  irpc_digital_2p_base_with_offers.csv, irpc_casa_base_with_offers.csv }import pandas as pd\n",
    "\n",
    "- attach \"Category\", \"Subcategory\", \"rpp_hsia_end_dt\", \"ASSMT_VALID_START_TS\", \"ASSMT_VALID_END_TS\", \"rk\" to 3 offers\n",
    "- ***pay attention to the rows that have \"None\" values in the promo_seg1, promo_seg2, promo_seg3 \n",
    "- concatenate concatenate digital_1p_reco1 + digital_1p_reco2 + digital_1p_reco3 \n",
    "- write .csv to gcs (digital_1p_base_irpc_offers.csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094d4086-b1fb-48b0-b07b-e389256d9e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def postprocess(project_id: str\n",
    "              , dataset_id: str\n",
    "              , table_id: str\n",
    "              , save_data_path: str\n",
    "              , base_type: str # digital_1p, digital_2p, casa \n",
    "              , token: str\n",
    "              ): \n",
    "    \n",
    "    import pandas as pd \n",
    "    import numpy as np \n",
    "    import datetime as dt \n",
    "    from google.cloud import bigquery\n",
    "    import logging\n",
    "    from datetime import datetime\n",
    "    \n",
    "    df = pd.read_csv(data_path)\n",
    "    \n",
    "    if base_type == \"digital_1p\" or base_type == \"casa\": \n",
    "        rk = [11, 21, 31]\n",
    "    elif base_type == \"digital_2p\": \n",
    "        rk = [10, 20, 30]\n",
    "    else: \n",
    "        print(\"\"\"a parameter 'base type' can only accept 'digital_1p', 'digital_2p', or 'casa' as input values\"\"\")\n",
    "\n",
    "    def create_dict_from_df(df):\n",
    "        \n",
    "        # Convert DataFrame to a list of dictionaries\n",
    "        records = df.to_dict(orient='records')\n",
    "\n",
    "        # Create the desired dictionary format\n",
    "        result_dict = {record[df.columns[0]]: [record[df.columns[1]], record[df.columns[2]]] for record in records}\n",
    "\n",
    "        return result_dict\n",
    "\n",
    "    #### For wb\n",
    "    import google.oauth2.credentials\n",
    "    CREDENTIALS = google.oauth2.credentials.Credentials(token)\n",
    "    \n",
    "    client = bigquery.Client(project=project_id, credentials=CREDENTIALS)\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    \n",
    "#     #### For prod \n",
    "#     client = bigquery.Client(project=project_id)\n",
    "#     job_config = bigquery.QueryJobConfig()\n",
    "    \n",
    "    # Change dataset / table + sp table name to version in bi-layer\n",
    "    query =\\\n",
    "        f'''\n",
    "            SELECT * FROM `{project_id}.{dataset_id}.{table_id}`\n",
    "        '''\n",
    "    \n",
    "    df_offer_details = client.query(query, job_config=job_config).to_dataframe()\n",
    "    \n",
    "#     if 'digital' in base_type: \n",
    "#         # read table `divg-groovyhoon-pr-d2eab4.nba_offer_targeting.nba_digital_offer_details` and store in df_offer_details \n",
    "#     elif 'casa' in base_type: \n",
    "#         # read table `divg-groovyhoon-pr-d2eab4.nba_offer_targeting.nba_casa_offer_details` and store in df_offer_details \n",
    "        \n",
    "    dict_offer_details = create_dict_from_df(df_offer_details)\n",
    "    \n",
    "    ### reco1\n",
    "\n",
    "    irpc_reco1 = df[df['promo_seg1'] !='']\n",
    "    irpc_reco1 = irpc_reco1.dropna(subset=['promo_seg1'], how='all')\n",
    "    new_1p_df1 = irpc_reco1['promo_seg1'].apply(lambda x: dict_offer_details[x]).apply(pd.Series) \n",
    "    ### Or, read the second and third column of digital & casa details tables where promo_seg1 == promo_seg1.value\n",
    "    irpc_reco1['promo_seg'] = new_1p_df1[0]\n",
    "    irpc_reco1['offer_code'] = new_1p_df1[1]\n",
    "    irpc_reco1['Category'] = 'Digital Renewal'\n",
    "    irpc_reco1['Subcategory'] = 'Internet'\n",
    "    irpc_reco1['rpp_hsia_end_dt'] = pd.to_datetime(irpc_reco1['rpp_hsia_end_dt'], utc=True)\n",
    "    irpc_reco1['rpp_ttv_end_dt'] = pd.to_datetime(irpc_reco1['rpp_ttv_end_dt'], utc=True)\n",
    "    irpc_reco1.loc[irpc_reco1[\"rpp_hsia_end_dt\"].isnull(), \"digital_category\"] = 'Re-contracting'\n",
    "    irpc_reco1.loc[irpc_reco1[\"rpp_hsia_end_dt\"].dt.date > dt.date.today(), \"digital_category\"] = 'Renewal'\n",
    "    irpc_reco1['ASSMT_VALID_START_TS'] = dt.datetime.now()\n",
    "    irpc_reco1['ASSMT_VALID_END_TS'] = dt.datetime.now() + dt.timedelta(days=90)\n",
    "    irpc_reco1['rk'] = rk[0]\n",
    "    \n",
    "    ### reco2\n",
    "\n",
    "    irpc_reco2 = df[df['promo_seg2'] !='']\n",
    "    irpc_reco2 = irpc_reco2.dropna(subset=['promo_seg2'], how='all')\n",
    "    new_1p_df2 = irpc_reco2['promo_seg2'].apply(lambda x: dict_offer_details[x]).apply(pd.Series)\n",
    "    irpc_reco2['promo_seg'] = new_1p_df2[0]\n",
    "    irpc_reco2['offer_code'] = new_1p_df2[1]\n",
    "    irpc_reco2['Category'] = 'Digital Renewal'\n",
    "    irpc_reco2['Subcategory'] = 'Internet'\n",
    "    irpc_reco2['rpp_hsia_end_dt'] = pd.to_datetime(irpc_reco2['rpp_hsia_end_dt'], utc=True)\n",
    "    irpc_reco2['rpp_ttv_end_dt'] = pd.to_datetime(irpc_reco2['rpp_ttv_end_dt'], utc=True)\n",
    "    irpc_reco2.loc[irpc_reco2[\"rpp_hsia_end_dt\"].isnull(), \"digital_category\"] = 'Re-contracting'\n",
    "    irpc_reco2.loc[irpc_reco2[\"rpp_hsia_end_dt\"].dt.date > dt.date.today(), \"digital_category\"] = 'Renewal'\n",
    "    irpc_reco2['ASSMT_VALID_START_TS'] = dt.datetime.now()\n",
    "    irpc_reco2['ASSMT_VALID_END_TS'] = dt.datetime.now() + dt.timedelta(days=90)\n",
    "    irpc_reco2['rk'] = rk[1]\n",
    "\n",
    "    ### reco3\n",
    "\n",
    "    irpc_reco3 = df[df['promo_seg3'] !='']\n",
    "    irpc_reco3 = irpc_reco3.dropna(subset=['promo_seg3'], how='all')\n",
    "    new_1p_df3 = irpc_reco3['promo_seg3'].apply(lambda x: dict_offer_details[x]).apply(pd.Series)\n",
    "    irpc_reco3['promo_seg'] = new_1p_df3[0]\n",
    "    irpc_reco3['offer_code'] = new_1p_df3[1]\n",
    "    irpc_reco3['Category'] = 'Digital Renewal'\n",
    "    irpc_reco3['Subcategory'] = 'Internet'\n",
    "    irpc_reco3['rpp_hsia_end_dt'] = pd.to_datetime(irpc_reco3['rpp_hsia_end_dt'], utc=True)\n",
    "    irpc_reco3['rpp_ttv_end_dt'] = pd.to_datetime(irpc_reco3['rpp_ttv_end_dt'], utc=True)\n",
    "    irpc_reco3.loc[irpc_reco2[\"rpp_hsia_end_dt\"].isnull(), \"digital_category\"] = 'Re-contracting'\n",
    "    irpc_reco3.loc[irpc_reco2[\"rpp_hsia_end_dt\"].dt.date > dt.date.today(), \"digital_category\"] = 'Renewal'\n",
    "    irpc_reco3['ASSMT_VALID_START_TS'] = dt.datetime.now()\n",
    "    irpc_reco3['ASSMT_VALID_END_TS'] = dt.datetime.now() + dt.timedelta(days=90)\n",
    "    irpc_reco3['rk'] = rk[2]\n",
    "    \n",
    "    # concatenate irpc_reco1 + irpc_reco2 + irpc_reco3 \n",
    "    # write .csv to gcs\n",
    "    irpc_recos = pd.concat([irpc_reco1, irpc_reco2, irpc_reco3], ignore_index=True)\n",
    "    irpc_recos.reset_index(inplace=True)\n",
    "    \n",
    "    irpc_recos.to_csv(save_data_path)\n",
    "    \n",
    "    print(f'csv successfully written to {save_data_path}')\n",
    "    \n",
    "    \n",
    "project_id = \"divg-groovyhoon-pr-d2eab4\"\n",
    "dataset_id = \"nba_offer_targeting\"\n",
    "table_id = \"nba_digital_offer_details\" \n",
    "save_data_path = \"gs://divg-groovyhoon-pr-d2eab4-default/uploads/irpc_digital_1p_base_with_offers.csv\" \n",
    "base_type = \"digital_1p\" \n",
    "\n",
    "import google.oauth2.credentials\n",
    "token = !gcloud auth print-access-token\n",
    "token_str = token[0]\n",
    "\n",
    "postprocess(project_id, dataset_id, table_id, save_data_path, base_type, token_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1dde54-f51f-44fe-b0fd-5e56a68a6e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.cloud import bigquery\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import google.oauth2.credentials\n",
    "token = !gcloud auth print-access-token\n",
    "token_str = token[0]\n",
    "    \n",
    "project_id = \"divg-groovyhoon-pr-d2eab4\"\n",
    "dataset_id = \"nba_offer_targeting\"\n",
    "table_id = \"nba_digital_offer_details\" \n",
    "\n",
    "#### For wb\n",
    "import google.oauth2.credentials\n",
    "CREDENTIALS = google.oauth2.credentials.Credentials(token_str)\n",
    "\n",
    "client = bigquery.Client(project=project_id, credentials=CREDENTIALS)\n",
    "job_config = bigquery.QueryJobConfig()\n",
    "\n",
    "#     #### For prod \n",
    "#     client = bigquery.Client(project=project_id)\n",
    "#     job_config = bigquery.QueryJobConfig()\n",
    "\n",
    "# Change dataset / table + sp table name to version in bi-layer\n",
    "query =\\\n",
    "    f'''\n",
    "        SELECT * FROM `{project_id}.{dataset_id}.{table_id}`\n",
    "    '''\n",
    "\n",
    "df_offer_details = client.query(query, job_config=job_config).to_dataframe()\n",
    "\n",
    "def create_dict_from_df(df):\n",
    "\n",
    "    # Convert DataFrame to a list of dictionaries\n",
    "    records = df.to_dict(orient='records')\n",
    "\n",
    "    # Create the desired dictionary format\n",
    "    result_dict = {record[df.columns[0]]: [record[df.columns[1]], record[df.columns[2]]] for record in records}\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "dict_offer_details = create_dict_from_df(df_offer_details)\n",
    "\n",
    "print(dict_offer_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bac3214-74d0-4eba-b9ff-7736186706f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(data_path)\n",
    "df[df['promo_seg1'] !='']\n",
    "df.to_csv('gs://divg-groovyhoon-pr-d2eab4-default/uploads/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0699a852-caeb-4096-adeb-3248bcf8d28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[10311:10312].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cdf01a-9d3e-4255-9e5c-12e3944f4c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87ab43c-4e83-4dd4-aeea-e33554bc7752",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95ccc05-28e3-4011-9c8d-5eaf572f012c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.dropna(subset=['rpp_hsia_end_dt'], how='all')\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fca076e-6322-4383-98ff-e11884b1c5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt \n",
    "dt.date.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79261e4f-72a0-44b8-a037-c9edc9053785",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3907bb62-17c4-4ef9-8987-04a78025f6a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
