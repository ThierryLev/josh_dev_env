# PIPELINE DEFINITION
# Name: bq-import-tbl-to-df
# Inputs:
#    dataset_id: str
#    project_id: str
#    region: str
#    save_data_path: str
#    table_id: str
#    token: str
components:
  comp-bq-import-tbl-to-df:
    executorLabel: exec-bq-import-tbl-to-df
    inputDefinitions:
      parameters:
        dataset_id:
          parameterType: STRING
        project_id:
          parameterType: STRING
        region:
          parameterType: STRING
        save_data_path:
          parameterType: STRING
        table_id:
          parameterType: STRING
        token:
          parameterType: STRING
deploymentSpec:
  executors:
    exec-bq-import-tbl-to-df:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - bq_import_tbl_to_df
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'kfp==2.0.1'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef bq_import_tbl_to_df(project_id: str\n              , dataset_id:\
          \ str\n              , table_id: str\n              , region: str\n    \
          \          , save_data_path: str\n              , token: str \n        \
          \      ): \n\n    from google.cloud import bigquery\n    import logging\n\
          \    from datetime import datetime\n\n    #### For wb\n    import google.oauth2.credentials\n\
          \    CREDENTIALS = google.oauth2.credentials.Credentials(token)\n\n    client\
          \ = bigquery.Client(project=project_id, credentials=CREDENTIALS)\n    job_config\
          \ = bigquery.QueryJobConfig()\n\n#     #### For prod \n#     client = bigquery.Client(project=project_id)\n\
          #     job_config = bigquery.QueryJobConfig()\n\n    # Change dataset / table\
          \ + sp table name to version in bi-layer\n    query =\\\n        f'''\n\
          \            SELECT * FROM `{project_id}.{dataset_id}.{table_id}`\n    \
          \    '''\n\n    df = client.query(query, job_config=job_config).to_dataframe()\n\
          \n    df.to_csv(save_data_path, index=False) \n\n    col_list = list([col\
          \ for col in df.columns])\n\n    return (col_list,)\n\n"
        image: northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/kfp-pycaret-slim:latest
pipelineInfo:
  name: bq-import-tbl-to-df
root:
  dag:
    tasks:
      bq-import-tbl-to-df:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-bq-import-tbl-to-df
        inputs:
          parameters:
            dataset_id:
              componentInputParameter: dataset_id
            project_id:
              componentInputParameter: project_id
            region:
              componentInputParameter: region
            save_data_path:
              componentInputParameter: save_data_path
            table_id:
              componentInputParameter: table_id
            token:
              componentInputParameter: token
        taskInfo:
          name: bq-import-tbl-to-df
  inputDefinitions:
    parameters:
      dataset_id:
        parameterType: STRING
      project_id:
        parameterType: STRING
      region:
        parameterType: STRING
      save_data_path:
        parameterType: STRING
      table_id:
        parameterType: STRING
      token:
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.0.1
