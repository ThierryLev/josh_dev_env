{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d76e166-1ae6-4cb8-a8c2-79d439b7dad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import google\n",
    "from google.oauth2 import credentials\n",
    "from google.oauth2 import service_account\n",
    "from google.oauth2.service_account import Credentials\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# SERVICE_TYPE = 'tos_cross_sell'\n",
    "# DATASET_ID = '{}_dataset'.format(SERVICE_TYPE)\n",
    "# PROJECT_ID = mapping['PROJECT_ID']\n",
    "# RESOURCE_BUCKET = mapping['resources_bucket']\n",
    "# FILE_BUCKET = mapping['gcs_csv_bucket']\n",
    "# REGION = mapping['REGION']\n",
    "# MODEL_ID = '5060'\n",
    "# FOLDER_NAME = 'xgb_{}_{}_predict_deploy'.format(SERVICE_TYPE, MODEL_ID)\n",
    "# QUERIES_PATH = 'vertex_pipelines/' + FOLDER_NAME + '/queries/'\n",
    "\n",
    "SERVICE_TYPE = 'tos_cross_sell'\n",
    "DATASET_ID = 'tos_cross_sell'\n",
    "PROJECT_ID = 'divg-josh-pr-d1cc3a' #mapping['PROJECT_ID']\n",
    "RESOURCE_BUCKET = 'divg-josh-pr-d1cc3a-default' #mapping['resources_bucket']\n",
    "FILE_BUCKET = 'divg-josh-pr-d1cc3a-default' #mapping['gcs_csv_bucket']\n",
    "REGION = 'northamerica-northeast1' #mapping['REGION']\n",
    "MODEL_ID = '5060'\n",
    "FOLDER_NAME = 'xgb_tos_cross_sell_train_deploy'.format(MODEL_ID)\n",
    "QUERIES_PATH = 'vertex_pipelines/' + FOLDER_NAME + '/queries/'\n",
    "\n",
    "scoringDate = date.today() - relativedelta(days=5)\n",
    "\n",
    "# current day views\n",
    "CONSL_VIEW_NAME = '{}_pipeline_consl_data_curr_bi_layer'.format(SERVICE_TYPE)  # done\n",
    "FFH_BILLING_VIEW_NAME = '{}_pipeline_ffh_billing_data_curr_bi_layer'.format(SERVICE_TYPE)  # done\n",
    "HS_USAGE_VIEW_NAME = '{}_pipeline_hs_usage_data_curr_bi_layer'.format(SERVICE_TYPE)  # done\n",
    "DEMO_INCOME_VIEW_NAME = '{}_pipeline_demo_income_data_curr_bi_layer'.format(SERVICE_TYPE)  # done\n",
    "PROMO_EXPIRY_VIEW_NAME = '{}_pipeline_promo_expiry_data_curr_bi_layer'.format(SERVICE_TYPE)  # done\n",
    "GPON_COPPER_VIEW_NAME = '{}_pipeline_gpon_copper_data_curr_bi_layer'.format(SERVICE_TYPE)  # done\n",
    "CLCKSTRM_TELUS_VIEW_NAME = '{}_pipeline_clckstrm_telus_curr_bi_layer'.format(SERVICE_TYPE)\n",
    "ALARMDOTCOM_APP_USAGE_VIEW_NAME = '{}_pipeline_alarmdotcom_app_usage_curr_bi_layer'.format(SERVICE_TYPE)\n",
    "TOS_ACTIVE_BANS_VIEW_NAME = '{}_pipeline_tos_active_bans_curr_bi_layer'.format(SERVICE_TYPE) \n",
    "\n",
    "# dates\n",
    "SCORE_DATE = scoringDate.strftime('%Y%m%d')  # date.today().strftime('%Y%m%d')\n",
    "SCORE_DATE_DASH = scoringDate.strftime('%Y-%m-%d')\n",
    "SCORE_DATE_MINUS_6_MOS_DASH = ((scoringDate - relativedelta(months=6)).replace(day=1)).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_LAST_MONTH_START_DASH = (scoringDate.replace(day=1) - timedelta(days=1)).replace(day=1).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_LAST_MONTH_END_DASH = ((scoringDate.replace(day=1)) - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_LAST_MONTH_YEAR = ((scoringDate.replace(day=1)) - timedelta(days=1)).year\n",
    "SCORE_DATE_LAST_MONTH_MONTH = ((scoringDate.replace(day=1)) - timedelta(days=1)).month\n",
    "\n",
    "SCORE_DATE_DELTA = 0\n",
    "SCORE_DATE_VAL_DELTA = 0\n",
    "TICKET_DATE_WINDOW = 30  # Days of ticket data to be queried\n",
    "\n",
    "ACCOUNT_CONSL_QUERY_PATH = QUERIES_PATH + 'create_input_account_consl_query.txt'\n",
    "ACCOUNT_GPON_COPPER_QUERY_PATH = QUERIES_PATH + 'create_input_account_gpon_copper_query.txt'\n",
    "ACCOUNT_PROMO_EXPIRY_QUERY_PATH = QUERIES_PATH + 'create_input_account_promo_expiry_query.txt'\n",
    "ACCOUNT_DEMO_INCOME_QUERY_PATH = QUERIES_PATH + 'create_input_account_demo_income_query.txt'\n",
    "ACCOUNT_HS_USAGE_QUERY_PATH = QUERIES_PATH + 'create_input_account_hs_usage_query.txt'\n",
    "ACCOUNT_FFH_BILLING_QUERY_PATH = QUERIES_PATH + 'create_input_account_ffh_billing_query.txt'\n",
    "ACCOUNT_CLCKSTRM_TELUS_QUERY_PATH = QUERIES_PATH + 'create_input_account_clckstrm_telus_query.txt'\n",
    "ACCOUNT_ALARMDOTCOM_APP_USAGE_QUERY_PATH = QUERIES_PATH + 'create_input_account_alarmdotcom_app_usage_query.txt'\n",
    "ACCOUNT_TOS_ACTIVE_BANS_QUERY_PATH = QUERIES_PATH + 'create_input_account_tos_active_bans_query.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "365fc8c2-5bf9-4326-9756-da5d89e5c8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_prediction(\n",
    "        project_id: str,\n",
    "        dataset_id: str,\n",
    "        file_bucket: str,\n",
    "        service_type: str,\n",
    "        score_table: str,\n",
    "        score_date_dash: str\n",
    "        # metrics: Output[Metrics],\n",
    "        # metricsc: Output[ClassificationMetrics],\n",
    "):\n",
    "    import time\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    from datetime import date\n",
    "    from dateutil.relativedelta import relativedelta\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "\n",
    "    MODEL_ID = '5060'\n",
    "\n",
    "    def if_tbl_exists(bq_client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            bq_client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "\n",
    "\n",
    "        \n",
    "    def upsert_table(project_id, dataset_id, table_id, sql, result):\n",
    "        new_values = ',\\n'.join(result.apply(lambda row: row_format(row), axis=1))\n",
    "        new_sql = sql.format(proj_id=project_id, dataset_id=dataset_id, table_id=table_id,\n",
    "                             new_values=new_values)\n",
    "        \n",
    "        def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "            token = os.popen('gcloud auth print-access-token').read()\n",
    "            token = re.sub(f'\\n$', '', token)\n",
    "            credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "            bq_client = bigquery.Client(project=project_id)\n",
    "            if use_local_credential:\n",
    "                bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "            return bq_client\n",
    "\n",
    "        # bq_client = bigquery.Client(project=project_id)\n",
    "        bq_client = get_gcp_bqclient(project_id)\n",
    "        \n",
    "        code = bq_client.query(new_sql)\n",
    "        time.sleep(5)\n",
    "\n",
    "    def row_format(row):\n",
    "        values = row.values\n",
    "        new_values = \"\"\n",
    "        v = str(values[0]) if not pd.isnull(values[0]) else 'NULL'\n",
    "        if 'str' in str(type(values[0])):\n",
    "            new_values += f\"'{v}'\"\n",
    "        else:\n",
    "            new_values += f\"{v}\"\n",
    "\n",
    "        for i in range(1, len(values)):\n",
    "            v = str(values[i]) if not pd.isnull(values[i]) else 'NULL'\n",
    "            if 'str' in str(type(values[i])):\n",
    "                new_values += f\",'{v}'\"\n",
    "            else:\n",
    "                new_values += f\",{v}\"\n",
    "        return '(' + new_values + ')'\n",
    "\n",
    "    def generate_sql_file(ll):\n",
    "        s = 'MERGE INTO `{proj_id}.{dataset_id}.{table_id}` a'\n",
    "        s += \" USING UNNEST(\"\n",
    "        s += \"[struct<\"\n",
    "        for i in range(len(ll) - 1):\n",
    "            v = ll[i]\n",
    "            s += \"{} {},\".format(v[0], v[1])\n",
    "        s += \"{} {}\".format(ll[-1][0], ll[-1][1])\n",
    "        s += \">{new_values}]\"\n",
    "        s += \") b\"\n",
    "        s += \" ON a.ban = b.ban and a.score_date = b.score_date\"\n",
    "        s += \" WHEN MATCHED THEN\"\n",
    "        s += \" UPDATE SET \"\n",
    "        s += \"a.{}=b.{},\".format(ll[0][0], ll[0][0])\n",
    "        for i in range(1, len(ll) - 1):\n",
    "            v = ll[i]\n",
    "            s += \"a.{}=b.{},\".format(v[0], v[0])\n",
    "        s += \"a.{}=b.{}\".format(ll[-1][0], ll[-1][0])\n",
    "        s += \" WHEN NOT MATCHED THEN\"\n",
    "        s += \" INSERT(\"\n",
    "        for i in range(len(ll) - 1):\n",
    "            v = ll[i]\n",
    "            s += \"{},\".format(v[0])\n",
    "        s += \"{})\".format(ll[-1][0])\n",
    "        s += \" VALUES(\"\n",
    "        for i in range(len(ll) - 1):\n",
    "            s += \"b.{},\".format(ll[i][0])\n",
    "        s += \"b.{}\".format(ll[-1][0])\n",
    "        s += \")\"\n",
    "\n",
    "        return s\n",
    "\n",
    "    MODEL_PATH = '{}_xgb_models/'.format(service_type)\n",
    "    df_score = pd.read_csv('gs://{}/{}_score.csv.gz'.format(file_bucket, service_type), compression='gzip')\n",
    "    df_score.dropna(subset=['ban'], inplace=True)\n",
    "    df_score.reset_index(drop=True, inplace=True)\n",
    "    print('......scoring data loaded:{}'.format(df_score.shape))\n",
    "    time.sleep(10)\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(file_bucket)\n",
    "    blobs = storage_client.list_blobs(file_bucket, prefix='{}{}_models_xgb_'.format(MODEL_PATH, service_type))\n",
    "\n",
    "    model_lists = []\n",
    "    for blob in blobs:\n",
    "        model_lists.append(blob.name)\n",
    "\n",
    "    blob = bucket.blob(model_lists[-1])\n",
    "    blob_in = blob.download_as_string()\n",
    "    model_dict = pickle.loads(blob_in)\n",
    "    model_xgb = model_dict['model']\n",
    "    features = model_dict['features']\n",
    "    print('...... model loaded')\n",
    "    time.sleep(10)\n",
    "\n",
    "    ll = [('ban', 'string'), ('score_date', 'string'), ('model_id', 'string'), ('score', 'float64')]\n",
    "    sql = generate_sql_file(ll)\n",
    "\n",
    "    df_score['ban'] = df_score['ban'].astype(int)\n",
    "    print('.... scoring for {} tos cross sell bans base'.format(len(df_score)))\n",
    "\n",
    "    # get full score to cave into bucket\n",
    "    pred_prob = model_xgb.predict_proba(df_score[features], ntree_limit=model_xgb.best_iteration)[:, 1]\n",
    "    result = pd.DataFrame(columns=['ban', 'score_date', 'model_id', 'score'])\n",
    "    result['score'] = list(pred_prob)\n",
    "    result['score'] = result['score'].fillna(0.0).astype('float64')\n",
    "    result['ban'] = list(df_score['ban'])\n",
    "    result['ban'] = result['ban'].astype('str')\n",
    "    result['score_date'] = score_date_dash\n",
    "    result['model_id'] = MODEL_ID\n",
    "\n",
    "    result.to_csv('gs://{}/ucar/{}_prediction.csv.gz'.format(file_bucket, service_type), compression='gzip',\n",
    "                  index=False)\n",
    "    time.sleep(60)\n",
    "\n",
    "    batch_size = 1000\n",
    "    n_batchs = int(df_score.shape[0] / batch_size) + 1\n",
    "    print('...... will upsert {} batches'.format(n_batchs))\n",
    "\n",
    "    # start batch prediction\n",
    "    all_scores = np.array(result['score'].values)\n",
    "    for i in range(n_batchs):\n",
    "\n",
    "        s, e = i * batch_size, (i + 1) * batch_size\n",
    "        if e >= df_score.shape[0]:\n",
    "            e = df_score.shape[0]\n",
    "\n",
    "        df_temp = df_score.iloc[s:e]\n",
    "        pred_prob = all_scores[s:e]\n",
    "        batch_result = pd.DataFrame(columns=['ban', 'score_date', 'model_id', 'score'])\n",
    "        batch_result['score'] = list(pred_prob)\n",
    "        batch_result['score'] = batch_result['score'].fillna(0.0).astype('float64')\n",
    "        batch_result['ban'] = list(df_temp['ban'])\n",
    "        batch_result['ban'] = batch_result['ban'].astype('str')\n",
    "        batch_result['score_date'] = score_date_dash\n",
    "        batch_result['model_id'] = MODEL_ID\n",
    "\n",
    "        upsert_table(project_id,\n",
    "                     dataset_id,\n",
    "                     score_table,\n",
    "                     sql,\n",
    "                     batch_result,\n",
    "                     )\n",
    "        if i % 20 == 0:\n",
    "            print('predict for batch {} done'.format(i), end=' \\n')\n",
    "\n",
    "    time.sleep(120)\n",
    "\n",
    "\n",
    "    #-------------------------------------------------------complete upto here----------------------------------------------------------------#\n",
    "\n",
    "\n",
    "def postprocess(\n",
    "        project_id: str,\n",
    "        file_bucket: str,\n",
    "        service_type: str,\n",
    "        score_date_dash: str,\n",
    "):\n",
    "    import time\n",
    "    import pandas as pd\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    MODEL_ID = '5060'\n",
    "    file_name = 'gs://{}/ucar/{}_prediction.csv.gz'.format(file_bucket, service_type)\n",
    "    df_orig = pd.read_csv(file_name, compression='gzip')\n",
    "    df_orig.dropna(subset=['ban'], inplace=True)\n",
    "    df_orig.reset_index(drop=True, inplace=True)\n",
    "    df_orig['scoring_date'] = score_date_dash\n",
    "    df_orig.ban = df_orig.ban.astype(int)\n",
    "    df_orig = df_orig.rename(columns={'ban': 'bus_bacct_num', 'score': 'score_num'})\n",
    "    df_orig.score_num = df_orig.score_num.astype(float)\n",
    "    df_orig['decile_grp_num'] = pd.qcut(df_orig['score_num'], q=10, labels=False)\n",
    "    df_orig.decile_grp_num = df_orig.decile_grp_num + 1\n",
    "    df_orig['percentile_pct'] = df_orig.score_num.rank(pct=True)\n",
    "    df_orig['predict_model_nm'] = 'FFH TOS CROSS SELL Model - DIVG'\n",
    "    df_orig['model_type_cd'] = 'FFH'\n",
    "    df_orig['subscriber_no'] = \"\"\n",
    "    df_orig['prod_instnc_resrc_str'] = \"\"\n",
    "    df_orig['service_instnc_id'] = \"\"\n",
    "    df_orig['segment_nm'] = \"\"\n",
    "    df_orig['segment_id'] = \"\"\n",
    "    df_orig['classn_nm'] = \"\"\n",
    "    df_orig['predict_model_id'] = MODEL_ID\n",
    "    df_orig.drop(columns=['model_id', 'score_date'], axis=1, inplace=True)\n",
    "\n",
    "    get_cust_id = \"\"\"\n",
    "    WITH bq_snpsht_max_date AS(\n",
    "    SELECT PARSE_DATE('%Y%m%d', MAX(partition_id)) AS max_date\n",
    "        FROM `cio-datahub-enterprise-pr-183a.ent_cust_cust.INFORMATION_SCHEMA.PARTITIONS` \n",
    "    WHERE table_name = 'bq_prod_instnc_snpsht' \n",
    "        AND partition_id <> '__NULL__'\n",
    "    ),\n",
    "    -- BANs can have multiple Cust ID. Create rank by product type and status, prioritizing ban/cust id with active FFH products\n",
    "    rank_prod_type AS (\n",
    "    SELECT DISTINCT\n",
    "        bacct_bus_bacct_num,\n",
    "        consldt_cust_bus_cust_id AS cust_id,\n",
    "        CASE WHEN pi_prod_instnc_resrc_typ_cd IN ('SING', 'HSIC', 'TTV', 'SMHM', 'STV', 'DIIC') AND pi_prod_instnc_stat_cd = 'A' THEN 1\n",
    "                WHEN pi_prod_instnc_resrc_typ_cd IN ('SING', 'HSIC', 'TTV', 'SMHM', 'STV', 'DIIC') THEN 2\n",
    "                WHEN pi_prod_instnc_stat_cd = 'A' THEN 3\n",
    "                ELSE 4\n",
    "                END AS prod_rank\n",
    "    FROM `cio-datahub-enterprise-pr-183a.ent_cust_cust.bq_prod_instnc_snpsht`\n",
    "    CROSS JOIN bq_snpsht_max_date\n",
    "    WHERE CAST(prod_instnc_ts AS DATE)=bq_snpsht_max_date.max_date\n",
    "    AND bus_prod_instnc_src_id = 1001\n",
    "    ),\n",
    "    --Rank Cust ID\n",
    "    rank_cust_id AS (\n",
    "    SELECT DISTINCT\n",
    "        bacct_bus_bacct_num,\n",
    "        cust_id,\n",
    "        RANK() OVER(PARTITION BY bacct_bus_bacct_num\n",
    "                        ORDER BY prod_rank,\n",
    "                                    cust_id) AS cust_id_rank               \n",
    "    FROM rank_prod_type\n",
    "    )\n",
    "    --Select best cust id\n",
    "    SELECT bacct_bus_bacct_num,\n",
    "        cust_id\n",
    "    FROM rank_cust_id\n",
    "    WHERE cust_id_rank = 1\n",
    "    \"\"\"\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    bq_client = get_gcp_bqclient(project_id)\n",
    "    df_cust = bq_client.query(get_cust_id).to_dataframe()\n",
    "    df_final = df_orig.set_index('bus_bacct_num').join(df_cust.set_index('bacct_bus_bacct_num')).reset_index()\n",
    "    df_final = df_final.rename(columns={'index': 'bus_bacct_num', 'cust_bus_cust_id': 'cust_id'})\n",
    "    df_final = df_final.sort_values(by=['score_num'], ascending=False)\n",
    "    df_final.to_csv(file_name, compression='gzip', index=False)\n",
    "    time.sleep(300)\n",
    "\n",
    "def pipeline(\n",
    "            project_id: str = PROJECT_ID,\n",
    "            region: str = REGION,\n",
    "            resource_bucket: str = RESOURCE_BUCKET,\n",
    "            file_bucket: str = FILE_BUCKET\n",
    "    ):\n",
    "    batch_prediction_op = batch_prediction(\n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID,\n",
    "        file_bucket=FILE_BUCKET,\n",
    "        service_type=SERVICE_TYPE,\n",
    "        score_date_dash=SCORE_DATE_DASH,\n",
    "        score_table='bq_tos_cross_sell_score',\n",
    "    )\n",
    "    postprocessing_op = postprocess(\n",
    "        project_id=PROJECT_ID,\n",
    "        file_bucket=FILE_BUCKET,\n",
    "        service_type=SERVICE_TYPE,\n",
    "        score_date_dash=SCORE_DATE_DASH,\n",
    "    )\n",
    "    batch_prediction_op\n",
    "      postprocessing_op\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b5a33c5-95c5-49ed-8646-4973829b9827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......scoring data loaded:(2077417, 69)\n",
      "...... model loaded\n",
      ".... scoring for 2077417 tos cross sell bans base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/xgboost/core.py:90: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...... will upsert 2078 batches\n",
      "predict for batch 0 done \n",
      "predict for batch 20 done \n",
      "predict for batch 40 done \n",
      "predict for batch 60 done \n",
      "predict for batch 80 done \n",
      "predict for batch 100 done \n",
      "predict for batch 120 done \n",
      "predict for batch 140 done \n",
      "predict for batch 160 done \n",
      "predict for batch 180 done \n",
      "predict for batch 200 done \n",
      "predict for batch 220 done \n",
      "predict for batch 240 done \n",
      "predict for batch 260 done \n",
      "predict for batch 280 done \n",
      "predict for batch 300 done \n",
      "predict for batch 320 done \n",
      "predict for batch 340 done \n",
      "predict for batch 360 done \n",
      "predict for batch 380 done \n",
      "predict for batch 400 done \n",
      "predict for batch 420 done \n",
      "predict for batch 440 done \n",
      "predict for batch 460 done \n",
      "predict for batch 480 done \n",
      "predict for batch 500 done \n",
      "predict for batch 520 done \n",
      "predict for batch 540 done \n",
      "predict for batch 560 done \n",
      "predict for batch 580 done \n",
      "predict for batch 600 done \n",
      "predict for batch 620 done \n",
      "predict for batch 640 done \n",
      "predict for batch 660 done \n",
      "predict for batch 680 done \n",
      "predict for batch 700 done \n",
      "predict for batch 720 done \n",
      "predict for batch 740 done \n",
      "predict for batch 760 done \n",
      "predict for batch 780 done \n",
      "predict for batch 800 done \n",
      "predict for batch 820 done \n",
      "predict for batch 840 done \n",
      "predict for batch 860 done \n",
      "predict for batch 880 done \n",
      "predict for batch 900 done \n",
      "predict for batch 920 done \n",
      "predict for batch 940 done \n",
      "predict for batch 960 done \n",
      "predict for batch 980 done \n",
      "predict for batch 1000 done \n",
      "predict for batch 1020 done \n",
      "predict for batch 1040 done \n",
      "predict for batch 1060 done \n",
      "predict for batch 1080 done \n",
      "predict for batch 1100 done \n",
      "predict for batch 1120 done \n",
      "predict for batch 1140 done \n",
      "predict for batch 1160 done \n",
      "predict for batch 1180 done \n",
      "predict for batch 1200 done \n",
      "predict for batch 1220 done \n",
      "predict for batch 1240 done \n",
      "predict for batch 1260 done \n",
      "predict for batch 1280 done \n",
      "predict for batch 1300 done \n",
      "predict for batch 1320 done \n",
      "predict for batch 1340 done \n",
      "predict for batch 1360 done \n",
      "predict for batch 1380 done \n",
      "predict for batch 1400 done \n",
      "predict for batch 1420 done \n",
      "predict for batch 1440 done \n",
      "predict for batch 1460 done \n",
      "predict for batch 1480 done \n",
      "predict for batch 1500 done \n",
      "predict for batch 1520 done \n",
      "predict for batch 1540 done \n",
      "predict for batch 1560 done \n",
      "predict for batch 1580 done \n",
      "predict for batch 1600 done \n",
      "predict for batch 1620 done \n",
      "predict for batch 1640 done \n",
      "predict for batch 1660 done \n",
      "predict for batch 1680 done \n",
      "predict for batch 1700 done \n",
      "predict for batch 1720 done \n",
      "predict for batch 1740 done \n",
      "predict for batch 1760 done \n",
      "predict for batch 1780 done \n",
      "predict for batch 1800 done \n",
      "predict for batch 1820 done \n",
      "predict for batch 1840 done \n",
      "predict for batch 1860 done \n",
      "predict for batch 1880 done \n",
      "predict for batch 1900 done \n",
      "predict for batch 1920 done \n",
      "predict for batch 1940 done \n",
      "predict for batch 1960 done \n",
      "predict for batch 1980 done \n",
      "predict for batch 2000 done \n",
      "predict for batch 2020 done \n",
      "predict for batch 2040 done \n",
      "predict for batch 2060 done \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.pipeline(project_id: str = 'divg-josh-pr-d1cc3a', region: str = 'northamerica-northeast1', resource_bucket: str = 'divg-josh-pr-d1cc3a-default', file_bucket: str = 'divg-josh-pr-d1cc3a-default')>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(project_id = PROJECT_ID, region = REGION, resource_bucket = RESOURCE_BUCKET, file_bucket = FILE_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bc6cb6-f828-46ef-bc3c-e9dadf865b64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
