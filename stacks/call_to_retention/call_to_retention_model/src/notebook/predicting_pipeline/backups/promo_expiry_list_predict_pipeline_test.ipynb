{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c150da3-6e1e-4f02-a778-bb3b12af9054",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b559b7e6-fe51-49cc-8e9c-832380843832",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output, OutputPath, ClassificationMetrics,\n",
    "                        Metrics, component)\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import google\n",
    "from google.oauth2 import credentials\n",
    "from google.oauth2 import service_account\n",
    "from google.oauth2.service_account import Credentials\n",
    "from google.cloud import storage\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google_cloud_pipeline_components.v1.batch_predict_job import \\\n",
    "    ModelBatchPredictOp as batch_prediction_op\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558ff533-70c8-4421-813e-c567d6bc496b",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f70d721-76e7-4c2e-8153-88b5bbe8ee47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tag cell with parameters\n",
    "PROJECT_ID =  ''\n",
    "BUCKET_NAME=''\n",
    "DATASET_ID = ''\n",
    "RESOURCE_BUCKET = ''\n",
    "FILE_BUCKET = ''\n",
    "REGION = ''\n",
    "MODEL_ID = '5090'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7035200a-de6b-412c-8406-64854b639cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag cell with parameters\n",
    "PROJECT_ID =  'divg-josh-pr-d1cc3a'\n",
    "BUCKET_NAME='divg-josh-pr-d1cc3a-default'\n",
    "DATASET_ID = 'call_to_retention_dataset'\n",
    "RESOURCE_BUCKET = 'divg-josh-pr-d1cc3a-default'\n",
    "FILE_BUCKET = 'divg-josh-pr-d1cc3a-default'\n",
    "MODEL_ID = '5090'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eeed21-e12b-45ee-b1e4-d0c8f3843533",
   "metadata": {},
   "source": [
    "### Service Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65d1668-e1d8-4f58-958f-edc44d06a6b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SERVICE_TYPE = 'call_to_retention'\n",
    "SERVICE_TYPE_NAME = 'call-to-retention'\n",
    "TABLE_ID = 'bq_call_to_retention_targets'\n",
    "REGION = \"northamerica-northeast1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2229e5d1-1362-40ed-aa7d-cb22e41e4960",
   "metadata": {},
   "source": [
    "### Pulumi Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d628b-e759-4c9c-aab5-568c54721aaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "STACK_NAME = 'call_to_retention'\n",
    "TRAIN_PIPELINE_NAME_PATH = 'train_pipeline'\n",
    "PREDICT_PIPELINE_NAME_PATH = 'predict_pipeline'\n",
    "TRAIN_PIPELINE_NAME = 'call-to-retention-train-pipeline' # Same name as pulumi.yaml\n",
    "PREDICT_PIPELINE_NAME = 'call-to-retention-predict-pipeline' # Same name as pulumi.yaml\n",
    "TRAIN_PIPELINE_DESCRIPTION = 'call-to-retention-train-pipeline'\n",
    "PREDICT_PIPELINE_DESCRIPTION = 'call-to-retention-predict-pipeline'\n",
    "REGION = \"northamerica-northeast1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85558b11-be96-49ea-91c5-d17b15c00bf1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Query + Pre-Processing Component Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370852f7-2e23-4ac3-8cd8-b40c39be85ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_QUERIES_PATH = f\"{STACK_NAME}/{TRAIN_PIPELINE_NAME_PATH}/queries/\" \n",
    "TRAIN_UTILS_FILE_PATH = f\"{STACK_NAME}/{TRAIN_PIPELINE_NAME_PATH}/utils\" \n",
    "UTILS_FILENAME = 'utils.py'\n",
    "\n",
    "PROCESSED_SERVING_DATA_TABLENAME = 'processed_serving_data'\n",
    "INPUT_SERVING_DATA_TABLENAME = 'input_serving_data'\n",
    "\n",
    "QUERY_DATE = (date.today() - relativedelta(days=1)).strftime('%Y-%m-%d')\n",
    "TARGET_TABLE_REF = '{}.{}.{}'.format(PROJECT_ID, DATASET_ID, TABLE_ID)\n",
    "\n",
    "QUERIES_PATH = 'call_to_retention/queries/'\n",
    "\n",
    "#Query Paths\n",
    "ACCOUNT_PROMO_EXPIRY_LIST_QUERY_PATH = QUERIES_PATH + 'create_input_account_promo_expiry_list_query.sql'\n",
    "ACCOUNT_CONSL_QUERY_PATH = QUERIES_PATH + 'create_input_account_consl_query.sql'\n",
    "ACCOUNT_FFH_BILLING_QUERY_PATH = QUERIES_PATH + 'create_input_account_ffh_billing_query.sql'\n",
    "ACCOUNT_FFH_DISCOUNTS_QUERY_PATH = QUERIES_PATH + 'create_input_account_ffh_discounts_query.sql'\n",
    "ACCOUNT_HS_USAGE_QUERY_PATH = QUERIES_PATH + 'create_input_account_hs_usage_query.sql'\n",
    "ACCOUNT_DEMO_INCOME_QUERY_PATH = QUERIES_PATH + 'create_input_account_demo_income_query.sql'\n",
    "ACCOUNT_GPON_COPPER_QUERY_PATH = QUERIES_PATH + 'create_input_account_gpon_copper_query.sql'\n",
    "ACCOUNT_PRICE_PLAN_QUERY_PATH = QUERIES_PATH + 'create_input_account_price_plan_query.sql'\n",
    "ACCOUNT_CLCKSTRM_TELUS_QUERY_PATH = QUERIES_PATH + 'create_input_account_clckstrm_telus_query.sql'\n",
    "ACCOUNT_CALL_HISTORY_QUERY_PATH = QUERIES_PATH + 'create_input_account_call_history_query.sql'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc0bd98-1d58-491e-bc5e-bbac378c7bbf",
   "metadata": {},
   "source": [
    "### Import Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f442904b-a40a-4ba8-b237-e9894ffd4f26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download required component files to local\n",
    "prefix = f'{STACK_NAME}/{TRAIN_PIPELINE_NAME_PATH}/components/'\n",
    "dl_dir = 'components/'\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(RESOURCE_BUCKET)\n",
    "blobs = bucket.list_blobs(prefix=prefix)  # Get list of files\n",
    "for blob in blobs: # download each file that starts with \"prefix\" into \"dl_dir\"\n",
    "    if blob.name.endswith(\"/\"):\n",
    "        continue\n",
    "    file_split = blob.name.split(prefix)\n",
    "    file_path = f\"{dl_dir}{file_split[-1]}\"\n",
    "    directory = \"/\".join(file_path.split(\"/\")[0:-1])\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    blob.download_to_filename(file_path) \n",
    "\n",
    "# import main pipeline components\n",
    "import components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c315d454-416f-4eed-b12d-1acd5aed1207",
   "metadata": {},
   "source": [
    "### Date Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfad310-a482-4285-aa31-6d9113008be6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scoringDate = date.today() - relativedelta(days=3)\n",
    "\n",
    "# current views\n",
    "PROMO_EXPIRY_LIST_VIEW_NAME = '{}_pipeline_promo_expiry_list_data_curr_bi_layer'.format(SERVICE_TYPE)  \n",
    "CONSL_VIEW_NAME = '{}_pipeline_consl_data_curr_bi_layer'.format(SERVICE_TYPE)  \n",
    "FFH_BILLING_VIEW_NAME = '{}_pipeline_ffh_billing_data_curr_bi_layer'.format(SERVICE_TYPE)  \n",
    "FFH_DISCOUNTS_VIEW_NAME = '{}_pipeline_ffh_discounts_data_curr_bi_layer'.format(SERVICE_TYPE)  \n",
    "HS_USAGE_VIEW_NAME = '{}_pipeline_hs_usage_data_curr_bi_layer'.format(SERVICE_TYPE)  \n",
    "DEMO_INCOME_VIEW_NAME = '{}_pipeline_demo_income_data_curr_bi_layer'.format(SERVICE_TYPE)  \n",
    "GPON_COPPER_VIEW_NAME = '{}_pipeline_gpon_copper_data_curr_bi_layer'.format(SERVICE_TYPE)  \n",
    "PRICE_PLAN_VIEW_NAME = '{}_pipeline_price_plan_data_curr_bi_layer'.format(SERVICE_TYPE)  \n",
    "CLCKSTRM_TELUS_VIEW_NAME = '{}_pipeline_clckstrm_telus_curr_bi_layer'.format(SERVICE_TYPE)\n",
    "CALL_HISTORY_VIEW_NAME = '{}_pipeline_call_history_data_curr_bi_layer'.format(SERVICE_TYPE)  \n",
    "\n",
    "# training dates\n",
    "SCORE_DATE = scoringDate.strftime('%Y%m%d')  # date.today().strftime('%Y%m%d')\n",
    "SCORE_DATE_DASH = scoringDate.strftime('%Y-%m-%d')\n",
    "SCORE_DATE_MINUS_6_MOS_DASH = ((scoringDate - relativedelta(months=6)).replace(day=1)).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_THIS_MONTH_START_DASH = scoringDate.replace(day=1)\n",
    "SCORE_DATE_THIS_MONTH_END_DASH = (((scoringDate.replace(day=1)) + relativedelta(months=1)).replace(day=1) - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_LAST_MONTH_START_DASH = (scoringDate.replace(day=1) - timedelta(days=1)).replace(day=1).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_LAST_MONTH_END_DASH = ((scoringDate.replace(day=1)) - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_LAST_MONTH_YEAR = ((scoringDate.replace(day=1)) - timedelta(days=1)).year\n",
    "SCORE_DATE_LAST_MONTH_MONTH = ((scoringDate.replace(day=1)) - timedelta(days=1)).month\n",
    "\n",
    "#revert these changes after 2023-05-30\n",
    "# PROMO_EXPIRY_START = (scoringDate.replace(day=1) + relativedelta(months=3)).replace(day=1).strftime('%Y%m%d')\n",
    "# PROMO_EXPIRY_END = (scoringDate.replace(day=1) + relativedelta(months=4)).replace(day=1).strftime('%Y%m%d')\n",
    "PROMO_EXPIRY_START = (scoringDate.replace(day=1) + relativedelta(months=4)).replace(day=1).strftime('%Y%m%d')\n",
    "PROMO_EXPIRY_END = (scoringDate.replace(day=1) + relativedelta(months=5)).replace(day=1).strftime('%Y%m%d')\n",
    "\n",
    "SCORE_DATE_DELTA = 0\n",
    "SCORE_DATE_VAL_DELTA = 0\n",
    "TICKET_DATE_WINDOW = 30  # Days of ticket data to be queried\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc9b8dc-9863-4e0b-b5e2-40adfbbbf69e",
   "metadata": {},
   "source": [
    "### 1.create_input_account_promo_expiry_list_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02852157-aa1e-4d9f-897b-1de7f225ccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_account_promo_expiry_list_view(view_name: str,\n",
    "                                           score_date: str,\n",
    "                                           score_date_delta: str,\n",
    "                                           dataset_id: str,\n",
    "                                           project_id: str,\n",
    "                                           region: str,\n",
    "                                           resource_bucket: str,\n",
    "                                           query_path: str, \n",
    "                                           promo_expiry_start: str, \n",
    "                                           promo_expiry_end: str\n",
    "                                           ):\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    bq_client = get_gcp_bqclient(project_id)\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    dataset = bq_client.dataset(dataset_id)\n",
    "    table_ref = dataset.table(view_name)\n",
    "\n",
    "    # load query from .txt file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(resource_bucket)\n",
    "    blob = bucket.get_blob(query_path)\n",
    "    content = blob.download_as_string()\n",
    "    content = str(content, 'utf-8')\n",
    "\n",
    "    def if_tbl_exists(client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "\n",
    "    if if_tbl_exists(bq_client, table_ref):\n",
    "        bq_client.delete_table(table_ref)\n",
    "\n",
    "    create_base_feature_set_query = content.format(score_date=score_date,\n",
    "                                                   score_date_delta=score_date_delta,\n",
    "                                                   view_name=view_name,\n",
    "                                                   dataset_id=dataset_id,\n",
    "                                                   project_id=project_id,\n",
    "                                                   promo_expiry_start=promo_expiry_start, \n",
    "                                                   promo_expiry_end=promo_expiry_end\n",
    "                                                   )\n",
    "    shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "    base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "    base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "    base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "    base_feature_set_view = bq_client.create_table(base_feature_set_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fddfece-062c-48e9-a5f4-cff98e47ebb1",
   "metadata": {},
   "source": [
    "### 2.create_input_account_consl_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb809ffb-e6a1-4268-8f89-06944489d0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_account_consl_view(view_name: str,\n",
    "                                    score_date: str,\n",
    "                                    score_date_delta: str,\n",
    "                                    project_id: str,\n",
    "                                    dataset_id: str,\n",
    "                                    region: str,\n",
    "                                    resource_bucket: str,\n",
    "                                    query_path: str,\n",
    "                                    ):\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "\n",
    "    def if_tbl_exists(client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    bq_client = get_gcp_bqclient(project_id)\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    dataset = bq_client.dataset(dataset_id)\n",
    "    table_ref = dataset.table(view_name)\n",
    "\n",
    "    # load query from .txt file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(resource_bucket)\n",
    "    blob = bucket.get_blob(query_path)\n",
    "    content = blob.download_as_string()\n",
    "    content = str(content, 'utf-8')\n",
    "\n",
    "    if if_tbl_exists(bq_client, table_ref):\n",
    "        bq_client.delete_table(table_ref)\n",
    "\n",
    "    # content = open(query_path, 'r').read()\n",
    "\n",
    "    create_base_feature_set_query = content.format(score_date=score_date,\n",
    "                                                   score_date_delta=score_date_delta,\n",
    "                                                   view_name=view_name,\n",
    "                                                   dataset_id=dataset_id,\n",
    "                                                   project_id=project_id,\n",
    "                                                   )\n",
    "    shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "    base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "    base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "    base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "    base_feature_set_view = bq_client.create_table(base_feature_set_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b450c36c-0fe4-4d33-a20a-d9676faf1711",
   "metadata": {},
   "source": [
    "### 3.create_input_account_ffh_billing_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63071292-8a10-4ac4-8b15-f4a705c97e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_account_ffh_billing_view(view_name: str,\n",
    "                                          v_report_date: str,\n",
    "                                          v_start_date: str,\n",
    "                                          v_end_date: str,\n",
    "                                          v_bill_year: str,\n",
    "                                          v_bill_month: str,\n",
    "                                          dataset_id: str,\n",
    "                                          project_id: str,\n",
    "                                          region: str,\n",
    "                                          resource_bucket: str,\n",
    "                                          query_path: str\n",
    "                                          ):\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    bq_client = get_gcp_bqclient(project_id)\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    dataset = bq_client.dataset(dataset_id)\n",
    "    table_ref = dataset.table(view_name)\n",
    "\n",
    "    # load query from .txt file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(resource_bucket)\n",
    "    blob = bucket.get_blob(query_path)\n",
    "    content = blob.download_as_string()\n",
    "    content = str(content, 'utf-8')\n",
    "\n",
    "    def if_tbl_exists(client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "\n",
    "    if if_tbl_exists(bq_client, table_ref):\n",
    "        bq_client.delete_table(table_ref)\n",
    "\n",
    "    create_base_feature_set_query = content.format(v_report_date=v_report_date,\n",
    "                                                   v_start_date=v_start_date,\n",
    "                                                   v_end_date=v_end_date,\n",
    "                                                   v_bill_year=v_bill_year,\n",
    "                                                   v_bill_month=v_bill_month,\n",
    "                                                   )\n",
    "\n",
    "    shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "    base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "    base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "    base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "    base_feature_set_view = bq_client.create_table(base_feature_set_view)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c3ef7e-efaa-4206-a831-a671257fa73a",
   "metadata": {},
   "source": [
    "### 4.create_input_account_ffh_discounts_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9117918-6c4f-40fb-96c5-ec26d5a3a627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_account_ffh_discounts_view(view_name: str,\n",
    "                                        score_date: str,\n",
    "                                        score_date_delta: str,\n",
    "                                        project_id: str,\n",
    "                                        dataset_id: str,\n",
    "                                        region: str,\n",
    "                                        resource_bucket: str,\n",
    "                                        query_path: str,\n",
    "                                        ):\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "\n",
    "    def if_tbl_exists(client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    bq_client = get_gcp_bqclient(project_id)\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    dataset = bq_client.dataset(dataset_id)\n",
    "    table_ref = dataset.table(view_name)\n",
    "\n",
    "    # load query from .txt file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(resource_bucket)\n",
    "    blob = bucket.get_blob(query_path)\n",
    "    content = blob.download_as_string()\n",
    "    content = str(content, 'utf-8')\n",
    "    \n",
    "    if if_tbl_exists(bq_client, table_ref):\n",
    "        bq_client.delete_table(table_ref)\n",
    "\n",
    "    # content = open(query_path, 'r').read()\n",
    "\n",
    "    create_base_feature_set_query = content.format(score_date=score_date,\n",
    "                                                   score_date_delta=score_date_delta, \n",
    "                                                   view_name=view_name,\n",
    "                                                   dataset_id=dataset_id,\n",
    "                                                   project_id=project_id\n",
    "                                                   )\n",
    "    shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "    base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "    base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "    base_feature_set_view.view_query = create_base_feature_set_query\n",
    "    base_feature_set_view = bq_client.create_table(base_feature_set_view)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcac8e47-12fd-4483-bf7c-778e423b4799",
   "metadata": {},
   "source": [
    "### 5.create_input_account_hs_usage_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee66702-4817-4452-ad4a-07775d2aa02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_account_hs_usage_view(view_name: str,\n",
    "                                       v_report_date: str,\n",
    "                                       v_start_date: str,\n",
    "                                       v_end_date: str,\n",
    "                                       v_bill_year: str,\n",
    "                                       v_bill_month: str,\n",
    "                                       dataset_id: str,\n",
    "                                       project_id: str,\n",
    "                                       region: str,\n",
    "                                       resource_bucket: str,\n",
    "                                       query_path: str\n",
    "                                       ):\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    bq_client = get_gcp_bqclient(project_id)\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    dataset = bq_client.dataset(dataset_id)\n",
    "    table_ref = dataset.table(view_name)\n",
    "\n",
    "    # load query from .txt file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(resource_bucket)\n",
    "    blob = bucket.get_blob(query_path)\n",
    "    content = blob.download_as_string()\n",
    "    content = str(content, 'utf-8')\n",
    "\n",
    "    def if_tbl_exists(client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "\n",
    "    if if_tbl_exists(bq_client, table_ref):\n",
    "        bq_client.delete_table(table_ref)\n",
    "\n",
    "    create_base_feature_set_query = content.format(v_report_date=v_report_date,\n",
    "                                                   v_start_date=v_start_date,\n",
    "                                                   v_end_date=v_end_date,\n",
    "                                                   v_bill_year=v_bill_year,\n",
    "                                                   v_bill_month=v_bill_month,\n",
    "                                                   )\n",
    "\n",
    "    shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "    base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "    base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "    base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "    base_feature_set_view = bq_client.create_table(base_feature_set_view)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec90cef-a04a-410c-8976-8b0bd71dd0ad",
   "metadata": {},
   "source": [
    "### 6.create_input_account_demo_income_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f68587-e7ab-4764-b767-4da55c51f162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_account_demo_income_view(view_name: str,\n",
    "                                          score_date: str,\n",
    "                                          score_date_delta: str,\n",
    "                                          dataset_id: str,\n",
    "                                          project_id: str,\n",
    "                                          region: str,\n",
    "                                          resource_bucket: str,\n",
    "                                          query_path: str\n",
    "                                          ):\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    bq_client = get_gcp_bqclient(project_id)\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    dataset = bq_client.dataset(dataset_id)\n",
    "    table_ref = dataset.table(view_name)\n",
    "\n",
    "    # load query from .txt file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(resource_bucket)\n",
    "    blob = bucket.get_blob(query_path)\n",
    "    content = blob.download_as_string()\n",
    "    content = str(content, 'utf-8')\n",
    "\n",
    "    def if_tbl_exists(client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "\n",
    "    if if_tbl_exists(bq_client, table_ref):\n",
    "        bq_client.delete_table(table_ref)\n",
    "\n",
    "    create_base_feature_set_query = content.format(score_date=score_date,\n",
    "                                                   score_date_delta=score_date_delta,\n",
    "                                                   project_id=project_id,\n",
    "                                                   dataset_id='common_dataset',\n",
    "                                                   )\n",
    "\n",
    "    shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "    base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "    base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "    base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "    base_feature_set_view = bq_client.create_table(base_feature_set_view)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0148a-0a7a-4fa5-8e53-7ef7bd91eda6",
   "metadata": {},
   "source": [
    "### 7.create_input_account_gpon_copper_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2190f37-a3d3-423d-a2f4-f8f5e4832835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_account_gpon_copper_view(view_name: str,\n",
    "                                          score_date: str,\n",
    "                                          score_date_delta: str,\n",
    "                                          dataset_id: str,\n",
    "                                          project_id: str,\n",
    "                                          region: str,\n",
    "                                          resource_bucket: str,\n",
    "                                          query_path: str\n",
    "                                          ):\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    bq_client = get_gcp_bqclient(project_id)\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    dataset = bq_client.dataset(dataset_id)\n",
    "    table_ref = dataset.table(view_name)\n",
    "\n",
    "    # load query from .txt file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(resource_bucket)\n",
    "    blob = bucket.get_blob(query_path)\n",
    "    content = blob.download_as_string()\n",
    "    content = str(content, 'utf-8')\n",
    "\n",
    "    def if_tbl_exists(client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "\n",
    "    if if_tbl_exists(bq_client, table_ref):\n",
    "        bq_client.delete_table(table_ref)\n",
    "\n",
    "    create_base_feature_set_query = content.format(score_date=score_date,\n",
    "                                                   score_date_delta=score_date_delta,\n",
    "                                                   )\n",
    "\n",
    "    shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "    base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "    base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "    base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "    base_feature_set_view = bq_client.create_table(base_feature_set_view)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4637e1d7-d125-454d-a1e4-cb00057c9bd3",
   "metadata": {},
   "source": [
    "### 8.create_input_account_price_plan_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67990674-ff9e-4274-b881-ac9fee47b5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_account_price_plan_view(view_name: str,\n",
    "                                        score_date: str,\n",
    "                                        score_date_delta: str,\n",
    "                                        project_id: str,\n",
    "                                        dataset_id: str,\n",
    "                                        region: str,\n",
    "                                        resource_bucket: str,\n",
    "                                        query_path: str,\n",
    "                                        ):\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "\n",
    "    def if_tbl_exists(client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    bq_client = get_gcp_bqclient(project_id)\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    dataset = bq_client.dataset(dataset_id)\n",
    "    table_ref = dataset.table(view_name)\n",
    "\n",
    "    # load query from .txt file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(resource_bucket)\n",
    "    blob = bucket.get_blob(query_path)\n",
    "    content = blob.download_as_string()\n",
    "    content = str(content, 'utf-8')\n",
    "\n",
    "    if if_tbl_exists(bq_client, table_ref):\n",
    "        bq_client.delete_table(table_ref)\n",
    "\n",
    "    # content = open(query_path, 'r').read()\n",
    "\n",
    "    create_base_feature_set_query = content.format(score_date=score_date,\n",
    "                                                   score_date_delta=score_date_delta,\n",
    "                                                   view_name=view_name,\n",
    "                                                   dataset_id=dataset_id,\n",
    "                                                   project_id=project_id,\n",
    "                                                   )\n",
    "    shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "    base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "    base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "    base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "    base_feature_set_view = bq_client.create_table(base_feature_set_view)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4f1056-61de-4aa9-9167-66a2a9400f24",
   "metadata": {},
   "source": [
    "### 9.create_input_account_clckstrm_telus_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf4fc55-03b6-4680-b52b-e3f8f664685e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_account_clckstrm_telus_view(view_name: str,\n",
    "                                    score_date: str,\n",
    "                                    score_date_delta: str,\n",
    "                                    project_id: str,\n",
    "                                    dataset_id: str,\n",
    "                                    region: str,\n",
    "                                    resource_bucket: str,\n",
    "                                    query_path: str,\n",
    "                                    ):\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "\n",
    "    def if_tbl_exists(client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    bq_client = get_gcp_bqclient(project_id)\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    dataset = bq_client.dataset(dataset_id)\n",
    "    table_ref = dataset.table(view_name)\n",
    "\n",
    "    # load query from .txt file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(resource_bucket)\n",
    "    blob = bucket.get_blob(query_path)\n",
    "    content = blob.download_as_string()\n",
    "    content = str(content, 'utf-8')\n",
    "\n",
    "    if if_tbl_exists(bq_client, table_ref):\n",
    "        bq_client.delete_table(table_ref)\n",
    "\n",
    "    # content = open(query_path, 'r').read()\n",
    "\n",
    "    create_base_feature_set_query = content.format(score_date=score_date,\n",
    "                                                   score_date_delta=score_date_delta,\n",
    "                                                   view_name=view_name,\n",
    "                                                   dataset_id=dataset_id,\n",
    "                                                   project_id=project_id,\n",
    "                                                   )\n",
    "    shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "    base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "    base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "    base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "    base_feature_set_view = bq_client.create_table(base_feature_set_view)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df2a9dd-3c80-4dcd-a55d-adce24f144de",
   "metadata": {},
   "source": [
    "### 10.create_input_account_call_history_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f4a761-e3d4-402f-87d9-6a4ed21ff3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_account_call_history_view(view_name: str,\n",
    "                                        score_date: str,\n",
    "                                        score_date_delta: str,\n",
    "                                        project_id: str,\n",
    "                                        dataset_id: str,\n",
    "                                        region: str,\n",
    "                                        resource_bucket: str,\n",
    "                                        query_path: str,\n",
    "                                        ):\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "\n",
    "    def if_tbl_exists(client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    bq_client = get_gcp_bqclient(project_id)\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    dataset = bq_client.dataset(dataset_id)\n",
    "    table_ref = dataset.table(view_name)\n",
    "\n",
    "    # load query from .txt file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(resource_bucket)\n",
    "    blob = bucket.get_blob(query_path)\n",
    "    content = blob.download_as_string()\n",
    "    content = str(content, 'utf-8')\n",
    "\n",
    "    if if_tbl_exists(bq_client, table_ref):\n",
    "        bq_client.delete_table(table_ref)\n",
    "\n",
    "    # content = open(query_path, 'r').read()\n",
    "\n",
    "    create_base_feature_set_query = content.format(score_date=score_date,\n",
    "                                                   score_date_delta=score_date_delta,\n",
    "                                                   view_name=view_name,\n",
    "                                                   dataset_id=dataset_id,\n",
    "                                                   project_id=project_id,\n",
    "                                                   )\n",
    "    shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "    base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "    base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "    base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "    base_feature_set_view = bq_client.create_table(base_feature_set_view)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281c6676-70b0-41a6-b860-aa6d95fec0da",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8623f9-e2fd-406a-b0a7-2ea7c42af66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "        promo_expiry_list_view: str, \n",
    "        account_consl_view: str, \n",
    "        account_bill_view: str, \n",
    "        account_discounts_view: str, \n",
    "        hs_usage_view: str, \n",
    "        demo_income_view: str, \n",
    "        gpon_copper_view: str, \n",
    "        # price_plan_view: str, \n",
    "        clckstrm_telus_view: str, \n",
    "        call_history_view: str, \n",
    "        save_data_path: str,\n",
    "        project_id: str,\n",
    "        dataset_id: str\n",
    "):\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    import pandas as pd\n",
    "    import gc\n",
    "    import time\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    client = get_gcp_bqclient(project_id)\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    \n",
    "    #1.df_promo_expiry_list\n",
    "    promo_expiry_list_set = f\"{project_id}.{dataset_id}.{promo_expiry_list_view}\" \n",
    "    build_df_promo_expiry_list = '''SELECT * FROM `{promo_expiry_list_set}`'''.format(promo_expiry_list_set=promo_expiry_list_set)\n",
    "    df_promo_expiry_list = client.query(build_df_promo_expiry_list).to_dataframe()\n",
    "    df_promo_expiry_list = df_promo_expiry_list.set_index('ban')\n",
    "    df_join = df_promo_expiry_list.copy()\n",
    "    print('......df_promo_expiry_list done')\n",
    "    \n",
    "    #2.df_consl\n",
    "    consl_data_set = f\"{project_id}.{dataset_id}.{account_consl_view}\" \n",
    "    build_df_consl = '''SELECT * FROM `{consl_data_set}`'''.format(consl_data_set=consl_data_set)\n",
    "    df_consl = client.query(build_df_consl).to_dataframe()\n",
    "    df_mix = df_consl[[\n",
    "        'ban',\n",
    "        'customer_tenure', \n",
    "        'product_mix_all',\n",
    "        'hsic_count',\n",
    "        'ttv_count',\n",
    "        'sing_count',\n",
    "        'mob_count',\n",
    "        'shs_count',\n",
    "        'new_hsic_ind',\n",
    "        'new_ttv_ind',\n",
    "        'new_sing_ind',\n",
    "        'new_c_ind',\n",
    "        'new_smhm_ind',\n",
    "        'mnh_ind'\n",
    "    ]]\n",
    "    df_mix = df_mix.drop_duplicates(subset=['ban']).set_index('ban').add_prefix('productMix_').fillna(0)\n",
    "    df_join = df_join.join(df_mix)\n",
    "    del df_mix\n",
    "    gc.collect()\n",
    "    print('......df_consl done')\n",
    "    \n",
    "    #3.df_bill\n",
    "    bill_data_set = f\"{project_id}.{dataset_id}.{account_bill_view}\" \n",
    "    build_df_bill = '''SELECT * FROM `{bill_data_set}`'''.format(bill_data_set=bill_data_set)\n",
    "    df_bill = client.query(build_df_bill).to_dataframe() \n",
    "    df_bill = df_bill.set_index('ban').add_prefix('ffhBill_')\n",
    "    df_join = df_join.join(df_bill).fillna(0) \n",
    "    del df_bill\n",
    "    gc.collect()\n",
    "    print('......df_bill done')\n",
    "    \n",
    "    #4.df_discounts\n",
    "    discounts_data_set = f\"{project_id}.{dataset_id}.{account_discounts_view}\" \n",
    "    build_df_discounts = '''SELECT * FROM `{discounts_data_set}`'''.format(discounts_data_set=discounts_data_set)\n",
    "    df_discounts = client.query(build_df_discounts).to_dataframe() \n",
    "    df_discounts = df_discounts.set_index('ban').add_prefix('ffhDiscounts_')\n",
    "    df_join = df_join.join(df_discounts).fillna(0) \n",
    "    del df_discounts\n",
    "    gc.collect()\n",
    "    print('......df_discounts done')\n",
    "\n",
    "    #5.df_hs_usage\n",
    "    hs_usage_data_set = f\"{project_id}.{dataset_id}.{hs_usage_view}\" \n",
    "    build_df_hs_usage = '''SELECT * FROM `{hs_usage_data_set}`'''.format(hs_usage_data_set=hs_usage_data_set)\n",
    "    df_hs_usage = client.query(build_df_hs_usage).to_dataframe() \n",
    "    df_hs_usage = df_hs_usage.set_index('ban').add_prefix('hsiaUsage_')\n",
    "    df_join = df_join.join(df_hs_usage).fillna(0) \n",
    "    del df_hs_usage\n",
    "    gc.collect()\n",
    "    print('......df_hs_usage done')\n",
    "\n",
    "    #6.df_income\n",
    "    demo_income_data_set = f\"{project_id}.{dataset_id}.{demo_income_view}\" \n",
    "    build_df_demo_income = '''SELECT * FROM `{demo_income_data_set}`'''.format(demo_income_data_set=demo_income_data_set)\n",
    "    df_income = client.query(build_df_demo_income).to_dataframe()\n",
    "    df_income = df_income.set_index('ban')\n",
    "    df_income['demo_urban_flag'] = df_income.demo_sgname.str.lower().str.contains('urban').fillna(0).astype(int)\n",
    "    df_income['demo_rural_flag'] = df_income.demo_sgname.str.lower().str.contains('rural').fillna(0).astype(int)\n",
    "    df_income['demo_family_flag'] = df_income.demo_lsname.str.lower().str.contains('families').fillna(0).astype(int)\n",
    "    df_income_dummies = pd.get_dummies(df_income[['demo_lsname']])\n",
    "    df_income_dummies.columns = df_income_dummies.columns.str.replace('&', 'and')\n",
    "    df_income_dummies.columns = df_income_dummies.columns.str.replace(' ', '_')\n",
    "    df_income = df_income[['demo_avg_income', 'demo_urban_flag', 'demo_rural_flag', 'demo_family_flag']].join(\n",
    "        df_income_dummies)\n",
    "    df_income.demo_avg_income = df_income.demo_avg_income.astype(float)\n",
    "    df_income.demo_avg_income = df_income.demo_avg_income.fillna(df_income.demo_avg_income.median())\n",
    "    df_group_income = df_income.groupby('ban').agg('mean')\n",
    "    df_group_income = df_group_income.add_prefix('demographics_')\n",
    "    df_join = df_join.join(df_group_income.fillna(df_group_income.median()))\n",
    "    del df_group_income\n",
    "    del df_income\n",
    "    gc.collect()\n",
    "    print('......df_income done')\n",
    "\n",
    "    #7.df_gpon_copper\n",
    "    gpon_copper_data_set = f\"{project_id}.{dataset_id}.{gpon_copper_view}\"\n",
    "    build_df_gpon_copper = '''SELECT * FROM `{gpon_copper_data_set}`'''.format(gpon_copper_data_set=gpon_copper_data_set)\n",
    "    df_gpon_copper = client.query(build_df_gpon_copper).to_dataframe()\n",
    "    df_gpon_copper = df_gpon_copper.set_index('ban')\n",
    "    df_join = df_join.join(df_gpon_copper.add_prefix('infra_')).fillna(0)\n",
    "    del df_gpon_copper\n",
    "    gc.collect()\n",
    "    print('......df_gpon_copper done')\n",
    "\n",
    "#     #8.df_price_plan\n",
    "#     price_plan_data_set = f\"{project_id}.{dataset_id}.{price_plan_view}\"\n",
    "#     build_df_price_plan = '''SELECT * FROM `{price_plan_data_set}`'''.format(price_plan_data_set=price_plan_data_set)\n",
    "#     df_price_plan = client.query(build_df_price_plan).to_dataframe()\n",
    "#     df_price_plan = df_price_plan.set_index('ban')\n",
    "#     df_pp_dummies = pd.get_dummies(df_price_plan[['price_plan']])\n",
    "#     df_pp_dummies.columns = df_pp_dummies.columns.str.replace('&', 'and')\n",
    "#     df_pp_dummies.columns = df_pp_dummies.columns.str.replace(' ', '_')\n",
    "#     df_price_plan = df_price_plan.join(df_pp_dummies)\n",
    "#     df_price_plan.drop(columns=['price_plan'], axis=1, inplace=True)\n",
    "#     print(df_price_plan.columns)\n",
    "#     df_join = df_join.join(df_price_plan.add_prefix('infra_')).fillna(0)\n",
    "#     del df_price_plan\n",
    "#     gc.collect()\n",
    "#     print('......df_price_plan done')\n",
    "\n",
    "    #9.df_clckstrm_telus\n",
    "    clckstrm_telus_data_set = f\"{project_id}.{dataset_id}.{clckstrm_telus_view}\" \n",
    "    build_df_clckstrm_telus = '''SELECT * FROM `{clckstrm_telus_data_set}`'''.format(clckstrm_telus_data_set=clckstrm_telus_data_set)\n",
    "    df_clckstrm_telus = client.query(build_df_clckstrm_telus).to_dataframe() \n",
    "    df_clckstrm_telus = df_clckstrm_telus.set_index('ban').add_prefix('clckstrmData_')\n",
    "    df_join = df_join.join(df_clckstrm_telus).fillna(0) \n",
    "    del df_clckstrm_telus\n",
    "    gc.collect()\n",
    "    print('......df_clckstrm_telus done')\n",
    "\n",
    "    #10.df_call_history\n",
    "    call_history_data_set = f\"{project_id}.{dataset_id}.{call_history_view}\" \n",
    "    build_df_call_history = '''SELECT * FROM `{call_history_data_set}`'''.format(call_history_data_set=call_history_data_set)\n",
    "    df_call_history = client.query(build_df_call_history).to_dataframe() \n",
    "    df_call_history = df_call_history.set_index('ban').add_prefix('callHistory_')\n",
    "    df_join = df_join.join(df_call_history)\n",
    "    df_join[['callHistory_frequency', 'callHistory_have_called']] = df_join[['callHistory_frequency', 'callHistory_have_called']].fillna(0)\n",
    "    df_join[['callHistory_recency']] = df_join[['callHistory_recency']].fillna(999)\n",
    "    del df_call_history\n",
    "    gc.collect()\n",
    "    print('......df_call_history done')\n",
    "\n",
    "    #column name clean-up\n",
    "    df_join.columns = df_join.columns.str.replace(' ', '_')\n",
    "    df_join.columns = df_join.columns.str.replace('-', '_')\n",
    "\n",
    "    #df_final\n",
    "    df_final = df_join.copy()\n",
    "    del df_join\n",
    "    gc.collect()\n",
    "    print('......df_final done')\n",
    "\n",
    "    for f in df_final.columns:\n",
    "        df_final[f] = list(df_final[f])\n",
    "\n",
    "    df_final.to_csv(save_data_path, index=True, compression='gzip') \n",
    "    del df_final\n",
    "    gc.collect()\n",
    "    print(f'......csv saved in {save_data_path}')\n",
    "    time.sleep(120)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330396ff-dfe9-4c23-9d67-922e9a190bdc",
   "metadata": {},
   "source": [
    "### Batch Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f33719-127b-45c7-bccd-dcd3970b003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_prediction(\n",
    "        project_id: str,\n",
    "        dataset_id: str,\n",
    "        file_bucket: str,\n",
    "        service_type: str,\n",
    "        score_table: str,\n",
    "        score_date_dash: str\n",
    "):\n",
    "    import time\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    from datetime import date\n",
    "    from dateutil.relativedelta import relativedelta\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    MODEL_ID = '5090'\n",
    "    \n",
    "    def if_tbl_exists(bq_client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            bq_client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "\n",
    "    def upsert_table(project_id, dataset_id, table_id, sql, result):\n",
    "        new_values = ',\\n'.join(result.apply(lambda row: row_format(row), axis=1))\n",
    "        new_sql = sql.format(proj_id=project_id, dataset_id=dataset_id, table_id=table_id,\n",
    "                             new_values=new_values)\n",
    "\n",
    "        def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "            token = os.popen('gcloud auth print-access-token').read()\n",
    "            token = re.sub(f'\\n$', '', token)\n",
    "            credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "            bq_client = bigquery.Client(project=project_id)\n",
    "            if use_local_credential:\n",
    "                bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "            return bq_client\n",
    "\n",
    "        bq_client = get_gcp_bqclient(project_id)\n",
    "\n",
    "        # bq_client = bigquery.Client(project=project_id)\n",
    "        \n",
    "        code = bq_client.query(new_sql)\n",
    "        time.sleep(5)\n",
    "\n",
    "    def row_format(row):\n",
    "        values = row.values\n",
    "        new_values = \"\"\n",
    "        v = str(values[0]) if not pd.isnull(values[0]) else 'NULL'\n",
    "        if 'str' in str(type(values[0])):\n",
    "            new_values += f\"'{v}'\"\n",
    "        else:\n",
    "            new_values += f\"{v}\"\n",
    "\n",
    "        for i in range(1, len(values)):\n",
    "            v = str(values[i]) if not pd.isnull(values[i]) else 'NULL'\n",
    "            if 'str' in str(type(values[i])):\n",
    "                new_values += f\",'{v}'\"\n",
    "            else:\n",
    "                new_values += f\",{v}\"\n",
    "        return '(' + new_values + ')'\n",
    "\n",
    "    def generate_sql_file(ll):\n",
    "        s = 'MERGE INTO `{proj_id}.{dataset_id}.{table_id}` a'\n",
    "        s += \" USING UNNEST(\"\n",
    "        s += \"[struct<\"\n",
    "        for i in range(len(ll) - 1):\n",
    "            v = ll[i]\n",
    "            s += \"{} {},\".format(v[0], v[1])\n",
    "        s += \"{} {}\".format(ll[-1][0], ll[-1][1])\n",
    "        s += \">{new_values}]\"\n",
    "        s += \") b\"\n",
    "        s += \" ON a.ban = b.ban and a.score_date = b.score_date\"\n",
    "        s += \" WHEN MATCHED THEN\"\n",
    "        s += \" UPDATE SET \"\n",
    "        s += \"a.{}=b.{},\".format(ll[0][0], ll[0][0])\n",
    "        for i in range(1, len(ll) - 1):\n",
    "            v = ll[i]\n",
    "            s += \"a.{}=b.{},\".format(v[0], v[0])\n",
    "        s += \"a.{}=b.{}\".format(ll[-1][0], ll[-1][0])\n",
    "        s += \" WHEN NOT MATCHED THEN\"\n",
    "        s += \" INSERT(\"\n",
    "        for i in range(len(ll) - 1):\n",
    "            v = ll[i]\n",
    "            s += \"{},\".format(v[0])\n",
    "        s += \"{})\".format(ll[-1][0])\n",
    "        s += \" VALUES(\"\n",
    "        for i in range(len(ll) - 1):\n",
    "            s += \"b.{},\".format(ll[i][0])\n",
    "        s += \"b.{}\".format(ll[-1][0])\n",
    "        s += \")\"\n",
    "\n",
    "        return s\n",
    "\n",
    "    MODEL_PATH = '{}_xgb_models/'.format(service_type)\n",
    "    df_score = pd.read_csv('gs://{}/{}_score.csv.gz'.format(file_bucket, service_type), compression='gzip')\n",
    "    df_score.dropna(subset=['ban'], inplace=True)\n",
    "    df_score.reset_index(drop=True, inplace=True)\n",
    "    print('......scoring data loaded:{}'.format(df_score.shape))\n",
    "    time.sleep(10)\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(file_bucket)\n",
    "    blobs = storage_client.list_blobs(file_bucket, prefix='{}{}_models_xgb_'.format(MODEL_PATH, service_type))\n",
    "\n",
    "    model_lists = []\n",
    "    for blob in blobs:\n",
    "        model_lists.append(blob.name)\n",
    "\n",
    "    blob = bucket.blob(model_lists[-1])\n",
    "    blob_in = blob.download_as_string()\n",
    "    model_dict = pickle.loads(blob_in)\n",
    "    model_xgb = model_dict['model']\n",
    "    features = model_dict['features']\n",
    "    print('...... model loaded')\n",
    "    time.sleep(10)\n",
    "\n",
    "    ll = [('ban', 'string'), ('score_date', 'string'), ('model_id', 'string'), ('score', 'float64')]\n",
    "    sql = generate_sql_file(ll)\n",
    "\n",
    "    df_score['ban'] = df_score['ban'].astype(int)\n",
    "    print('.... scoring for {} promo expiry bans base'.format(len(df_score)))\n",
    "\n",
    "    # get full score to cave into bucket\n",
    "    pred_prob = model_xgb.predict_proba(df_score[features], ntree_limit=model_xgb.best_iteration)[:, 1]\n",
    "    result = pd.DataFrame(columns=['ban', 'score_date', 'model_id', 'score'])\n",
    "    result['score'] = list(pred_prob)\n",
    "    result['score'] = result['score'].fillna(0.0).astype('float64')\n",
    "    result['ban'] = list(df_score['ban'])\n",
    "    result['ban'] = result['ban'].astype('str')\n",
    "    result['score_date'] = score_date_dash\n",
    "    result['model_id'] = MODEL_ID\n",
    "\n",
    "    result.to_csv('gs://{}/ucar/{}_prediction.csv.gz'.format(file_bucket, service_type), compression='gzip',\n",
    "                  index=False)\n",
    "    time.sleep(60)\n",
    "\n",
    "    batch_size = 1000\n",
    "    n_batchs = int(df_score.shape[0] / batch_size) + 1\n",
    "    print('...... will upsert {} batches'.format(n_batchs))\n",
    "\n",
    "    # start batch prediction\n",
    "    all_scores = np.array(result['score'].values)\n",
    "    for i in range(n_batchs):\n",
    "    \n",
    "        s, e = i * batch_size, (i + 1) * batch_size\n",
    "        if e >= df_score.shape[0]:\n",
    "            e = df_score.shape[0]\n",
    "\n",
    "        df_temp = df_score.iloc[s:e]\n",
    "        pred_prob = all_scores[s:e]\n",
    "        batch_result = pd.DataFrame(columns=['ban', 'score_date', 'model_id', 'score'])\n",
    "        batch_result['score'] = list(pred_prob)\n",
    "        batch_result['score'] = batch_result['score'].fillna(0.0).astype('float64')\n",
    "        batch_result['ban'] = list(df_temp['ban'])\n",
    "        batch_result['ban'] = batch_result['ban'].astype('str')\n",
    "        batch_result['score_date'] = score_date_dash\n",
    "        batch_result['model_id'] = MODEL_ID\n",
    "\n",
    "        upsert_table(project_id,\n",
    "                     dataset_id,\n",
    "                     score_table,\n",
    "                     sql,\n",
    "                     batch_result,\n",
    "                     )\n",
    "        if i % 20 == 0:\n",
    "            print('predict for batch {} done'.format(i), end=' ')\n",
    "\n",
    "    time.sleep(120)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1b7f3a-31a7-4737-a763-89a6356648cc",
   "metadata": {},
   "source": [
    "### Post Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e403c5-a7b0-401d-b602-81dff0746149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(\n",
    "        project_id: str,\n",
    "        file_bucket: str,\n",
    "        service_type: str,\n",
    "        score_date_dash: str,\n",
    "):\n",
    "    import time\n",
    "    import pandas as pd\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    MODEL_ID = '5090'\n",
    "    file_name = 'gs://{}/ucar/{}_prediction.csv.gz'.format(file_bucket, service_type)\n",
    "    df_orig = pd.read_csv(file_name, compression='gzip')\n",
    "    df_orig.dropna(subset=['ban'], inplace=True)\n",
    "    df_orig.reset_index(drop=True, inplace=True)\n",
    "    df_orig['scoring_date'] = score_date_dash\n",
    "    df_orig.ban = df_orig.ban.astype(int)\n",
    "    df_orig = df_orig.rename(columns={'ban': 'bus_bacct_num', 'score': 'score_num'})\n",
    "    df_orig.score_num = df_orig.score_num.astype(float)\n",
    "    df_orig['decile_grp_num'] = pd.qcut(df_orig['score_num'], q=10, labels=[i for i in range(10, 0, -1)])\n",
    "    df_orig['percentile_pct'] = df_orig.score_num.rank(pct=True)\n",
    "    df_orig['predict_model_nm'] = 'FFH Call To Retention Model - DIVG'\n",
    "    df_orig['model_type_cd'] = 'FFH'\n",
    "    df_orig['subscriber_no'] = \"\"\n",
    "    df_orig['prod_instnc_resrc_str'] = \"\"\n",
    "    df_orig['service_instnc_id'] = \"\"\n",
    "    df_orig['segment_nm'] = \"\"\n",
    "    df_orig['segment_id'] = \"\"\n",
    "    df_orig['classn_nm'] = \"\"\n",
    "    df_orig['predict_model_id'] = MODEL_ID\n",
    "    df_orig.drop(columns=['model_id', 'score_date'], axis=1, inplace=True)\n",
    "\n",
    "    get_cust_id = \"\"\"\n",
    "    WITH bq_snpsht_max_date AS(\n",
    "    SELECT PARSE_DATE('%Y%m%d', MAX(partition_id)) AS max_date\n",
    "        FROM `cio-datahub-enterprise-pr-183a.ent_cust_cust.INFORMATION_SCHEMA.PARTITIONS` \n",
    "    WHERE table_name = 'bq_prod_instnc_snpsht' \n",
    "        AND partition_id <> '__NULL__'\n",
    "    ),\n",
    "    -- BANs can have multiple Cust ID. Create rank by product type and status, prioritizing ban/cust id with active FFH products\n",
    "    rank_prod_type AS (\n",
    "    SELECT DISTINCT\n",
    "        bacct_bus_bacct_num,\n",
    "        consldt_cust_bus_cust_id AS cust_id,\n",
    "        CASE WHEN pi_prod_instnc_resrc_typ_cd IN ('SING', 'HSIC', 'TTV', 'SMHM', 'STV', 'DIIC') AND pi_prod_instnc_stat_cd = 'A' THEN 1\n",
    "                WHEN pi_prod_instnc_resrc_typ_cd IN ('SING', 'HSIC', 'TTV', 'SMHM', 'STV', 'DIIC') THEN 2\n",
    "                WHEN pi_prod_instnc_stat_cd = 'A' THEN 3\n",
    "                ELSE 4\n",
    "                END AS prod_rank\n",
    "    FROM `cio-datahub-enterprise-pr-183a.ent_cust_cust.bq_prod_instnc_snpsht`\n",
    "    CROSS JOIN bq_snpsht_max_date\n",
    "    WHERE CAST(prod_instnc_ts AS DATE)=bq_snpsht_max_date.max_date\n",
    "    AND bus_prod_instnc_src_id = 1001\n",
    "    ),\n",
    "    --Rank Cust ID\n",
    "    rank_cust_id AS (\n",
    "    SELECT DISTINCT\n",
    "        bacct_bus_bacct_num,\n",
    "        cust_id,\n",
    "        RANK() OVER(PARTITION BY bacct_bus_bacct_num\n",
    "                        ORDER BY prod_rank,\n",
    "                                    cust_id) AS cust_id_rank               \n",
    "    FROM rank_prod_type\n",
    "    )\n",
    "    --Select best cust id\n",
    "    SELECT bacct_bus_bacct_num,\n",
    "        cust_id\n",
    "    FROM rank_cust_id\n",
    "    WHERE cust_id_rank = 1\n",
    "    \"\"\"\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    client = get_gcp_bqclient(project_id)\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    \n",
    "    # client = bigquery.Client(project=project_id)\n",
    "    df_cust = client.query(get_cust_id).to_dataframe()\n",
    "    df_final = df_orig.set_index('bus_bacct_num').join(df_cust.set_index('bacct_bus_bacct_num')).reset_index()\n",
    "    df_final = df_final.rename(columns={'index': 'bus_bacct_num', 'cust_bus_cust_id': 'cust_id'})\n",
    "    df_final = df_final.sort_values(by=['score_num'], ascending=False)\n",
    "    df_final.to_csv(file_name, compression='gzip', index=False)\n",
    "    time.sleep(300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9754fb-807c-470c-8c1f-c55093725b34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e2eea99-a3f0-48ea-8962-115edcaf26d9",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3cda87-dc1f-495e-be67-a3d623c13e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @dsl.pipeline(\n",
    "#     # A name for the pipeline.\n",
    "#     name=\"{}-xgb-pipeline\".format(SERVICE_TYPE_NAME),\n",
    "#     description=' pipeline for training {} model'.format(SERVICE_TYPE_NAME)\n",
    "# )\n",
    "def pipeline(\n",
    "        project_id: str = PROJECT_ID,\n",
    "        region: str = REGION,\n",
    "        resource_bucket: str = RESOURCE_BUCKET,\n",
    "        file_bucket: str = FILE_BUCKET\n",
    "    ):\n",
    "    # ------------- train view ops ---------------\n",
    "    #1.create_input_account_promo_expiry_list_view\n",
    "    create_input_account_promo_expiry_list_view_op = create_input_account_promo_expiry_list_view(\n",
    "        view_name=PROMO_EXPIRY_LIST_VIEW_NAME,\n",
    "        score_date=SCORE_DATE,\n",
    "        score_date_delta=SCORE_DATE_DELTA,\n",
    "        promo_expiry_start = PROMO_EXPIRY_START, \n",
    "        promo_expiry_end = PROMO_EXPIRY_END,\n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID,\n",
    "        region=REGION,\n",
    "        resource_bucket=RESOURCE_BUCKET,\n",
    "        query_path=ACCOUNT_PROMO_EXPIRY_LIST_QUERY_PATH\n",
    "    )\n",
    "    # create_input_account_promo_expiry_list_view_op.set_memory_limit('16G')\n",
    "    # create_input_account_promo_expiry_list_view_op.set_cpu_limit('4')\n",
    "\n",
    "    #2.create_input_account_consl_view\n",
    "    create_input_account_consl_view_op = create_input_account_consl_view(\n",
    "        view_name=CONSL_VIEW_NAME,\n",
    "        score_date=SCORE_DATE,\n",
    "        score_date_delta=SCORE_DATE_DELTA,\n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID,\n",
    "        region=REGION,\n",
    "        resource_bucket=RESOURCE_BUCKET,\n",
    "        query_path=ACCOUNT_CONSL_QUERY_PATH\n",
    "    )\n",
    "    # create_input_account_consl_view_op.set_memory_limit('16G')\n",
    "    # create_input_account_consl_view_op.set_cpu_limit('4')\n",
    "\n",
    "    #3.create_input_account_ffh_billing_view\n",
    "    create_input_account_ffh_billing_view_op = create_input_account_ffh_billing_view(\n",
    "        v_report_date=SCORE_DATE_DASH,\n",
    "        v_start_date=SCORE_DATE_MINUS_6_MOS_DASH,\n",
    "        v_end_date=SCORE_DATE_LAST_MONTH_END_DASH,\n",
    "        v_bill_year=SCORE_DATE_LAST_MONTH_YEAR,\n",
    "        v_bill_month=SCORE_DATE_LAST_MONTH_MONTH,\n",
    "        view_name=FFH_BILLING_VIEW_NAME,\n",
    "        dataset_id=DATASET_ID,\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        resource_bucket=RESOURCE_BUCKET,\n",
    "        query_path=ACCOUNT_FFH_BILLING_QUERY_PATH \n",
    "    )\n",
    "\n",
    "    # create_input_account_ffh_billing_view_op.set_memory_limit('16G')\n",
    "    # create_input_account_ffh_billing_view_op.set_cpu_limit('4')\n",
    "\n",
    "    #4.create_input_account_ffh_discounts_view\n",
    "    create_input_account_ffh_discounts_view_op = create_input_account_ffh_discounts_view(\n",
    "        view_name=FFH_DISCOUNTS_VIEW_NAME,\n",
    "        score_date=SCORE_DATE,\n",
    "        score_date_delta=SCORE_DATE_DELTA,\n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID,\n",
    "        region=REGION,\n",
    "        resource_bucket=RESOURCE_BUCKET,\n",
    "        query_path=ACCOUNT_FFH_DISCOUNTS_QUERY_PATH\n",
    "    )\n",
    "\n",
    "    # create_input_account_ffh_discounts_view_op.set_memory_limit('16G')\n",
    "    # create_input_account_ffh_discounts_view_op.set_cpu_limit('4')\n",
    "\n",
    "    #5.create_input_account_hs_usage_view\n",
    "    create_input_account_hs_usage_view_op = create_input_account_hs_usage_view(\n",
    "        v_report_date=SCORE_DATE_DASH,\n",
    "        v_start_date=SCORE_DATE_MINUS_6_MOS_DASH,\n",
    "        v_end_date=SCORE_DATE_LAST_MONTH_END_DASH,\n",
    "        v_bill_year=SCORE_DATE_LAST_MONTH_YEAR,\n",
    "        v_bill_month=SCORE_DATE_LAST_MONTH_MONTH,\n",
    "        view_name=HS_USAGE_VIEW_NAME,\n",
    "        dataset_id=DATASET_ID,\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        resource_bucket=RESOURCE_BUCKET,\n",
    "        query_path=ACCOUNT_HS_USAGE_QUERY_PATH \n",
    "    )\n",
    "\n",
    "    # create_input_account_hs_usage_view_op.set_memory_limit('16G')\n",
    "    # create_input_account_hs_usage_view_op.set_cpu_limit('4')\n",
    "\n",
    "    #6.create_input_account_demo_income_view\n",
    "    create_input_account_demo_income_view_op = create_input_account_demo_income_view(\n",
    "        score_date=SCORE_DATE,\n",
    "        score_date_delta=SCORE_DATE_DELTA,\n",
    "        view_name=DEMO_INCOME_VIEW_NAME ,\n",
    "        dataset_id=DATASET_ID,\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        resource_bucket=RESOURCE_BUCKET,\n",
    "        query_path=ACCOUNT_DEMO_INCOME_QUERY_PATH \n",
    "    )\n",
    "\n",
    "    # create_input_account_demo_income_view_op.set_memory_limit('16G')\n",
    "    # create_input_account_demo_income_view_op.set_cpu_limit('4')\n",
    "\n",
    "    #7.create_input_account_gpon_copper_view\n",
    "    create_input_account_gpon_copper_view_op = create_input_account_gpon_copper_view(\n",
    "        score_date=SCORE_DATE,\n",
    "        score_date_delta=SCORE_DATE_DELTA,\n",
    "        view_name=GPON_COPPER_VIEW_NAME,\n",
    "        dataset_id=DATASET_ID,\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        resource_bucket=RESOURCE_BUCKET,\n",
    "        query_path=ACCOUNT_GPON_COPPER_QUERY_PATH \n",
    "    )\n",
    "\n",
    "    # create_input_account_gpon_copper_view_op.set_memory_limit('16G')\n",
    "    # create_input_account_gpon_copper_view_op.set_cpu_limit('4')\n",
    "\n",
    "#     #8.create_input_account_price_plan_view\n",
    "#     create_input_account_price_plan_view_op = create_input_account_price_plan_view(\n",
    "#         score_date=SCORE_DATE,\n",
    "#         score_date_delta=SCORE_DATE_DELTA,\n",
    "#         view_name=PRICE_PLAN_VIEW_NAME ,\n",
    "#         dataset_id=DATASET_ID,\n",
    "#         project_id=PROJECT_ID,\n",
    "#         region=REGION,\n",
    "#         resource_bucket=RESOURCE_BUCKET,\n",
    "#         query_path=ACCOUNT_PRICE_PLAN_QUERY_PATH \n",
    "#     )\n",
    "\n",
    "#     # create_input_account_price_plan_view_op.set_memory_limit('16G')\n",
    "#     # create_input_account_price_plan_view_op.set_cpu_limit('4')\n",
    "\n",
    "    #9.create_input_account_clckstrm_telus_view\n",
    "    create_input_account_clckstrm_telus_view_op = create_input_account_clckstrm_telus_view(\n",
    "        score_date=SCORE_DATE,\n",
    "        score_date_delta=SCORE_DATE_DELTA,\n",
    "        view_name=CLCKSTRM_TELUS_VIEW_NAME,\n",
    "        dataset_id=DATASET_ID,\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        resource_bucket=RESOURCE_BUCKET,\n",
    "        query_path=ACCOUNT_CLCKSTRM_TELUS_QUERY_PATH\n",
    "    )\n",
    "\n",
    "    # create_input_account_clckstrm_telus_view_op.set_memory_limit('16G')\n",
    "    # create_input_account_clckstrm_telus_view_op.set_cpu_limit('4')\n",
    "\n",
    "    #10.create_input_account_call_history_view\n",
    "    create_input_account_call_history_view_op = create_input_account_call_history_view(\n",
    "        score_date=SCORE_DATE,\n",
    "        score_date_delta=SCORE_DATE_DELTA,\n",
    "        view_name=CALL_HISTORY_VIEW_NAME,\n",
    "        dataset_id=DATASET_ID,\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        resource_bucket=RESOURCE_BUCKET,\n",
    "        query_path=ACCOUNT_CALL_HISTORY_QUERY_PATH \n",
    "    )\n",
    "\n",
    "    # create_input_account_call_history_view_op.set_memory_limit('16G')\n",
    "    # create_input_account_call_history_view_op.set_cpu_limit('4')\n",
    "\n",
    "    # ----- preprocessing train data --------\n",
    "    preprocess_train_op = preprocess(\n",
    "        promo_expiry_list_view = PROMO_EXPIRY_LIST_VIEW_NAME, \n",
    "        account_consl_view=CONSL_VIEW_NAME,\n",
    "        account_bill_view=FFH_BILLING_VIEW_NAME,\n",
    "        account_discounts_view=FFH_DISCOUNTS_VIEW_NAME, \n",
    "        hs_usage_view=HS_USAGE_VIEW_NAME,\n",
    "        demo_income_view=DEMO_INCOME_VIEW_NAME,\n",
    "        gpon_copper_view=GPON_COPPER_VIEW_NAME,\n",
    "        # price_plan_view=PRICE_PLAN_VIEW_NAME,\n",
    "        clckstrm_telus_view=CLCKSTRM_TELUS_VIEW_NAME, \n",
    "        call_history_view=CALL_HISTORY_VIEW_NAME, \n",
    "        save_data_path='gs://{}/{}_score.csv.gz'.format(RESOURCE_BUCKET, SERVICE_TYPE),\n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID\n",
    "    )\n",
    "\n",
    "    # preprocess_train_op.set_memory_limit('128G')\n",
    "    # preprocess_train_op.set_cpu_limit('32')\n",
    "\n",
    "    create_input_account_promo_expiry_list_view_op\n",
    "    create_input_account_consl_view_op\n",
    "    create_input_account_ffh_billing_view_op\n",
    "    create_input_account_ffh_discounts_view_op\n",
    "    create_input_account_hs_usage_view_op\n",
    "    create_input_account_demo_income_view_op\n",
    "    create_input_account_gpon_copper_view_op\n",
    "    # create_input_account_price_plan_view_op\n",
    "    create_input_account_clckstrm_telus_view_op\n",
    "    create_input_account_call_history_view_op\n",
    "    preprocess_train_op\n",
    "\n",
    "    batch_prediction_op = batch_prediction(\n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID,\n",
    "        file_bucket=FILE_BUCKET,\n",
    "        service_type=SERVICE_TYPE,\n",
    "        score_date_dash=SCORE_DATE_DASH,\n",
    "        score_table='bq_call_to_retention_scores',\n",
    "    )\n",
    "    # batch_prediction_op.set_memory_limit('32G')\n",
    "    # batch_prediction_op.set_cpu_limit('4')\n",
    "\n",
    "    batch_prediction_op\n",
    "\n",
    "    postprocessing_op = postprocess(\n",
    "        project_id=PROJECT_ID,\n",
    "        file_bucket=FILE_BUCKET,\n",
    "        service_type=SERVICE_TYPE,\n",
    "        score_date_dash=SCORE_DATE_DASH,\n",
    "    )\n",
    "    # postprocessing_op.set_memory_limit('16G')\n",
    "    # postprocessing_op.set_cpu_limit('4')\n",
    "\n",
    "    postprocessing_op\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2a51c8-4c37-4392-a21a-c4c3279d36d4",
   "metadata": {},
   "source": [
    "### Run the Pipeline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5313e5-b5cd-4a78-9df5-91af24168e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline(project_id = PROJECT_ID,\n",
    "#         region = REGION,\n",
    "#         resource_bucket = RESOURCE_BUCKET,\n",
    "#         file_bucket = FILE_BUCKET)\n",
    "\n",
    "\n",
    "pipeline(project_id = PROJECT_ID,\n",
    "        region = REGION,\n",
    "        resource_bucket = RESOURCE_BUCKET, \n",
    "        file_bucket = FILE_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27465a63-5c3b-4e96-9c08-f9cc5dafde90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kfp.v2 import compiler\n",
    "# from google.cloud.aiplatform import pipeline_jobs\n",
    "\n",
    "# import json\n",
    "\n",
    "# compiler.Compiler().compile(\n",
    "#    pipeline_func=pipeline, package_path=\"pipeline.json\"\n",
    "# )\n",
    "\n",
    "# job = pipeline_jobs.PipelineJob(\n",
    "#                                display_name=PIPELINE_NAME,\n",
    "#                                template_path=\"pipeline.json\",\n",
    "#                                location=REGION,\n",
    "#                                enable_caching=False,\n",
    "#                                pipeline_root = f\"gs://{RESOURCE_BUCKET}\"\n",
    "# )\n",
    "# job.run(\n",
    "#    service_account = f\"bilayer-sa@{PROJECT_ID}.iam.gserviceaccount.com\"\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
