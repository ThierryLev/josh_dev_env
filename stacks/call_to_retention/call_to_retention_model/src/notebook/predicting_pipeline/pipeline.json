{
  "pipelineSpec": {
    "components": {
      "comp-batch-prediction": {
        "executorLabel": "exec-batch-prediction",
        "inputDefinitions": {
          "parameters": {
            "dataset_id": {
              "type": "STRING"
            },
            "file_bucket": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "score_date_dash": {
              "type": "STRING"
            },
            "score_table": {
              "type": "STRING"
            },
            "service_type": {
              "type": "STRING"
            },
            "temp_table": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "metrics": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            },
            "metricsc": {
              "artifactType": {
                "schemaTitle": "system.ClassificationMetrics",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-bq-create-dataset": {
        "executorLabel": "exec-bq-create-dataset",
        "inputDefinitions": {
          "parameters": {
            "dataset_id": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "promo_expiry_end": {
              "type": "STRING"
            },
            "promo_expiry_start": {
              "type": "STRING"
            },
            "region": {
              "type": "STRING"
            },
            "score_date": {
              "type": "STRING"
            },
            "score_date_delta": {
              "type": "INT"
            },
            "token": {
              "type": "STRING"
            },
            "v_end_date": {
              "type": "STRING"
            },
            "v_start_date": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "parameters": {
            "col_list": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-postprocess": {
        "executorLabel": "exec-postprocess",
        "inputDefinitions": {
          "parameters": {
            "dataset_id": {
              "type": "STRING"
            },
            "file_bucket": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "score_date_dash": {
              "type": "STRING"
            },
            "service_type": {
              "type": "STRING"
            },
            "temp_table": {
              "type": "STRING"
            },
            "token": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-preprocess": {
        "executorLabel": "exec-preprocess",
        "inputDefinitions": {
          "parameters": {
            "dataset_id": {
              "type": "STRING"
            },
            "pipeline_dataset": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "save_data_path": {
              "type": "STRING"
            }
          }
        }
      }
    },
    "deploymentSpec": {
      "executors": {
        "exec-batch-prediction": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "batch_prediction"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.18' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef batch_prediction(\n        project_id: str,\n        dataset_id: str,\n        file_bucket: str,\n        service_type: str,\n        score_table: str,\n        score_date_dash: str,\n        temp_table: str,\n        metrics: Output[Metrics],\n        metricsc: Output[ClassificationMetrics],\n):\n    import time\n    import pandas as pd\n    import numpy as np\n    import pickle\n    from datetime import date\n    from dateutil.relativedelta import relativedelta\n    from google.cloud import bigquery\n    from google.cloud import storage\n\n    MODEL_ID = '5090'\n\n    def if_tbl_exists(bq_client, table_ref):\n        from google.cloud.exceptions import NotFound\n        try:\n            bq_client.get_table(table_ref)\n            return True\n        except NotFound:\n            return False\n\n    def upsert_table(project_id, dataset_id, table_id, sql, result):\n        new_values = ',\\n'.join(result.apply(lambda row: row_format(row), axis=1))\n        new_sql = sql.format(proj_id=project_id, dataset_id=dataset_id, table_id=table_id,\n                             new_values=new_values)\n        bq_client = bigquery.Client(project=project_id)\n        code = bq_client.query(new_sql)\n        time.sleep(5)\n\n    def row_format(row):\n        values = row.values\n        new_values = \"\"\n        v = str(values[0]) if not pd.isnull(values[0]) else 'NULL'\n        if 'str' in str(type(values[0])):\n            new_values += f\"'{v}'\"\n        else:\n            new_values += f\"{v}\"\n\n        for i in range(1, len(values)):\n            v = str(values[i]) if not pd.isnull(values[i]) else 'NULL'\n            if 'str' in str(type(values[i])):\n                new_values += f\",'{v}'\"\n            else:\n                new_values += f\",{v}\"\n        return '(' + new_values + ')'\n\n    def generate_sql_file(ll):\n        s = 'MERGE INTO `{proj_id}.{dataset_id}.{table_id}` a'\n        s += \" USING UNNEST(\"\n        s += \"[struct<\"\n        for i in range(len(ll) - 1):\n            v = ll[i]\n            s += \"{} {},\".format(v[0], v[1])\n        s += \"{} {}\".format(ll[-1][0], ll[-1][1])\n        s += \">{new_values}]\"\n        s += \") b\"\n        s += \" ON a.ban = b.ban and a.score_date = b.score_date\"\n        s += \" WHEN MATCHED THEN\"\n        s += \" UPDATE SET \"\n        s += \"a.{}=b.{},\".format(ll[0][0], ll[0][0])\n        for i in range(1, len(ll) - 1):\n            v = ll[i]\n            s += \"a.{}=b.{},\".format(v[0], v[0])\n        s += \"a.{}=b.{}\".format(ll[-1][0], ll[-1][0])\n        s += \" WHEN NOT MATCHED THEN\"\n        s += \" INSERT(\"\n        for i in range(len(ll) - 1):\n            v = ll[i]\n            s += \"{},\".format(v[0])\n        s += \"{})\".format(ll[-1][0])\n        s += \" VALUES(\"\n        for i in range(len(ll) - 1):\n            s += \"b.{},\".format(ll[i][0])\n        s += \"b.{}\".format(ll[-1][0])\n        s += \")\"\n\n        return s\n\n    MODEL_PATH = '{}_xgb_models/'.format(service_type)\n    df_score = pd.read_csv('gs://{}/{}_score.csv.gz'.format(file_bucket, service_type), compression='gzip')\n    df_score.dropna(subset=['ban'], inplace=True)\n    df_score.reset_index(drop=True, inplace=True)\n    print('......scoring data loaded:{}'.format(df_score.shape))\n    time.sleep(10)\n\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(file_bucket)\n    blobs = storage_client.list_blobs(file_bucket, prefix='{}{}_models_xgb_'.format(MODEL_PATH, service_type))\n\n    model_lists = []\n    for blob in blobs:\n        model_lists.append(blob.name)\n\n    blob = bucket.blob(model_lists[-1])\n    blob_in = blob.download_as_string()\n    model_dict = pickle.loads(blob_in)\n    model_xgb = model_dict['model']\n    features = model_dict['features']\n    print('...... model loaded')\n    time.sleep(10)\n\n    ll = [('ban', 'string'), ('score_date', 'string'), ('model_id', 'string'), ('score', 'float64')]\n    sql = generate_sql_file(ll)\n\n    df_score['ban'] = df_score['ban'].astype(int)\n    print('.... scoring for {} promo expiry bans base'.format(len(df_score)))\n\n    # get full score to cave into bucket\n    pred_prob = model_xgb.predict_proba(df_score[features], ntree_limit=model_xgb.best_iteration)[:, 1]\n    result = pd.DataFrame(columns=['ban', 'score_date', 'model_id', 'score'])\n    result['score'] = list(pred_prob)\n    result['score'] = result['score'].fillna(0.0).astype('float64')\n    result['ban'] = list(df_score['ban'])\n    result['ban'] = result['ban'].astype('str')\n    result['score_date'] = score_date_dash\n    result['model_id'] = MODEL_ID\n\n    result.to_csv('gs://{}/ucar/{}_prediction.csv.gz'.format(file_bucket, service_type), compression='gzip',\n                  index=False)\n    time.sleep(60)\n\n    table_ref = f'{project_id}.{dataset_id}.{score_table}'\n    client = bigquery.Client(project=project_id)\n    table = client.get_table(table_ref)\n    schema = table.schema\n\n    ll = []\n    for item in schema:\n        col = item.name\n        d_type = item.field_type\n        if 'float' in str(d_type).lower():\n            d_type = 'FLOAT64'\n        ll.append((col, d_type))\n\n        if 'integer' in str(d_type).lower():\n            result[col] = result[col].fillna(0).astype(int)\n        if 'float' in str(d_type).lower():\n            result[col] = result[col].fillna(0.0).astype(float)\n        if 'string' in str(d_type).lower():\n            result[col] = result[col].fillna('').astype(str)\n\n    table_ref = '{}.{}.{}'.format(project_id, dataset_id, temp_table)\n    client = bigquery.Client(project=project_id)\n    if if_tbl_exists(client, table_ref):\n        client.delete_table(table_ref)\n\n    client.create_table(table_ref)\n    config = bigquery.LoadJobConfig(schema=schema)\n    config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n    bq_table_instance = client.load_table_from_dataframe(result, table_ref, job_config=config)\n    time.sleep(5)\n\n    drop_sql = f\"\"\"delete from `{project_id}.{dataset_id}.{score_table}` where score_date = '{score_date_dash}'\"\"\"  # .format(project_id, dataset_id, score_date_dash)\n    client.query(drop_sql)\n    #\n    load_sql = f\"\"\"insert into `{project_id}.{dataset_id}.{score_table}`\n                  select * from `{project_id}.{dataset_id}.{temp_table}`\"\"\"\n    client.query(load_sql)\n\n"
            ],
            "image": "northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/kfp-pycaret-slim:latest",
            "resources": {
              "cpuLimit": 4.0,
              "memoryLimit": 32.0
            }
          }
        },
        "exec-bq-create-dataset": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "bq_create_dataset"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.18' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef bq_create_dataset(score_date: str,\n                      score_date_delta: int,\n                      project_id: str,\n                      dataset_id: str,\n                      region: str,\n                      promo_expiry_start: str, \n                      promo_expiry_end: str, \n                      v_start_date: str,\n                      v_end_date: str,\n                      token: str) -> NamedTuple(\"output\", [(\"col_list\", list)]):\n\n    import google\n    from google.cloud import bigquery\n    from datetime import datetime\n    import logging \n    import os \n    import re \n    from google.oauth2 import credentials\n    from google.oauth2 import service_account\n    # For wb\n    # import google.oauth2.credentials\n    # CREDENTIALS = google.oauth2.credentials.Credentials(token)\n\n    #def get_gcp_bqclient(project_id, use_local_credential=True):\n        #token = os.popen('gcloud auth print-access-token').read()\n        #token = re.sub(f'\\n$', '', token)\n        #credentials = google.oauth2.credentials.Credentials(token)\n\n        #bq_client = bigquery.Client(project=project_id)\n        #if use_local_credential:\n            #bq_client = bigquery.Client(project=project_id, credentials=credentials)\n        #return bq_client\n\n    #client = get_gcp_bqclient(project_id)\n    #job_config = bigquery.QueryJobConfig()\n\n    CREDENTIALS = google.oauth2.credentials.Credentials(token) # get credentials from token\n\n    client = bigquery.Client(project=project_id, credentials=CREDENTIALS)\n    job_config = bigquery.QueryJobConfig()\n\n    # Change dataset / table + sp table name to version in bi-layer\n    query =\\\n        f'''\n            DECLARE score_date DATE DEFAULT \"{score_date}\";\n            DECLARE promo_expiry_start DATE DEFAULT \"{promo_expiry_start}\";\n            DECLARE promo_expiry_end DATE DEFAULT \"{promo_expiry_end}\";\n            DECLARE start_date DATE DEFAULT \"{v_start_date}\";\n            DECLARE end_date DATE DEFAULT \"{v_end_date}\";\n\n            -- Change dataset / sp name to the version in the bi_layer\n            CALL {dataset_id}.bq_sp_ctr_pipeline_dataset(score_date, promo_expiry_start, promo_expiry_end, start_date, end_date);\n\n            SELECT\n                *\n            FROM {dataset_id}.INFORMATION_SCHEMA.PARTITIONS\n            WHERE table_name='bq_ctr_pipeline_dataset'\n\n        '''\n\n    df = client.query(query, job_config=job_config).to_dataframe()\n    logging.info(df.to_string())\n\n    logging.info(f\"Loaded {df.total_rows[0]} rows into \\\n             {df.table_catalog[0]}.{df.table_schema[0]}.{df.table_name[0]} on \\\n             {datetime.strftime((df.last_modified_time[0]), '%Y-%m-%d %H:%M:%S') } !\")\n\n    ######################################## Save column list_##########################\n    query =\\\n        f'''\n           SELECT\n                *\n            FROM {dataset_id}.bq_ctr_pipeline_dataset\n\n        '''\n\n    df = client.query(query, job_config=job_config).to_dataframe()\n\n    col_list = list([col for col in df.columns])\n    return (col_list,)\n\n"
            ],
            "image": "northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/kfp-pycaret-slim:latest"
          }
        },
        "exec-postprocess": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "postprocess"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.18' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef postprocess(\n        project_id: str,\n        file_bucket: str,\n        dataset_id: str,\n        service_type: str,\n        score_date_dash: str,\n        temp_table: str, \n        token: str\n):\n    import google\n    import time\n    from datetime import date\n    from dateutil.relativedelta import relativedelta\n    import pandas as pd\n    from google.cloud import bigquery\n    from google.oauth2 import credentials\n    from google.oauth2 import service_account\n\n    def if_tbl_exists(client, table_ref):\n        from google.cloud.exceptions import NotFound\n        try:\n            client.get_table(table_ref)\n            return True\n        except NotFound:\n            return False\n\n    MODEL_ID = '5090'\n    file_name = 'gs://{}/ucar/{}_prediction.csv.gz'.format(file_bucket, service_type)\n    df_orig = pd.read_csv(file_name, compression='gzip')\n    df_orig.dropna(subset=['ban'], inplace=True)\n    df_orig.reset_index(drop=True, inplace=True)\n    df_orig['scoring_date'] = score_date_dash\n    df_orig.ban = df_orig.ban.astype(int)\n    df_orig = df_orig.rename(columns={'ban': 'bus_bacct_num', 'score': 'score_num'})\n    df_orig.score_num = df_orig.score_num.astype(float)\n    df_orig['decile_grp_num'] = pd.qcut(df_orig['score_num'], q=10, labels=[i for i in range(10, 0, -1)])\n    df_orig.decile_grp_num = df_orig.decile_grp_num.astype(int)\n    df_orig['percentile_pct'] = (1 - df_orig.score_num.rank(pct=True))*100\n    df_orig['percentile_pct'] = df_orig['percentile_pct'].apply(round, 0).astype(int)\n    df_orig['predict_model_nm'] = 'FFH CALL TO RETENTION Model - DIVG'\n    df_orig['model_type_cd'] = 'FFH'\n    df_orig['subscriber_no'] = \"\"\n    df_orig['prod_instnc_resrc_str'] = \"\"\n    df_orig['service_instnc_id'] = \"\"\n    df_orig['segment_nm'] = \"\"\n    df_orig['segment_id'] = \"\"\n    df_orig['classn_nm'] = \"\"\n    df_orig['predict_model_id'] = MODEL_ID\n    df_orig.drop(columns=['model_id', 'score_date'], axis=1, inplace=True)\n\n    get_cust_id = \"\"\"\n    WITH bq_snpsht_max_date AS(\n    SELECT PARSE_DATE('%Y%m%d', MAX(partition_id)) AS max_date\n        FROM `cio-datahub-enterprise-pr-183a.ent_cust_cust.INFORMATION_SCHEMA.PARTITIONS` \n    WHERE table_name = 'bq_prod_instnc_snpsht' \n        AND partition_id <> '__NULL__'\n    ),\n    -- BANs can have multiple Cust ID. Create rank by product type and status, prioritizing ban/cust id with active FFH products\n    rank_prod_type AS (\n    SELECT DISTINCT\n        bacct_bus_bacct_num,\n        consldt_cust_bus_cust_id AS cust_id,\n        CASE WHEN pi_prod_instnc_resrc_typ_cd IN ('SING', 'HSIC', 'TTV', 'SMHM', 'STV', 'DIIC') AND pi_prod_instnc_stat_cd = 'A' THEN 1\n                WHEN pi_prod_instnc_resrc_typ_cd IN ('SING', 'HSIC', 'TTV', 'SMHM', 'STV', 'DIIC') THEN 2\n                WHEN pi_prod_instnc_stat_cd = 'A' THEN 3\n                ELSE 4\n                END AS prod_rank\n    FROM `cio-datahub-enterprise-pr-183a.ent_cust_cust.bq_prod_instnc_snpsht`\n    CROSS JOIN bq_snpsht_max_date\n    WHERE CAST(prod_instnc_ts AS DATE)=bq_snpsht_max_date.max_date\n    AND bus_prod_instnc_src_id = 1001\n    ),\n    --Rank Cust ID\n    rank_cust_id AS (\n    SELECT DISTINCT\n        bacct_bus_bacct_num,\n        cust_id,\n        RANK() OVER(PARTITION BY bacct_bus_bacct_num\n                        ORDER BY prod_rank,\n                                    cust_id) AS cust_id_rank               \n    FROM rank_prod_type\n    )\n    --Select best cust id\n    SELECT bacct_bus_bacct_num,\n        cust_id\n    FROM rank_cust_id\n    WHERE cust_id_rank = 1\n    \"\"\"\n\n    CREDENTIALS = google.oauth2.credentials.Credentials(token) # get credentials from token\n\n    client = bigquery.Client(project=project_id, credentials=CREDENTIALS)\n    df_cust = client.query(get_cust_id).to_dataframe()\n    df_final = df_orig.set_index('bus_bacct_num').join(df_cust.set_index('bacct_bus_bacct_num')).reset_index()\n    df_final = df_final.rename(columns={'index': 'bus_bacct_num', 'cust_bus_cust_id': 'cust_id'})\n    df_final = df_final.sort_values(by=['score_num'], ascending=False)\n    df_final.to_csv(file_name, compression='gzip', index=False)\n    time.sleep(120)\n\n"
            ],
            "image": "northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/kfp-pycaret-slim:latest",
            "resources": {
              "cpuLimit": 4.0,
              "memoryLimit": 32.0
            }
          }
        },
        "exec-preprocess": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "preprocess"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.18' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef preprocess(\n        pipeline_dataset: str, \n        save_data_path: str,\n        project_id: str,\n        dataset_id: str\n):\n    from google.cloud import bigquery\n    import pandas as pd\n    import gc\n    import time\n\n    client = bigquery.Client(project=project_id)\n\n    # pipeline_dataset \n    pipeline_dataset_name = f\"{project_id}.{dataset_id}.{pipeline_dataset}\" \n    build_df_pipeline_dataset = f'SELECT * FROM `{pipeline_dataset_name}`'\n    df_pipeline_dataset = client.query(build_df_pipeline_dataset).to_dataframe()\n    df_pipeline_dataset = df_pipeline_dataset.set_index('ban') \n\n    # demo columns\n    df_pipeline_dataset['demo_urban_flag'] = df_pipeline_dataset.demo_sgname.str.lower().str.contains('urban').fillna(0).astype(int)\n    df_pipeline_dataset['demo_rural_flag'] = df_pipeline_dataset.demo_sgname.str.lower().str.contains('rural').fillna(0).astype(int)\n    df_pipeline_dataset['demo_family_flag'] = df_pipeline_dataset.demo_lsname.str.lower().str.contains('families').fillna(0).astype(int)\n\n    df_income_dummies = pd.get_dummies(df_pipeline_dataset[['demo_lsname']]) \n    df_income_dummies.columns = df_income_dummies.columns.str.replace('&', 'and')\n    df_income_dummies.columns = df_income_dummies.columns.str.replace(' ', '_')\n\n    df_pipeline_dataset.drop(columns=['demo_sgname', 'demo_lsname'], axis=1, inplace=True)\n\n    df_pipeline_dataset = df_pipeline_dataset.join(df_income_dummies)\n\n    df_join = df_pipeline_dataset.copy()\n\n    #column name clean-up\n    df_join.columns = df_join.columns.str.replace(' ', '_')\n    df_join.columns = df_join.columns.str.replace('-', '_')\n\n    #df_final\n    df_final = df_join.copy()\n    del df_join\n    gc.collect()\n    print('......df_final done')\n\n    for f in df_final.columns:\n        df_final[f] = list(df_final[f])\n\n    df_final.to_csv(save_data_path, index=True, compression='gzip') \n    print(df_final.shape)\n    del df_final\n    gc.collect()\n    print(f'......csv saved in {save_data_path}')\n    time.sleep(120)\n\n"
            ],
            "image": "northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/kfp-pycaret-slim:latest",
            "resources": {
              "cpuLimit": 4.0,
              "memoryLimit": 32.0
            }
          }
        }
      }
    },
    "pipelineInfo": {
      "name": "call-to-retention-predict-pipeline"
    },
    "root": {
      "dag": {
        "outputs": {
          "artifacts": {
            "batch-prediction-metrics": {
              "artifactSelectors": [
                {
                  "outputArtifactKey": "metrics",
                  "producerSubtask": "batch-prediction"
                }
              ]
            },
            "batch-prediction-metricsc": {
              "artifactSelectors": [
                {
                  "outputArtifactKey": "metricsc",
                  "producerSubtask": "batch-prediction"
                }
              ]
            }
          }
        },
        "tasks": {
          "batch-prediction": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-batch-prediction"
            },
            "dependentTasks": [
              "preprocess"
            ],
            "inputs": {
              "parameters": {
                "dataset_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "call_to_retention_dataset"
                    }
                  }
                },
                "file_bucket": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-groovyhoon-pr-d2eab4-default"
                    }
                  }
                },
                "project_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-groovyhoon-pr-d2eab4"
                    }
                  }
                },
                "score_date_dash": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "2023-07-01"
                    }
                  }
                },
                "score_table": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "bq_call_to_retention_scores"
                    }
                  }
                },
                "service_type": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "call_to_retention"
                    }
                  }
                },
                "temp_table": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "temp_call_to_retention_scores"
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "batch-prediction"
            }
          },
          "bq-create-dataset": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-bq-create-dataset"
            },
            "inputs": {
              "parameters": {
                "dataset_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "call_to_retention_dataset"
                    }
                  }
                },
                "project_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-groovyhoon-pr-d2eab4"
                    }
                  }
                },
                "promo_expiry_end": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "2023-11-01"
                    }
                  }
                },
                "promo_expiry_start": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "2023-10-01"
                    }
                  }
                },
                "region": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "northamerica-northeast1"
                    }
                  }
                },
                "score_date": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "2023-07-01"
                    }
                  }
                },
                "score_date_delta": {
                  "runtimeValue": {
                    "constantValue": {
                      "intValue": "0"
                    }
                  }
                },
                "token": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "ya29.a0AfB_byAIPDNKrrFpJVOtjpAs6cOaYdXM5aCzkhRHqOtIKcdU-d00XKfvisg9u6OdYt-DJ5gcQllkZQ-T2xnS0zhJhQHRf-k-FkHlRim86lAaRrug0xAxvAL__H-4O6i49RklRp27iahIQzRzo9pqjiLsyB01zynHg1x8GJCvRJkaCgYKAZsSARISFQGOcNnCiwmPciD3HyM5p5Cq0ezMIw0178"
                    }
                  }
                },
                "v_end_date": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "2023-06-30"
                    }
                  }
                },
                "v_start_date": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "2023-01-01"
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "bq-create-dataset"
            }
          },
          "postprocess": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-postprocess"
            },
            "dependentTasks": [
              "batch-prediction"
            ],
            "inputs": {
              "parameters": {
                "dataset_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "call_to_retention_dataset"
                    }
                  }
                },
                "file_bucket": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-groovyhoon-pr-d2eab4-default"
                    }
                  }
                },
                "project_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-groovyhoon-pr-d2eab4"
                    }
                  }
                },
                "score_date_dash": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "2023-07-01"
                    }
                  }
                },
                "service_type": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "call_to_retention"
                    }
                  }
                },
                "temp_table": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "temp_call_to_retention_scores"
                    }
                  }
                },
                "token": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "ya29.a0AfB_byAIPDNKrrFpJVOtjpAs6cOaYdXM5aCzkhRHqOtIKcdU-d00XKfvisg9u6OdYt-DJ5gcQllkZQ-T2xnS0zhJhQHRf-k-FkHlRim86lAaRrug0xAxvAL__H-4O6i49RklRp27iahIQzRzo9pqjiLsyB01zynHg1x8GJCvRJkaCgYKAZsSARISFQGOcNnCiwmPciD3HyM5p5Cq0ezMIw0178"
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "postprocess"
            }
          },
          "preprocess": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-preprocess"
            },
            "dependentTasks": [
              "bq-create-dataset"
            ],
            "inputs": {
              "parameters": {
                "dataset_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "call_to_retention_dataset"
                    }
                  }
                },
                "pipeline_dataset": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "bq_ctr_pipeline_dataset"
                    }
                  }
                },
                "project_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-groovyhoon-pr-d2eab4"
                    }
                  }
                },
                "save_data_path": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "gs://divg-groovyhoon-pr-d2eab4-default/call_to_retention_score.csv.gz"
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "preprocess"
            }
          }
        }
      },
      "inputDefinitions": {
        "parameters": {
          "file_bucket": {
            "type": "STRING"
          },
          "project_id": {
            "type": "STRING"
          },
          "region": {
            "type": "STRING"
          },
          "resource_bucket": {
            "type": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "batch-prediction-metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          },
          "batch-prediction-metricsc": {
            "artifactType": {
              "schemaTitle": "system.ClassificationMetrics",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    },
    "schemaVersion": "2.0.0",
    "sdkVersion": "kfp-1.8.18"
  },
  "runtimeConfig": {
    "parameters": {
      "file_bucket": {
        "stringValue": "divg-groovyhoon-pr-d2eab4-default"
      },
      "project_id": {
        "stringValue": "divg-groovyhoon-pr-d2eab4"
      },
      "region": {
        "stringValue": "northamerica-northeast1"
      },
      "resource_bucket": {
        "stringValue": "divg-groovyhoon-pr-d2eab4-default"
      }
    }
  }
}