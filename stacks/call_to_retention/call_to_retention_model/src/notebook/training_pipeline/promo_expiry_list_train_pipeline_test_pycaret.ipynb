{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c150da3-6e1e-4f02-a778-bb3b12af9054",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b559b7e6-fe51-49cc-8e9c-832380843832",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output, OutputPath, ClassificationMetrics,\n",
    "                        Metrics, component)\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import google\n",
    "from google.oauth2 import credentials\n",
    "from google.oauth2 import service_account\n",
    "from google.oauth2.service_account import Credentials\n",
    "from google.cloud import storage\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google_cloud_pipeline_components.v1.batch_predict_job import \\\n",
    "    ModelBatchPredictOp as batch_prediction_op\n",
    "\n",
    "import logging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a857ffa-be95-4fae-b4c0-f66f1e513588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "pd.options.display.max_columns = 170\n",
    "from datetime import date, timedelta\n",
    "import sys\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (15, 5)\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "import math\n",
    "\n",
    "import scipy.stats as stats\n",
    "import statsmodels.stats.api as sms\n",
    "\n",
    "from pycaret.utils import check_metric\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "from pycaret.classification import * \n",
    "\n",
    "dtype_bq_mapping = {\n",
    "    np.dtype('int64') : \"INTEGER\",\n",
    "    np.dtype('float64') : \"FLOAT\",\n",
    "    np.dtype('float32') : \"FLOAT\",\n",
    "    np.dtype('object') : \"STRING\",\n",
    "    np.dtype('bool') : \"BOOLEAN\",\n",
    "    np.dtype('datetime64[ns]') : \"DATE\",\n",
    "    pd.Int64Dtype() : \"INTEGER\"\n",
    "\n",
    "}\n",
    "\n",
    "def export_dataframe_to_bq(df, table_id='', schema_list=[], generate_schema=True, write='overwrite'):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    df: dataframe that you want to export to BQ\n",
    "    table_id: string with dataset and table name. Ie: 'customer_personas_reports.firefly_campaign_output' \n",
    "    schema_list: List of the SchemaFields if provided, otherwise the function can generate it for you.\n",
    "    generate_schema: True (Function will generate schema for you). False: Provide own schema list\n",
    "    \n",
    "    Ie. table_id = project_id.dataset_id.table_name\n",
    "    write = 'overwrite' will overwrite the existing table in BQ\n",
    "    \"\"\"\n",
    "    if write == 'overwrite':\n",
    "        write_type = 'WRITE_TRUNCATE'\n",
    "    else:\n",
    "        write_type = 'WRITE_APPEND'\n",
    "    \n",
    "    if ((generate_schema == False) & (len(schema_list) == 0)):\n",
    "        print('Error: Provide Schema List, otherwise set generate_schema to True')\n",
    "        return \n",
    "    if table_id=='':\n",
    "        print('Error: Provide table_id')\n",
    "    else:\n",
    "        if generate_schema==True:\n",
    "            schema_list=[]\n",
    "            for column in df.columns:\n",
    "                schema_list.append(bigquery.SchemaField(column, dtype_bq_mapping[df.dtypes[column]], mode='NULLABLE'))\n",
    "        print(schema_list)\n",
    "        \n",
    "        #Sending to bigquery\n",
    "        client=bigquery.Client()\n",
    "        job_config = bigquery.LoadJobConfig(schema=schema_list, write_disposition=write_type)\n",
    "        job=client.load_table_from_dataframe(df, table_id, job_config=job_config)\n",
    "        job.result()\n",
    "        table = client.get_table(table_id)  # Make an API request.\n",
    "        print(\"Loaded {} rows and {} columns to {}\".format(table.num_rows, len(table.schema), table_id))\n",
    "\n",
    "\n",
    "def woe_cat_columns(df,cat_columns):\n",
    "    dict_list = []\n",
    "    for col in cat_columns :\n",
    "        print(col)\n",
    "        feature,target = col,'rpc_flag'\n",
    "        df_woe_iv = (pd.crosstab(df[feature],df[target],normalize='columns')\n",
    "                                                                             .assign(woe=lambda dfx: np.log(dfx[1] / dfx[0]))\n",
    "                                                                             .assign(iv=lambda dfx: np.sum(dfx['woe']*(dfx[1]-dfx[0])))).reset_index()\n",
    "       \n",
    "        # df_woe_iv[\"iv\"] = df_woe_iv.assign(iv=lambda dfx: np.sum(dfx['woe']* (dfx[1]-dfx[0])))\n",
    "        df[feature] = df[feature].map(dict(zip(df_woe_iv[feature], df_woe_iv.woe)))  \n",
    "       \n",
    "        dict_list.append(dict(zip(df_woe_iv[col], df_woe_iv.woe)))\n",
    "    return df, dict_list\n",
    "\n",
    "def pre_process_data(df, plan_df, whsia_plan_df):\n",
    "    df_missing_values = pd.DataFrame(df.isnull().sum()/df.shape[0]*100, columns=['perc_missing'])\n",
    "    # Display(df_missing_values.transpose())\n",
    "\n",
    "    # Drop features where entire column is empty\n",
    "    drop_cols = list(df_missing_values.loc[df_missing_values['perc_missing'] == 100].index)\n",
    "    df.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "    numeric_features = df.select_dtypes(include=['int64','float64']).columns\n",
    "    ccai_features = [col for col in df.columns if 'ccai' in col or 'convrstn_durtn' in col]\n",
    "    marketing_msg_features = [col for col in df.columns if 'mrkt_msg' in col]\n",
    "    trip_bank_features = [col for col in df.columns if 'total_trips' in col or 'total_vacation' in col]\n",
    "    billing_features = [col for col in df.columns if 'befor_discnt' in col or 'disc_amt' in col]\n",
    "    wls_clckstrm_features = [col for col in df.columns if 'wls' in col]\n",
    "    data_usg_features = [col for col in df.columns if 'data_usg' in col]\n",
    "    txt_usg_features = [col for col in df.columns if 'sms_actl_unit' in col]\n",
    "    call_usg_features = ['total_airtime_dur_min_1m','total_calls_1m','total_ld_calls_1m','total_ld_us_og_calls_1m','total_ld_us_og_airtime_dur_min_1m','total_airtime_chrg_amt_1m',\n",
    "                        'total_airtime_num_mins_to_rate_1m','ld_call_duration_dly_mins_1m','local_call_duration_dly_mins_1m','total_toll_dur_min_1m','total_toll_num_mins_to_rate_1m','total_toll_chrg_amt_1m',]\n",
    "    memo_features = [col for col in df.columns if 'memo' in col]\n",
    "\n",
    "    # Create plan_type feature and drop rows where price plan is not cellular\n",
    "\n",
    "    df['plan_type'] = 'other'\n",
    "    df.loc[df.pp_bus_pp_catlg_itm_cd.isin(plan_df.Plan_5G), 'plan_type'] = '5G'\n",
    "    df.loc[df.pp_bus_pp_catlg_itm_cd.isin(plan_df.Plan_5G_Plus), 'plan_type'] = '5G+'\n",
    "    df.loc[df.pp_bus_pp_catlg_itm_cd.isin(plan_df.Unlimited_Plan), 'plan_type'] = 'Unlimited'\n",
    "    df.loc[df.pp_bus_pp_catlg_itm_cd.isin(plan_df.POM_POMC), 'plan_type'] = 'POM_POMC'\n",
    "    df.loc[df.pp_bus_pp_catlg_itm_cd.isin(plan_df.POM), 'plan_type'] = 'POM'\n",
    "    df.loc[df.pp_bus_pp_catlg_itm_cd.isin(plan_df.POMC), 'plan_type'] = 'POMC'\n",
    "    df.loc[df.pp_bus_pp_catlg_itm_cd.isin(plan_df.Plan_4G), 'plan_type'] = 'Plan_4G'\n",
    "    df.loc[df.pp_bus_pp_catlg_itm_cd.isin(whsia_plan_df.soc_cd), 'plan_type'] = 'WHSIA'\n",
    "\n",
    "    df = df.loc[df.pp_catlg_itm_nm.str.contains('(?i)watch|tablet|wireless internet')==False].reset_index(drop=True)\n",
    "    df = df.loc[df.plan_type!='WHSIA'].reset_index(drop=True)\n",
    "    df = df.loc[~(df.allowance_qty_normalized > 500)].reset_index(drop=True)\n",
    "\n",
    "    # Fill in missing values\n",
    "    df[['is_5g_capable', 'is_4g_capable']] = df[['is_5g_capable', 'is_4g_capable']].fillna('Unknown')\n",
    "    df['device_age_days'] = (datetime.today() - pd.to_datetime(df['proj_launch_dt1'])).dt.days\n",
    "\n",
    "    df['total_completed_checkouts'] = df['total_completed_checkouts'].fillna(0)\n",
    "    df['cnt_sub'] = df['cnt_sub'].fillna(1)\n",
    "\n",
    "    df['pp_recur_chrg_amt'] = df['pp_recur_chrg_amt'].fillna(df['pp_recur_chrg_amt'].mean())\n",
    "    df['allowance_qty_normalized'] = df['allowance_qty_normalized'].fillna(df['allowance_qty_normalized'].mean())\n",
    "    df['device_age_days'] = df['device_age_days'].fillna(df['device_age_days'].mean())\n",
    "\n",
    "    num_feats = [ccai_features, marketing_msg_features, trip_bank_features, billing_features, wls_clckstrm_features, data_usg_features, txt_usg_features, call_usg_features, memo_features]\n",
    "    num_feats_flatten = [col for num_feats_col in num_feats for col in num_feats_col]\n",
    "\n",
    "    df[num_feats_flatten] = df[num_feats_flatten].fillna(0)\n",
    "    cat_cols = ['is_5g_capable' ,'is_4g_capable', 'plan_type']\n",
    "    #df[cat_cols + ['rpc_flag']], cat_cols_transformed_dict = woe_cat_columns(df=df[cat_cols + ['rpc_flag']],cat_columns=cat_cols)\n",
    "    df['soc_effective_ts'] = df['soc_effective_ts'].astype('datetime64[ns]')\n",
    "    \n",
    "    dbdate_cols = ['partition_dt', 'starting_date_data_collection' ,'ending_date_data_collection',\n",
    "               'effective_dt' ,'proj_launch_dt1']\n",
    "\n",
    "    for col in dbdate_cols:\n",
    "        df[col] = pd.to_datetime(df[col], errors = 'coerce')\n",
    "        \n",
    "    convert_ts_to_datetime = [col for col in df.columns if (\"ts\" in col or \"dt\" in col) and df.dtypes[col] == object]\n",
    "    for col in convert_ts_to_datetime:\n",
    "        df[col] = pd.to_datetime(df[col], errors = 'coerce')\n",
    "        \n",
    "    df.loc[df.bacct_ebill_ind == 'Y', 'bacct_ebill_ind'] = 1\n",
    "    df.loc[df.bacct_ebill_ind == 'N', 'bacct_ebill_ind'] = 0\n",
    "    df.bacct_ebill_ind = df.bacct_ebill_ind.astype(int)\n",
    "    return df\n",
    "\n",
    "def downsampling(df,true_to_false_ratio):\n",
    "    expected_rpc_flag_0 = df[df[\"rpc_flag\"]==1][\"rpc_flag\"].value_counts().values[0]/true_to_false_ratio\n",
    "    perc_samples = expected_rpc_flag_0/df[df[\"rpc_flag\"]==0][\"rpc_flag\"].value_counts().values[0]\n",
    "    \n",
    "    unique_trips_data = df[df[\"rpc_flag\"]==0][[\"MSISDN\",\"partition_dt\"]].drop_duplicates()\n",
    "    unique_trips_data[\"event_id\"] = unique_trips_data[\"MSISDN\"] +\"#\"+unique_trips_data[\"partition_dt\"].astype(str)\n",
    "    unique_trips_sampled = unique_trips_data.sample(int(unique_trips_data.shape[0]*perc_samples))\n",
    "    data_negtv_class = df[df[\"rpc_flag\"]==0]\n",
    "    data_negtv_class[\"event_id\"] = data_negtv_class[\"MSISDN\"] +\"#\"+data_negtv_class[\"partition_dt\"].astype(str)\n",
    "    data_ngtv_sampled = data_negtv_class[data_negtv_class[\"event_id\"].isin(unique_trips_sampled.event_id.values)]\n",
    "    data_ngtv_sampled.drop(columns=[\"event_id\"],inplace=True)\n",
    "    data_downsampled = pd.concat([data_ngtv_sampled,df[df[\"rpc_flag\"]!=0]])\n",
    "    return data_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478a95dd-7a01-4d44-a296-ba77db559a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "telus_purple = '#4B286D'\n",
    "telus_green = '#66CC00'\n",
    "telus_grey = '#F4F4F7'\n",
    "from google.cloud import storage\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import plotly.express as px\n",
    "from pycaret.classification import * \n",
    "import joblib\n",
    "import os \n",
    "import pickle\n",
    "\n",
    "def create_folder_if_not_exists(path):\n",
    "    \"\"\"\n",
    "    Create a new folder based on a path if that path does not currently exist.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        print(f\"Folder created: {path}\")\n",
    "    else:\n",
    "        print(f\"Folder already exists: {path}\")\n",
    "\n",
    "def ploty_model_metrics(actual, predicted,  plot=False):\n",
    "    f1_score_ = f1_score(actual, predicted)\n",
    "    recall_score_ = recall_score(actual, predicted)\n",
    "    acc_score_ = accuracy_score(actual, predicted)\n",
    "    pr_score_ = precision_score(actual, predicted)\n",
    "    \n",
    "    metrics_df = pd.DataFrame(data=[[acc_score_, pr_score_,recall_score_, f1_score_,]],\n",
    "                              columns=['Accuracy', 'Precision', 'Recall', 'F1_score'])\n",
    "\n",
    "    trace = go.Bar(x = (metrics_df.T[0].values), \n",
    "                    y = list(metrics_df.columns), \n",
    "                    text = np.round_(metrics_df.T[0].values,4),\n",
    "                    textposition = 'auto',\n",
    "                    orientation = 'h', \n",
    "                    opacity = 0.8,\n",
    "                    marker=dict(\n",
    "                                color=[telus_purple] * 4,\n",
    "                                line=dict(color='#000000',width=1.5)\n",
    "                            )\n",
    "                   )\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(trace)\n",
    "    fig.update_layout(title='Model Metrics')\n",
    "    \n",
    "    if plot:\n",
    "        fig.show()\n",
    "    return  metrics_df, fig\n",
    "\n",
    "def plotly_confusion_matrix(actual, \n",
    "                            predicted, \n",
    "                            axis_labels='',\n",
    "                            plot=False):\n",
    "    cm=confusion_matrix(actual, predicted)\n",
    " \n",
    "    if axis_labels=='':\n",
    "        x = [str(x) for x in range(actual.nunique())]\n",
    "        #list(np.arange(0, actual.nunique()))\n",
    "        y = x\n",
    "    else:\n",
    "        y = axis_labels\n",
    "        x = axis_labels\n",
    "\n",
    "    fig = px.imshow(cm, \n",
    "                text_auto=True,\n",
    "                aspect='auto',\n",
    "                color_continuous_scale = 'Blues',\n",
    "                labels = dict(x = \"Predicted Labels\",\n",
    "                              y= \"Actual Labels\"),\n",
    "                x = x,\n",
    "                y = y\n",
    "                )\n",
    "    if plot:\n",
    "        fig.show()\n",
    "\n",
    "    return cm, fig\n",
    "\n",
    "def plotly_output_hist(actual,\n",
    "                       prediction_probs,\n",
    "                       plot=False\n",
    "                      ):\n",
    "    hist_ = px.histogram(x = prediction_probs,\n",
    "                         color = actual,\n",
    "                         nbins=100,\n",
    "                         labels=dict(color='True Labels',\n",
    "                                     x = 'Prediction Probability'\n",
    "                                    )\n",
    "                        )\n",
    "    if plot:\n",
    "        hist_.show()\n",
    "        \n",
    "        \n",
    "    return hist_\n",
    "\n",
    "\n",
    "def plotly_precision_recall(actual,\n",
    "                            predictions_prob,\n",
    "                            plot=False\n",
    "                           ):\n",
    "    prec, recall, threshold = precision_recall_curve(actual, predictions_prob)\n",
    "\n",
    "    trace = go.Scatter(\n",
    "                        x=recall,\n",
    "                        y=prec,\n",
    "                        mode='lines',\n",
    "                        line=dict(color=telus_purple),\n",
    "                        fill='tozeroy',\n",
    "                        name='Precision-Recall curve'\n",
    "                    )\n",
    "    layout = go.Layout(\n",
    "                        title='Precision-Recall Curve',\n",
    "                        xaxis=dict(title='Recall'),\n",
    "                        yaxis=dict(title='Precision')\n",
    "                    )\n",
    "    fig = go.Figure(data=[trace], layout=layout)\n",
    "        \n",
    "    if plot:\n",
    "        fig.show()\n",
    "        \n",
    "    return fig\n",
    "\n",
    "def plotly_roc(actual,\n",
    "                predictions_prob,\n",
    "                plot=False\n",
    "               ):\n",
    "    auc_score = roc_auc_score(actual, predictions_prob)\n",
    "    fpr, tpr, thresholds  = roc_curve(actual, predictions_prob)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "                        'False Positive Rate': fpr,\n",
    "                        'True Positive Rate': tpr\n",
    "                      }, \n",
    "                        index=thresholds)\n",
    "    df.index.name = \"Thresholds\"\n",
    "    df.columns.name = \"Rate\"\n",
    "    df = df.loc[df.index <= 1]\n",
    "    fig_tpr_fpr = 0\n",
    "    \n",
    "    fig_tpr_fpr= px.line(\n",
    "                        df, \n",
    "                        title='TPR and FPR at every threshold',\n",
    "                    )\n",
    "\n",
    "    # ROC Curve with AUC\n",
    "    trace = go.Scatter(\n",
    "                x=fpr,\n",
    "                y=tpr,\n",
    "                mode='lines',\n",
    "                line=dict(color=telus_purple),\n",
    "                fill='tozeroy',\n",
    "                name='Precision-Recall curve'\n",
    "            )\n",
    "    layout = go.Layout(\n",
    "                        title=f'ROC Curve (AUC={auc_score:.4f})',\n",
    "                        xaxis=dict(title='False Positive Rate'),\n",
    "                        yaxis=dict(title='True Positive Rate')\n",
    "                    )\n",
    "    fig_roc = go.Figure(data=[trace], layout=layout)\n",
    "\n",
    "    fig_roc.add_shape(\n",
    "        type='line', line=dict(dash='dash'),\n",
    "        x0=0, x1=1, y0=0, y1=1\n",
    "    )\n",
    "    fig_roc.update_xaxes(constrain='domain')\n",
    "        \n",
    "    if plot:\n",
    "        fig_tpr_fpr.show()\n",
    "        fig_roc.show()\n",
    "        \n",
    "\n",
    "    return fig_tpr_fpr, fig_roc, df, auc_score\n",
    "\n",
    "def plotly_lift_curve(actual,\n",
    "                      predictions_prob,\n",
    "                      step=0.01,\n",
    "                      plot=False\n",
    "                     ):\n",
    "    #Define an auxiliar dataframe to plot the curve\n",
    "    aux_lift = pd.DataFrame()\n",
    "    #Create a real and predicted column for our new DataFrame and assign values\n",
    "    aux_lift['real'] = actual\n",
    "    aux_lift['predicted'] = predictions_prob\n",
    "    #Order the values for the predicted probability column:\n",
    "    aux_lift.sort_values('predicted',ascending=False,inplace=True)\n",
    "\n",
    "    #Create the values that will go into the X axis of our plot\n",
    "    x_val = np.arange(step,1+step,step)\n",
    "    #Calculate the ratio of ones in our data\n",
    "    ratio_ones = aux_lift['real'].sum() / len(aux_lift)\n",
    "    #Create an empty vector with the values that will go on the Y axis our our plot\n",
    "    y_v = []\n",
    "\n",
    "    #Calculate for each x value its correspondent y value\n",
    "    for x in x_val:\n",
    "        num_data = int(np.ceil(x*len(aux_lift))) #The ceil function returns the closest integer bigger than our number \n",
    "        data_here = aux_lift.iloc[:num_data,:]   # ie. np.ceil(1.4) = 2\n",
    "        ratio_ones_here = data_here['real'].sum()/len(data_here)\n",
    "        y_v.append(ratio_ones_here / ratio_ones)\n",
    "        \n",
    "    \n",
    "   \n",
    "    # Lift Curve \n",
    "    trace = go.Scatter(\n",
    "                x=x_val,\n",
    "                y=y_v,\n",
    "                mode='lines',\n",
    "                line=dict(color=telus_purple),\n",
    "\n",
    "                name='Lift Curve'\n",
    "            )\n",
    "    layout = go.Layout(\n",
    "                        title=f'Lift Curve',\n",
    "                        xaxis=dict(title='Proportion of Sample'),\n",
    "                        yaxis=dict(title='Lift')\n",
    "                    )\n",
    "    fig_lift = go.Figure(data=[trace], layout=layout)\n",
    "\n",
    "    fig_lift.add_shape(\n",
    "        type='line', line=dict(dash='dash'),\n",
    "        x0=0, x1=1, y0=1, y1=1\n",
    "    )\n",
    "    fig_lift.update_xaxes(constrain='domain')\n",
    "        \n",
    "    if plot:\n",
    "        fig_lift.show()\n",
    "    \n",
    "    return fig_lift\n",
    "\n",
    "def plotly_feature_importance(model,\n",
    "                              columns,\n",
    "                              plot=False):\n",
    "    coefficients  = pd.DataFrame(model.feature_importances_)\n",
    "    column_data   = pd.DataFrame(columns)\n",
    "    coef_sumry    = (pd.merge(coefficients,column_data,left_index= True,\n",
    "                              right_index= True, how = \"left\"))\n",
    "    \n",
    "    coef_sumry.columns = [\"coefficients\",\"features\"]\n",
    "    coef_sumry = coef_sumry.sort_values(by = \"coefficients\",ascending = False)\n",
    "    \n",
    "    fig_feats = 0\n",
    "    trace= go.Bar(y = coef_sumry[\"features\"].head(15).iloc[::-1],\n",
    "                  x = coef_sumry[\"coefficients\"].head(15).iloc[::-1],\n",
    "                  name = \"coefficients\",\n",
    "                  marker = dict(color = coef_sumry[\"coefficients\"],\n",
    "                                colorscale = \"Viridis\",\n",
    "                                line = dict(width = .6,color = \"black\")\n",
    "                               ),\n",
    "                  orientation='h'\n",
    "                 )\n",
    "    layout = go.Layout(\n",
    "                        title='Feature Importance',\n",
    "                        yaxis=dict(title='Features')\n",
    "\n",
    "                    )\n",
    "    fig_feats = go.Figure(data=[trace], layout=layout)\n",
    "        \n",
    "    if plot:\n",
    "        fig_feats.update_yaxes(automargin=True)\n",
    "        fig_feats.show()\n",
    "    return coef_sumry, fig_feats\n",
    "\n",
    "def plotly_model_report(model,\n",
    "                        actual,\n",
    "                        predicted,\n",
    "                        predictions_prob,\n",
    "                        bucket_name,\n",
    "                        show_report = True,\n",
    "                        columns=[],\n",
    "                        save_path = ''\n",
    "                       ):\n",
    "    print(model.__class__.__name__)\n",
    "    \n",
    "    metrics_df, fig_metrics = ploty_model_metrics(actual, \n",
    "                                              predicted, \n",
    "                                              plot=False)\n",
    "    cm, fig_cm = plotly_confusion_matrix(actual, \n",
    "                                        predicted, \n",
    "                                        axis_labels='',\n",
    "                                        plot=False)\n",
    "    fig_hist = plotly_output_hist(actual, \n",
    "                                  prediction_probs=predictions_prob,\n",
    "                                  plot=False)\n",
    "    fig_pr = plotly_precision_recall(actual,\n",
    "                                predictions_prob,\n",
    "                                plot=False\n",
    "                               )\n",
    "    fig_tpr_fpr, fig_roc, _, auc_score = plotly_roc(actual,\n",
    "                                predictions_prob,\n",
    "                                plot=False\n",
    "                               )\n",
    "    try:\n",
    "        coefs_df, fig_feats = plotly_feature_importance(model=model, \n",
    "                                                        columns = columns,\n",
    "                                                          plot=False)\n",
    "        coefs_df=coefs_df.to_dict()\n",
    "    except:\n",
    "        coefs_df = 0\n",
    "        pass\n",
    "    fig_lift = plotly_lift_curve(actual,\n",
    "                          predictions_prob,\n",
    "                          step=0.01,\n",
    "                          plot=False\n",
    "                         )\n",
    "    # Figure out how to put this into report on Monday -> Not Urgent\n",
    "    cr=classification_report(actual,predicted, output_dict=True)\n",
    "\n",
    "    # Generate dataframe with summary of results in one row\n",
    "    \n",
    "    results_cols = ['date', 'model_name', 'estimator_type', \n",
    "                    'confusion_matrix','classification_report', \n",
    "                    'auc_score', 'feature_importances']\n",
    "    results_list = [datetime.now().strftime(\"%Y-%m-%d\"), model.__class__.__name__,  model._estimator_type,\n",
    "                    cm, cr,\n",
    "                    auc_score, coefs_df\n",
    "                   ]\n",
    "\n",
    "    results_df_combined = pd.concat([pd.DataFrame([results_list], columns=results_cols),\n",
    "                                     metrics_df],\n",
    "                                   axis=1)\n",
    "\n",
    "    # Generate Plotly page report\n",
    "    \n",
    "    report_fig = make_subplots(rows=4, \n",
    "                            cols=2, \n",
    "                            print_grid=False, \n",
    "                            specs=[[{}, {}], \n",
    "                                 [{}, {}],\n",
    "                                 [{}, {}],\n",
    "                                 [{}, {}],\n",
    "                                 ],\n",
    "                            subplot_titles=('Confusion Matrix',\n",
    "                                        'Model Metrics',\n",
    "                                        'Probability Output Histogram',\n",
    "                                        'Precision - Recall curve',\n",
    "                                        'TPR & FPR Vs. Threshold',\n",
    "                                        f'ROC Curve: AUC Score {np.round(auc_score, 3)}',                                        \n",
    "                                        'Feature importance',\n",
    "                                        'Lift Curve'\n",
    "                                        )\n",
    "                            )        \n",
    "\n",
    "    report_fig.append_trace(fig_cm.data[0],1,1)\n",
    "    report_fig.update_coloraxes(showscale=False)\n",
    "    report_fig.append_trace(fig_metrics.data[0],1,2)\n",
    "\n",
    "    report_fig.append_trace(fig_hist.data[0],2,1)\n",
    "    report_fig.append_trace(fig_hist.data[1],2,1)\n",
    "    report_fig.append_trace(fig_pr.data[0],2,2)\n",
    "\n",
    "    report_fig.append_trace(fig_tpr_fpr.data[0],3,1)\n",
    "    report_fig.append_trace(fig_tpr_fpr.data[1],3,1)\n",
    "    report_fig.append_trace(fig_roc.data[0],3,2)\n",
    "    try:\n",
    "        report_fig.append_trace(fig_feats.data[0],4,1)\n",
    "    except:\n",
    "        pass\n",
    "    report_fig.append_trace(fig_lift.data[0],4,2)    \n",
    "    title_str = f\"{model.__class__.__name__} : Model performance report\"\n",
    "    report_fig['layout'].update(title = f'<b>{title_str}</b><br>',\n",
    "                    autosize = True, height = 1500,width = 1200,\n",
    "                    plot_bgcolor = 'rgba(240,240,240, 0.95)',\n",
    "                    paper_bgcolor = 'rgba(240,240,240, 0.95)',\n",
    "                    margin = dict(b = 195))\n",
    "\n",
    "    report_fig[\"layout\"][\"xaxis1\"].update(dict(title = \"Predicted Labels\"))\n",
    "    report_fig[\"layout\"][\"yaxis1\"].update(dict(title = \"Actual Labels\"))\n",
    "\n",
    "    report_fig[\"layout\"][\"xaxis3\"].update(dict(title = \"Prediction Probabilities\"))\n",
    "    report_fig[\"layout\"][\"yaxis3\"].update(dict(title = \"Count\"))\n",
    "\n",
    "    report_fig[\"layout\"][\"xaxis4\"].update(dict(title = \"Recall\"))\n",
    "    report_fig[\"layout\"][\"yaxis4\"].update(dict(title = \"Precision\"))\n",
    "\n",
    "    report_fig[\"layout\"][\"xaxis5\"].update(dict(title = \"Thresholds\"))\n",
    "    report_fig[\"layout\"][\"yaxis5\"].update(dict(title = \"Rate\"))\n",
    "\n",
    "    report_fig[\"layout\"][\"xaxis6\"].update(dict(title = \"False Positive Rate\"))\n",
    "    report_fig[\"layout\"][\"yaxis6\"].update(dict(title = \"True Positive Rate\"))\n",
    "\n",
    "    report_fig[\"layout\"][\"yaxis7\"].update(dict(title = \"Features\"))\n",
    "\n",
    "    report_fig[\"layout\"][\"xaxis8\"].update(dict(title = \"Proportion of Sample\"))\n",
    "    report_fig[\"layout\"][\"yaxis8\"].update(dict(title = \"Lift\"))             \n",
    "\n",
    "    if show_report:\n",
    "        report_fig.show()       \n",
    "\n",
    "    #Save html report\n",
    "    todays_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    report_fig.write_html(f\"{save_path}{todays_date}_{model.__class__.__name__}.html\")\n",
    "    bucket = storage.Client().bucket(bucket_name)\n",
    "    filename = f\"{todays_date}_{model.__class__.__name__}.html\"\n",
    "    blob = bucket.blob(f\"{save_path}{filename}\")\n",
    "    blob.upload_from_filename(f\"{save_path}{todays_date}_{model.__class__.__name__}.html\")\n",
    "    print(f\"{filename} sucessfully uploaded to GCS bucket!\")\n",
    "    # Return dataframe with metrics\n",
    "    return results_df_combined,report_fig\n",
    "\n",
    "    \n",
    "\n",
    "def evaluate_and_save_models(models,bucket_name, save_path, test_df, actual_label_str, columns, save_columns=False, show_report=False):\n",
    "    # define_the_bucket\n",
    "    bucket = storage.Client().bucket(bucket_name)\n",
    "    date=datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    model_test_set_reports = []\n",
    "    model_to_report_map = {}\n",
    "    \n",
    "    # If single model passed through\n",
    "    if type(models) != list:\n",
    "        models = [models]\n",
    "        \n",
    "    create_folder_if_not_exists(save_path)\n",
    "    \n",
    "    if save_columns:\n",
    "        with open(f\"{save_path}{date}_columns.pkl\" , \"wb\") as f:\n",
    "            pickle.dump(columns, f)\n",
    "        print(f\"Columns saved as {save_path}{date}_columns.pkl !\")\n",
    "        filename = f\"{date}_columns.pkl\"\n",
    "        blob = bucket.blob(f\"{save_path}{filename}\")\n",
    "        blob.upload_from_filename(f\"{save_path}{date}_columns.pkl\")\n",
    "        print(f\"{save_path}/{date}_columns.pkl sucessfully uploaded to GCS bucket!\")\n",
    "    \n",
    "     # Add code to set model to a list if only 1 model passed\n",
    "    for i in range(len(models)):\n",
    "        print(models[i])\n",
    "        # Save model\n",
    "        \n",
    "        # Add code to create new folder if it does not exist\n",
    "        model_file_name = '{save_path}{model_type}_{date}'.format(save_path = save_path,                                                                     model_type=models[i].__class__.__name__,                                                                    date=datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "        save_model(models[i],model_file_name )\n",
    "        filename = '{model_type}_{date}.pkl'.format(model_type=models[i].__class__.__name__,                                        date=datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "        blob = bucket.blob(f\"{save_path}{filename}\")\n",
    "        blob.upload_from_filename(f\"{model_file_name}.pkl\")\n",
    "        print(f\"{filename} sucessfully uploaded to GCS bucket!\")\n",
    "        # joblib.dump(models[i], '{save_path}{model_type}_{date}.joblib'.format(\n",
    "        #                                                                     save_path = save_path,\n",
    "        #                                                                     model_type=models[i].__class__.__name__,\n",
    "        #                                                                     date=datetime.now().strftime(\"%Y-%m-%d\")))\n",
    "\n",
    "        # Get predictions on test set for model\n",
    "        predictions = predict_model(models[i], data=test_df)\n",
    "        # Normalize prediction probabilities \n",
    "        predictions['Score_Normalized']=predictions['Score']\n",
    "        predictions.loc[predictions['Label'] == 0,'Score_Normalized'] = 1 - predictions['Score']\n",
    "        predictions_prob = predictions[\"Score_Normalized\"].astype(float)\n",
    "\n",
    "        actual = predictions[actual_label_str].astype(int)\n",
    "        predicted = predictions[\"Label\"].astype(int)\n",
    "        \n",
    "        # Pass data to generate plotly_report\n",
    "        report_df,report_fig = plotly_model_report(model=models[i],\n",
    "                                        actual=actual,\n",
    "                                        predicted=predicted,\n",
    "                                        predictions_prob=predictions_prob,\n",
    "                                        bucket_name  = bucket_name  ,\n",
    "                                        show_report = show_report,\n",
    "                                        columns = columns,\n",
    "                                        save_path = save_path      \n",
    "                                       )\n",
    "        todays_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        model_to_report_map[models[i].__class__.__name__ ]=report_fig\n",
    "\n",
    "    # report_fig.write_html(f\"{save_path}{todays_date}_{model.__class__.__name__}.html\")\n",
    "        model_test_set_reports.append(report_df)\n",
    "        \n",
    "    model_test_set_reports_concat = pd.concat(model_test_set_reports)\n",
    "    model_test_set_reports_concat.to_csv(f\"{save_path}{date}_model_reports.csv\", index=False)\n",
    "    \n",
    "    filename = f\"{date}_model_reports.csv\"\n",
    "    blob = bucket.blob(f\"{save_path}{filename}\")\n",
    "    blob.upload_from_filename(f\"{save_path}{date}_model_reports.csv\")\n",
    "    print(f\"{filename} sucessfully uploaded to GCS bucket!\")\n",
    "    return model_test_set_reports_concat,model_to_report_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca3de2a-f018-46f1-8b5f-c64aefcdd3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from pycaret.classification import * \n",
    "\n",
    "def plot_lift_curve(y_val, y_pred,  step=0.01):\n",
    "\n",
    "    #Define an auxiliar dataframe to plot the curve\n",
    "    aux_lift = pd.DataFrame()\n",
    "    #Create a real and predicted column for our new DataFrame and assign values\n",
    "    aux_lift['real'] = y_val\n",
    "    aux_lift['predicted'] = y_pred\n",
    "    #Order the values for the predicted probability column:\n",
    "    aux_lift.sort_values('predicted',ascending=False,inplace=True)\n",
    "\n",
    "    #Create the values that will go into the X axis of our plot\n",
    "    x_val = np.arange(step,1+step,step)\n",
    "    #Calculate the ratio of ones in our data\n",
    "    ratio_ones = aux_lift['real'].sum() / len(aux_lift)\n",
    "    #Create an empty vector with the values that will go on the Y axis our our plot\n",
    "    y_v = []\n",
    "\n",
    "    #Calculate for each x value its correspondent y value\n",
    "    for x in x_val:\n",
    "        num_data = int(np.ceil(x*len(aux_lift))) #The ceil function returns the closest integer bigger than our number \n",
    "        data_here = aux_lift.iloc[:num_data,:]   # ie. np.ceil(1.4) = 2\n",
    "        ratio_ones_here = data_here['real'].sum()/len(data_here)\n",
    "        y_v.append(ratio_ones_here / ratio_ones)\n",
    "\n",
    "    #Plot the figure\n",
    "    \n",
    "    fig, axis = plt.subplots()\n",
    "    fig.figsize = (40,40)\n",
    "    axis.plot(x_val, y_v, 'g-', linewidth = 3, markersize = 5)\n",
    "    axis.plot(x_val, np.ones(len(x_val)), 'k-')\n",
    "    axis.set_xlabel('Proportion of sample')\n",
    "    axis.set_ylabel('Lift')\n",
    "    plt.title('Lift Curve')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def prediction_metrics_plots(df, model, display_output=True, actual_label_str=''):\n",
    "    print(model.__class__.__name__)\n",
    "    # Get predictions using pycaret predict_model function\n",
    "    \n",
    "    predictions = predict_model(model, data=df)\n",
    "    \n",
    "    # Normalize prediction probabilities \n",
    "    predictions['Score_Normalized']=predictions['Score']\n",
    "    predictions.loc[predictions['Label'] == 0,'Score_Normalized'] = 1 - predictions['Score']\n",
    "    predictions_prob = predictions[\"Score_Normalized\"].astype(float)\n",
    "    \n",
    "    actual = predictions[actual_label_str].astype(float)\n",
    "    predicted = predictions[\"Label\"].astype(float)\n",
    "    \n",
    "    # Precision - Recall\n",
    "    prec, recall, threshold = precision_recall_curve(actual, predictions_prob)\n",
    "    baseline = len(actual[actual==1]) / len(actual)\n",
    "    \n",
    "    if display_output:\n",
    "        plt.clf()\n",
    "        print(\"Precision Recall Curve\")\n",
    "        plt.plot([0, 1], [baseline, baseline], linestyle='--', label='baseline')\n",
    "        plt.plot(recall, prec, marker='.', label='Model')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    # ROC Curve\n",
    "    \n",
    "    baseline_prob = [0 for _ in range(len(actual))]\n",
    "    baseline_fpr, baseline_tpr, _ = roc_curve(actual, baseline_prob)\n",
    "    \n",
    "    auc_score = roc_auc_score(actual, predictions_prob)\n",
    "    fpr, tpr, _ = roc_curve(actual, predictions_prob)\n",
    "    \n",
    "    if display_output:\n",
    "        print(\"ROC Curve\")\n",
    "        plt.plot(baseline_fpr, baseline_tpr, linestyle='--', label='Baseline')\n",
    "        plt.plot(fpr, tpr, marker='.', label='Model ==>  AUC : {} '.format(auc_score))\n",
    "        # axis labels\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    # Lift Curve\n",
    "    if display_output:\n",
    "        print(\"lift curve\")\n",
    "        plot_lift_curve(actual,predictions_prob)\n",
    "        \n",
    "    # Other metrics\n",
    "    f1_score_ = f1_score(actual, predicted)\n",
    "    recall_score_ = recall_score(actual, predicted)\n",
    "    acc_score_ = accuracy_score(actual, predicted)\n",
    "    pr_score_ = precision_score(actual, predicted)\n",
    "    \n",
    "    # Classification Report\n",
    "    cr=classification_report(actual,predicted, output_dict=True)\n",
    "    if display_output:\n",
    "        display(pd.DataFrame(cr))\n",
    "        \n",
    "    # Confusion Matrix\n",
    "    cm=confusion_matrix(actual, predicted)\n",
    "    if display_output:\n",
    "        print(cm)\n",
    "        ax= plt.subplot()\n",
    "        sns.heatmap(cm, annot=True, fmt='g', ax=ax);  \n",
    "        ax.set_xlabel('Predicted labels');\n",
    "        ax.set_ylabel('True labels'); \n",
    "        ax.set_title('Confusion Matrix'); \n",
    "        #ax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);\n",
    "    \n",
    "\n",
    "\n",
    "    results_cols = ['date', 'model_name', 'estimator_type','actual_label', 'predicted_label',\n",
    "                    'score_normalized', 'auc', 'f1', 'recall', 'precision', 'accuracy',\n",
    "                    'classification_report', 'confusion_matrix']\n",
    "    \n",
    "    results_list = [datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "                    model.__class__.__name__, \n",
    "                    model._estimator_type, \n",
    "                    actual, \n",
    "                    predicted,\n",
    "                    predictions_prob, \n",
    "                    auc_score, \n",
    "                    f1_score_, \n",
    "                    recall_score_,\n",
    "                    pr_score_, \n",
    "                    acc_score_, \n",
    "                    cr, \n",
    "                    cm]\n",
    "    \n",
    "    results_df = pd.DataFrame([results_list], columns=results_cols)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558ff533-70c8-4421-813e-c567d6bc496b",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f70d721-76e7-4c2e-8153-88b5bbe8ee47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tag cell with parameters\n",
    "PROJECT_ID =  ''\n",
    "BUCKET_NAME=''\n",
    "DATASET_ID = ''\n",
    "RESOURCE_BUCKET = ''\n",
    "FILE_BUCKET = ''\n",
    "REGION = ''\n",
    "MODEL_ID = '5090'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7035200a-de6b-412c-8406-64854b639cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag cell with parameters\n",
    "PROJECT_ID =  'divg-josh-pr-d1cc3a'\n",
    "BUCKET_NAME='divg-josh-pr-d1cc3a-default'\n",
    "DATASET_ID = 'call_to_retention_dataset'\n",
    "RESOURCE_BUCKET = 'divg-josh-pr-d1cc3a-default'\n",
    "FILE_BUCKET = 'divg-josh-pr-d1cc3a-default'\n",
    "MODEL_ID = '5090'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eeed21-e12b-45ee-b1e4-d0c8f3843533",
   "metadata": {},
   "source": [
    "### Service Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65d1668-e1d8-4f58-958f-edc44d06a6b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SERVICE_TYPE = 'call_to_retention'\n",
    "SERVICE_TYPE_NAME = 'call-to-retention'\n",
    "TABLE_ID = 'bq_call_to_retention_targets'\n",
    "REGION = \"northamerica-northeast1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2229e5d1-1362-40ed-aa7d-cb22e41e4960",
   "metadata": {},
   "source": [
    "### Pulumi Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d628b-e759-4c9c-aab5-568c54721aaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "STACK_NAME = 'call_to_retention'\n",
    "TRAIN_PIPELINE_NAME_PATH = 'train_pipeline'\n",
    "PREDICT_PIPELINE_NAME_PATH = 'predict_pipeline'\n",
    "TRAIN_PIPELINE_NAME = 'call-to-retention-train-pipeline' # Same name as pulumi.yaml\n",
    "PREDICT_PIPELINE_NAME = 'call-to-retention-predict-pipeline' # Same name as pulumi.yaml\n",
    "TRAIN_PIPELINE_DESCRIPTION = 'call-to-retention-train-pipeline'\n",
    "PREDICT_PIPELINE_DESCRIPTION = 'call-to-retention-predict-pipeline'\n",
    "REGION = \"northamerica-northeast1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85558b11-be96-49ea-91c5-d17b15c00bf1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Query + Pre-Processing Component Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370852f7-2e23-4ac3-8cd8-b40c39be85ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_QUERIES_PATH = f\"{STACK_NAME}/{TRAIN_PIPELINE_NAME_PATH}/queries/\" \n",
    "TRAIN_UTILS_FILE_PATH = f\"{STACK_NAME}/{TRAIN_PIPELINE_NAME_PATH}/utils\" \n",
    "UTILS_FILENAME = 'utils.py'\n",
    "\n",
    "PROCESSED_SERVING_DATA_TABLENAME = 'processed_serving_data'\n",
    "INPUT_SERVING_DATA_TABLENAME = 'input_serving_data'\n",
    "\n",
    "QUERY_DATE = (date.today() - relativedelta(days=1)).strftime('%Y-%m-%d')\n",
    "TARGET_TABLE_REF = '{}.{}.{}'.format(PROJECT_ID, DATASET_ID, TABLE_ID)\n",
    "\n",
    "QUERIES_PATH = 'call_to_retention/queries/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc0bd98-1d58-491e-bc5e-bbac378c7bbf",
   "metadata": {},
   "source": [
    "### Import Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f442904b-a40a-4ba8-b237-e9894ffd4f26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download required component files to local\n",
    "prefix = f'{STACK_NAME}/{TRAIN_PIPELINE_NAME_PATH}/components/'\n",
    "dl_dir = 'components/'\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(RESOURCE_BUCKET)\n",
    "blobs = bucket.list_blobs(prefix=prefix)  # Get list of files\n",
    "for blob in blobs: # download each file that starts with \"prefix\" into \"dl_dir\"\n",
    "    if blob.name.endswith(\"/\"):\n",
    "        continue\n",
    "    file_split = blob.name.split(prefix)\n",
    "    file_path = f\"{dl_dir}{file_split[-1]}\"\n",
    "    directory = \"/\".join(file_path.split(\"/\")[0:-1])\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    blob.download_to_filename(file_path) \n",
    "\n",
    "# import main pipeline components\n",
    "import components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c315d454-416f-4eed-b12d-1acd5aed1207",
   "metadata": {},
   "source": [
    "### Date Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfad310-a482-4285-aa31-6d9113008be6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scoringDate = date(2022, 9, 1)  # date.today() - relativedelta(days=2)- relativedelta(months=30)\n",
    "valScoringDate = date(2022, 10, 1)  # scoringDate - relativedelta(days=2)\n",
    "\n",
    "# training dates\n",
    "SCORE_DATE = scoringDate.strftime('%Y%m%d')  # date.today().strftime('%Y%m%d')\n",
    "SCORE_DATE_DASH = scoringDate.strftime('%Y-%m-%d')\n",
    "SCORE_DATE_MINUS_6_MOS_DASH = ((scoringDate - relativedelta(months=6)).replace(day=1)).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_LAST_MONTH_START_DASH = (scoringDate.replace(day=1) - timedelta(days=1)).replace(day=1).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_LAST_MONTH_END_DASH = ((scoringDate.replace(day=1)) - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "PROMO_EXPIRY_START = (scoringDate.replace(day=1) + relativedelta(months=3)).replace(day=1).strftime('%Y-%m-%d')\n",
    "PROMO_EXPIRY_END = (scoringDate.replace(day=1) + relativedelta(months=4)).replace(day=1).strftime('%Y-%m-%d')\n",
    "\n",
    "# validation dates\n",
    "SCORE_DATE_VAL = valScoringDate.strftime('%Y%m%d')\n",
    "SCORE_DATE_VAL_DASH = valScoringDate.strftime('%Y-%m-%d')\n",
    "SCORE_DATE_VAL_MINUS_6_MOS_DASH = ((valScoringDate - relativedelta(months=6)).replace(day=1)).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_VAL_LAST_MONTH_START_DASH = (valScoringDate.replace(day=1) - timedelta(days=1)).replace(day=1).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_VAL_LAST_MONTH_END_DASH = ((valScoringDate.replace(day=1)) - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "PROMO_EXPIRY_START_VAL = (valScoringDate.replace(day=1) + relativedelta(months=3)).replace(day=1).strftime('%Y-%m-%d')\n",
    "PROMO_EXPIRY_END_VAL = (valScoringDate.replace(day=1) + relativedelta(months=4)).replace(day=1).strftime('%Y-%m-%d')\n",
    "\n",
    "SCORE_DATE_DELTA = 0\n",
    "SCORE_DATE_VAL_DELTA = 0\n",
    "TICKET_DATE_WINDOW = 30  # Days of ticket data to be queried\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0d12b8-ccad-4c70-8c12-d8837797225c",
   "metadata": {},
   "source": [
    "### bq_create_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324ba175-f34b-4e02-975e-c39fe1084e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "# from kfp.v2.dsl import (Model, Input, component)\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,HTML,\n",
    "                        OutputPath, ClassificationMetrics, Metrics, component)\n",
    "from typing import NamedTuple\n",
    "# Create Training Dataset for training pipeline\n",
    "@component(\n",
    "    base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/wb-platform/pipelines/kubeflow-pycaret:latest\",\n",
    "    output_component_file=\"bq_create_dataset.yaml\",\n",
    ")\n",
    "def bq_create_dataset(score_date: str,\n",
    "                      score_date_delta: int,\n",
    "                      project_id: str,\n",
    "                      dataset_id: str,\n",
    "                      region: str,\n",
    "                      promo_expiry_start: str, \n",
    "                      promo_expiry_end: str, \n",
    "                      v_start_date: str,\n",
    "                      v_end_date: str) -> NamedTuple(\"output\", [(\"col_list\", list)]):\n",
    " \n",
    "    from google.cloud import bigquery\n",
    "    import logging \n",
    "    from datetime import datetime\n",
    "    # For wb\n",
    "    # import google.oauth2.credentials\n",
    "    # CREDENTIALS = google.oauth2.credentials.Credentials(token)\n",
    "    \n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    client = get_gcp_bqclient(project_id)\n",
    "    # client = bigquery.Client(project=project_id, location=region)\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    \n",
    "    # Change dataset / table + sp table name to version in bi-layer\n",
    "    query =\\\n",
    "        f'''\n",
    "            DECLARE score_date DATE DEFAULT \"{score_date}\";\n",
    "            DECLARE promo_expiry_start DATE DEFAULT \"{promo_expiry_start}\";\n",
    "            DECLARE promo_expiry_end DATE DEFAULT \"{promo_expiry_end}\";\n",
    "            DECLARE start_date DATE DEFAULT \"{v_start_date}\";\n",
    "            DECLARE end_date DATE DEFAULT \"{v_end_date}\";\n",
    "        \n",
    "            -- Change dataset / sp name to the version in the bi_layer\n",
    "            CALL {dataset_id}.bq_sp_ctr_pipeline_dataset(score_date, promo_expiry_start, promo_expiry_end, start_date, end_date);\n",
    "\n",
    "            SELECT\n",
    "                *\n",
    "            FROM {dataset_id}.INFORMATION_SCHEMA.PARTITIONS\n",
    "            WHERE table_name='bq_ctr_pipeline_dataset'\n",
    "            \n",
    "        '''\n",
    "    \n",
    "    df = client.query(query, job_config=job_config).to_dataframe()\n",
    "    logging.info(df.to_string())\n",
    "    \n",
    "    logging.info(f\"Loaded {df.total_rows[0]} rows into \\\n",
    "             {df.table_catalog[0]}.{df.table_schema[0]}.{df.table_name[0]} on \\\n",
    "             {datetime.strftime((df.last_modified_time[0]), '%Y-%m-%d %H:%M:%S') } !\")\n",
    "    \n",
    "    ######################################## Save column list_##########################\n",
    "    query =\\\n",
    "        f'''\n",
    "           SELECT\n",
    "                *\n",
    "            FROM {dataset_id}.bq_ctr_pipeline_dataset\n",
    "\n",
    "        '''\n",
    "    \n",
    "    df = client.query(query, job_config=job_config).to_dataframe()\n",
    "    \n",
    "    col_list = list([col for col in df.columns])\n",
    "    return (col_list,)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281c6676-70b0-41a6-b860-aa6d95fec0da",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8623f9-e2fd-406a-b0a7-2ea7c42af66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "        pipeline_dataset: str, \n",
    "        save_data_path: str,\n",
    "        project_id: str,\n",
    "        dataset_id: str\n",
    "):\n",
    "    from google.cloud import bigquery\n",
    "    import pandas as pd\n",
    "    import gc\n",
    "    import time\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    client = get_gcp_bqclient(project_id)\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "\n",
    "    # pipeline_dataset \n",
    "    pipeline_dataset_name = f\"{project_id}.{dataset_id}.{pipeline_dataset}\" \n",
    "    build_df_pipeline_dataset = f'SELECT * FROM `{pipeline_dataset_name}`'\n",
    "    df_pipeline_dataset = client.query(build_df_pipeline_dataset).to_dataframe()\n",
    "    df_pipeline_dataset = df_pipeline_dataset.set_index('ban') \n",
    "\n",
    "    # demo columns\n",
    "    df_pipeline_dataset['demo_urban_flag'] = df_pipeline_dataset.demo_sgname.str.lower().str.contains('urban').fillna(0).astype(int)\n",
    "    df_pipeline_dataset['demo_rural_flag'] = df_pipeline_dataset.demo_sgname.str.lower().str.contains('rural').fillna(0).astype(int)\n",
    "    df_pipeline_dataset['demo_family_flag'] = df_pipeline_dataset.demo_lsname.str.lower().str.contains('families').fillna(0).astype(int)\n",
    "\n",
    "    df_income_dummies = pd.get_dummies(df_pipeline_dataset[['demo_lsname']]) \n",
    "    df_income_dummies.columns = df_income_dummies.columns.str.replace('&', 'and')\n",
    "    df_income_dummies.columns = df_income_dummies.columns.str.replace(' ', '_')\n",
    "\n",
    "    df_pipeline_dataset.drop(columns=['demo_sgname', 'demo_lsname'], axis=1, inplace=True)\n",
    "\n",
    "    df_pipeline_dataset = df_pipeline_dataset.join(df_income_dummies)\n",
    "\n",
    "    df_join = df_pipeline_dataset.copy()\n",
    "\n",
    "    #column name clean-up\n",
    "    df_join.columns = df_join.columns.str.replace(' ', '_')\n",
    "    df_join.columns = df_join.columns.str.replace('-', '_')\n",
    "\n",
    "    df_join.head()\n",
    "\n",
    "    #df_final\n",
    "    df_final = df_join.copy()\n",
    "    del df_join\n",
    "    gc.collect()\n",
    "    print('......df_final done')\n",
    "\n",
    "    for f in df_final.columns:\n",
    "        df_final[f] = list(df_final[f])\n",
    "\n",
    "    df_final.to_csv(save_data_path, index=True, compression='gzip') \n",
    "    del df_final\n",
    "    gc.collect()\n",
    "    print(f'......csv saved in {save_data_path}')\n",
    "    time.sleep(120)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330396ff-dfe9-4c23-9d67-922e9a190bdc",
   "metadata": {},
   "source": [
    "### Train and Save Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f33719-127b-45c7-bccd-dcd3970b003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_model(\n",
    "            file_bucket: str,\n",
    "            service_type: str,\n",
    "            score_date_dash: str,\n",
    "            score_date_val_dash: str,\n",
    "            project_id: str,\n",
    "            dataset_id: str\n",
    "):\n",
    "\n",
    "    import gc\n",
    "    import time\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    from google.cloud import storage\n",
    "    from google.cloud import bigquery\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    def get_lift(prob, y_test, q):\n",
    "        result = pd.DataFrame(columns=['Prob', 'Call_To_Retention'])\n",
    "        result['Prob'] = prob\n",
    "        result['Call_To_Retention'] = y_test\n",
    "        result['Decile'] = pd.qcut(result['Prob'], q, labels=[i for i in range(q, 0, -1)])\n",
    "        add = pd.DataFrame(result.groupby('Decile')['Call_To_Retention'].mean()).reset_index()\n",
    "        add.columns = ['Decile', 'avg_real_call_to_retention_rate']\n",
    "        result = result.merge(add, on='Decile', how='left')\n",
    "        result.sort_values('Decile', ascending=True, inplace=True)\n",
    "        lg = pd.DataFrame(result.groupby('Decile')['Prob'].mean()).reset_index()\n",
    "        lg.columns = ['Decile', 'avg_model_pred_call_to_retention_rate']\n",
    "        lg.sort_values('Decile', ascending=False, inplace=True)\n",
    "        lg['avg_call_to_retention_rate_total'] = result['Call_To_Retention'].mean()\n",
    "        lg = lg.merge(add, on='Decile', how='left')\n",
    "        lg['lift'] = lg['avg_real_call_to_retention_rate'] / lg['avg_call_to_retention_rate_total']\n",
    "\n",
    "        return lg\n",
    "\n",
    "    df_train = pd.read_csv('gs://{}/{}_train.csv.gz'.format(file_bucket, service_type),\n",
    "                           compression='gzip')  \n",
    "    df_test = pd.read_csv('gs://{}/{}_validation.csv.gz'.format(file_bucket, service_type),  \n",
    "                          compression='gzip')\n",
    "\n",
    "    #set up df_train\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    sql_train = ''' SELECT * FROM `{}.{}.bq_call_to_retention_targets` '''.format(project_id, dataset_id) \n",
    "    df_target_train = client.query(sql_train).to_dataframe()\n",
    "    df_target_train = df_target_train.loc[\n",
    "        df_target_train['YEAR_MONTH'] == '-'.join(score_date_dash.split('-')[:2])]  # score_date_dash = '2022-08-31'\n",
    "    df_target_train['ban'] = df_target_train['ban'].astype('int64')\n",
    "    df_target_train = df_target_train.groupby('ban').tail(1)\n",
    "    df_train = df_train.merge(df_target_train[['ban', 'target_ind']], on='ban', how='left')\n",
    "    df_train.rename(columns={'target_ind': 'target'}, inplace=True)\n",
    "    df_train.dropna(subset=['target'], inplace=True)\n",
    "    df_train['target'] = df_train['target'].astype(int)\n",
    "    print(df_train.shape)\n",
    "\n",
    "    #set up df_test\n",
    "    sql_test = ''' SELECT * FROM `{}.{}.bq_call_to_retention_targets` '''.format(project_id, dataset_id) \n",
    "    df_target_test = client.query(sql_test).to_dataframe()\n",
    "    df_target_test = df_target_test.loc[\n",
    "        df_target_test['YEAR_MONTH'] == '-'.join(score_date_val_dash.split('-')[:2])]  # score_date_dash = '2022-09-30'\n",
    "    df_target_test['ban'] = df_target_test['ban'].astype('int64')\n",
    "    df_target_test = df_target_test.groupby('ban').tail(1)\n",
    "    df_test = df_test.merge(df_target_test[['ban', 'target_ind']], on='ban', how='left')\n",
    "    df_test.rename(columns={'target_ind': 'target'}, inplace=True)\n",
    "    df_test.dropna(subset=['target'], inplace=True)\n",
    "    df_test['target'] = df_test['target'].astype(int)\n",
    "    print(df_test.shape)\n",
    "\n",
    "    #set up features (list)\n",
    "    cols_1 = df_train.columns.values\n",
    "    cols_2 = df_test.columns.values\n",
    "    cols = set(cols_1).intersection(set(cols_2))\n",
    "    features = [f for f in cols if f not in ['ban', 'target']]\n",
    "\n",
    "    #train test split\n",
    "    df_train, df_val = train_test_split(df_train, shuffle=True, test_size=0.2, random_state=42,\n",
    "                                        stratify=df_train['target']\n",
    "                                        )\n",
    "\n",
    "    ban_train = df_train['ban']\n",
    "    X_train = df_train[features]\n",
    "    y_train = np.squeeze(df_train['target'].values)\n",
    "\n",
    "    ban_val = df_val['ban']\n",
    "    X_val = df_val[features]\n",
    "    y_val = np.squeeze(df_val['target'].values)\n",
    "\n",
    "    ban_test = df_test['ban']\n",
    "    X_test = df_test[features]\n",
    "    y_test = np.squeeze(df_test['target'].values)\n",
    "\n",
    "    del df_train, df_val, df_test\n",
    "    gc.collect()\n",
    "\n",
    "    # build model and fit in training data\n",
    "    import xgboost as xgb\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        learning_rate=0.01,\n",
    "        n_estimators=100,\n",
    "        max_depth=8,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective='binary:logistic',\n",
    "        nthread=4,\n",
    "        scale_pos_weight=1\n",
    "        # seed=27\n",
    "    )\n",
    "\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    print('xgb training done')\n",
    "\n",
    "    from sklearn.preprocessing import normalize\n",
    "\n",
    "#     #predictions on X_val\n",
    "#     y_pred = xgb_model.predict_proba(X_val, ntree_limit=xgb_model.best_iteration)[:, 1]\n",
    "#     y_pred_label = (y_pred > 0.5).astype(int)\n",
    "#     auc = roc_auc_score(y_val, y_pred_label)\n",
    "#     metrics.log_metric(\"AUC\", auc)\n",
    "\n",
    "    pred_prb = xgb_model.predict_proba(X_test, ntree_limit=xgb_model.best_iteration)[:, 1]\n",
    "    lg = get_lift(pred_prb, y_test, 10)\n",
    "\n",
    "    # save the model in GCS\n",
    "    from datetime import datetime\n",
    "    models_dict = {}\n",
    "    create_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    models_dict['create_time'] = create_time\n",
    "    models_dict['model'] = xgb_model\n",
    "    models_dict['features'] = features\n",
    "    lg.to_csv('gs://{}/lift_on_scoring_data_{}.csv'.format(file_bucket, create_time, index=False))\n",
    "\n",
    "    with open('model_dict.pkl', 'wb') as handle:\n",
    "        pickle.dump(models_dict, handle)\n",
    "    handle.close()\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(file_bucket)\n",
    "\n",
    "    MODEL_PATH = '{}_xgb_models/'.format(service_type)\n",
    "    blob = bucket.blob(MODEL_PATH)\n",
    "    if not blob.exists(storage_client):\n",
    "        blob.upload_from_string('')\n",
    "\n",
    "    model_name_onbkt = '{}{}_models_xgb_{}'.format(MODEL_PATH, service_type, models_dict['create_time'])\n",
    "    blob = bucket.blob(model_name_onbkt)\n",
    "    blob.upload_from_filename('model_dict.pkl')\n",
    "\n",
    "    print(f\"....model loaded to GCS done at {str(create_time)}\")\n",
    "\n",
    "    time.sleep(120)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815bf58f-4aef-4cb2-9dd4-7684e3d34a29",
   "metadata": {},
   "source": [
    "### pycaret_automl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038e8fcd-7a34-43bf-a338-1d251202b149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "project_id = PROJECT_ID\n",
    "dataset_id = DATASET_ID\n",
    "region = REGION\n",
    "service_type = SERVICE_TYPE\n",
    "bucket_name = BUCKET_NAME\n",
    "file_bucket = FILE_BUCKET\n",
    "score_date_dash= SCORE_DATE_DASH\n",
    "score_date_val_dash= SCORE_DATE_VAL_DASH\n",
    "\n",
    "from pycaret.classification import setup,create_model,tune_model, predict_model,get_config,compare_models,save_model,tune_model\n",
    "\n",
    "df_train = pd.read_csv('gs://{}/{}_train.csv.gz'.format(file_bucket, service_type),\n",
    "                       compression='gzip')  \n",
    "df_test = pd.read_csv('gs://{}/{}_validation.csv.gz'.format(file_bucket, service_type),  \n",
    "                      compression='gzip')\n",
    "\n",
    "def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "    token = os.popen('gcloud auth print-access-token').read()\n",
    "    token = re.sub(f'\\n$', '', token)\n",
    "    credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "    bq_client = bigquery.Client(project=project_id)\n",
    "    if use_local_credential:\n",
    "        bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "    return bq_client\n",
    "\n",
    "client = get_gcp_bqclient(project_id)\n",
    "\n",
    "#set up df_train\n",
    "sql_train = ''' SELECT * FROM `{}.{}.bq_call_to_retention_targets` '''.format(project_id, dataset_id) \n",
    "df_target_train = client.query(sql_train).to_dataframe()\n",
    "df_target_train = df_target_train.loc[\n",
    "    df_target_train['YEAR_MONTH'] == '-'.join(score_date_dash.split('-')[:2])]  # score_date_dash = '2022-08-31'\n",
    "df_target_train['ban'] = df_target_train['ban'].astype('int64')\n",
    "df_target_train = df_target_train.groupby('ban').tail(1)\n",
    "df_train = df_train.merge(df_target_train[['ban', 'target_ind']], on='ban', how='left')\n",
    "df_train.rename(columns={'target_ind': 'target'}, inplace=True)\n",
    "df_train.dropna(subset=['target'], inplace=True)\n",
    "df_train['target'] = df_train['target'].astype(int)\n",
    "print(df_train.shape)\n",
    "\n",
    "#set up df_test\n",
    "sql_test = ''' SELECT * FROM `{}.{}.bq_call_to_retention_targets` '''.format(project_id, dataset_id) \n",
    "df_target_test = client.query(sql_test).to_dataframe()\n",
    "df_target_test = df_target_test.loc[\n",
    "    df_target_test['YEAR_MONTH'] == '-'.join(score_date_val_dash.split('-')[:2])]  # score_date_dash = '2022-09-30'\n",
    "df_target_test['ban'] = df_target_test['ban'].astype('int64')\n",
    "df_target_test = df_target_test.groupby('ban').tail(1)\n",
    "df_test = df_test.merge(df_target_test[['ban', 'target_ind']], on='ban', how='left')\n",
    "df_test.rename(columns={'target_ind': 'target'}, inplace=True)\n",
    "df_test.dropna(subset=['target'], inplace=True)\n",
    "df_test['target'] = df_test['target'].astype(int)\n",
    "print(df_test.shape)\n",
    "\n",
    "#set up features (list)\n",
    "cols_1 = df_train.columns.values\n",
    "cols_2 = df_test.columns.values\n",
    "cols = set(cols_1).intersection(set(cols_2))\n",
    "features = [f for f in cols if f not in ['ban', 'target']]\n",
    "\n",
    "#train test split\n",
    "df_train, df_val = train_test_split(df_train, shuffle=True, test_size=0.3, random_state=42,\n",
    "                                    stratify=df_train['target']\n",
    "                                    )\n",
    "\n",
    "#train test split\n",
    "df_test, df_final = train_test_split(df_test, shuffle=True, test_size=0.3, random_state=42,\n",
    "                                    stratify=df_test['target']\n",
    "                                    )\n",
    "\n",
    "train_sampled = df_train.drop(columns=['ban'], axis=1) \n",
    "valid_sampled = df_val.drop(columns=['ban'], axis=1) \n",
    "test_sampled = df_test.drop(columns=['ban'], axis=1) \n",
    "final_sampled = df_final.drop(columns=['ban'], axis=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14680646-06dd-484a-af5a-a0b08fbe7b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_sampled.shape) \n",
    "print(valid_sampled.shape) \n",
    "print(test_sampled.shape)\n",
    "print(final_sampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63996de-487e-424a-9b8c-c0b63e7f681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ban_train = df_train['ban']\n",
    "X_train = df_train[features]\n",
    "y_train = np.squeeze(df_train['target'].values)\n",
    "\n",
    "ban_val = df_val['ban']\n",
    "X_val = df_val[features]\n",
    "y_val = np.squeeze(df_val['target'].values)\n",
    "\n",
    "ban_test = df_test['ban']\n",
    "X_test = df_test[features]\n",
    "y_test = np.squeeze(df_test['target'].values)\n",
    "\n",
    "# del df_train, df_val, df_test\n",
    "# gc.collect()\n",
    "\n",
    "################################ Pycaret Setup initialize  ############################ \n",
    "classification_setup = setup(data=train_sampled, \n",
    "                         # ignore_features=drop_cols,\n",
    "                         test_data = valid_sampled,\n",
    "                         target='target',\n",
    "                         fix_imbalance=False,\n",
    "                         remove_outliers = True,\n",
    "                         normalize=True,\n",
    "                         normalize_method='zscore',\n",
    "                         log_experiment=False,\n",
    "                         remove_multicollinearity=True,\n",
    "                         multicollinearity_threshold=0.95,\n",
    "                         feature_selection=True,\n",
    "                         fold=5,\n",
    "                         fold_shuffle=True,\n",
    "                         session_id=123,\n",
    "                         numeric_features=features,\n",
    "                         silent=True)\n",
    "\n",
    "### Pycaret top 3 models to analyze\n",
    "best_model = compare_models(include = ['rf','xgboost','lightgbm'],errors='raise', n_select=3)\n",
    "\n",
    "# save the model reports and report fig of all top 2 models to GCS\n",
    "todays_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "save_path = f'pycaret/{todays_date}/'\n",
    "model_reports, model_to_report_map = evaluate_and_save_models(models=best_model.copy(), \n",
    "                                     bucket_name=bucket_name,\n",
    "                                     save_path=save_path, \n",
    "                                     test_df=test_sampled,\n",
    "                                     actual_label_str='target',\n",
    "                                     columns = get_config('X_train').columns,\n",
    "                                     save_columns=True,\n",
    "                                     show_report=False)\n",
    "\n",
    "# Find the top Model and top model's report Figs\n",
    "top_model = None\n",
    "for i in range(len(best_model)):\n",
    "    if best_model[i].__class__.__name__ == model_reports.sort_values([\"Recall\",\"Precision\"],ascending=False).head(1)[\"model_name\"][0]:\n",
    "        top_model = best_model.copy()[i]\n",
    "\n",
    "best_model = model_reports.sort_values([\"Recall\",\"Precision\"],ascending=False).head(1)[\"model_name\"][0]\n",
    "best_model_report = model_to_report_map[top_model.__class__.__name__]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e83e2a-fe6b-4328-9029-9fea4a079be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################ Export the top model's report and figs to GCS ###############################\n",
    "bucket = storage.Client().bucket(bucket_name)\n",
    "save_path = f'models/best_model/{todays_date}/'\n",
    "create_folder_if_not_exists(save_path)\n",
    "\n",
    "######### Save HTML report of the best model\n",
    "best_model_report.write_html(f\"{save_path}{todays_date}_{top_model.__class__.__name__}.html\")\n",
    "\n",
    "filename = f\"{todays_date}_{top_model.__class__.__name__}.html\"\n",
    "blob = bucket.blob(f\"{save_path}{filename}\")\n",
    "blob.upload_from_filename(f\"{save_path}{todays_date}_{top_model.__class__.__name__}.html\")\n",
    "logging.info(f\"{filename} sucessfully uploaded to GCS bucket!\")\n",
    "\n",
    "####### Save the model\n",
    "model_file_name = '{save_path}{model_type}_{date}'.format(save_path = save_path,\n",
    "                                  model_type = top_model.__class__.__name__,    \n",
    "                                  date=datetime.now().strftime(\"%Y-%m-%d\"))                                                                   \n",
    "save_model(top_model,model_file_name)\n",
    "filename = '{model_type}_{date}.pkl'.format(model_type=top_model.__class__.__name__,date=datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "blob = bucket.blob(f\"{save_path}{filename}\")\n",
    "blob.upload_from_filename(f\"{model_file_name}.pkl\")\n",
    "logging.info(f\"{filename} sucessfully uploaded to GCS bucket!\")\n",
    "\n",
    "###############################  Tuned Model  ##############################\n",
    "model_base = create_model(top_model)\n",
    "tuned_model, tuner = tune_model(model_base, optimize='recall', return_tuner = True, n_iter = 20)\n",
    "save_path = f'models/best_model/tuned/{todays_date}/'\n",
    "model_reports_tuned, model_to_report_map_tuned = evaluate_and_save_models(models=tuned_model, \n",
    "                                     bucket_name=bucket_name,\n",
    "                                     save_path=save_path, \n",
    "                                     test_df=test_sampled,\n",
    "                                     actual_label_str='target',\n",
    "                                     columns = get_config('X_train').columns,\n",
    "                                     save_columns=True,\n",
    "                                     show_report=False)\n",
    "\n",
    "###############################   Define  Final Model     ##############################\n",
    "\n",
    "final_model_report = None\n",
    "final_model_class_name = None\n",
    "final_model_file = None\n",
    "if model_reports_tuned.Recall.values[0] >= 1 :\n",
    "    logging.info(\"CAUTION : TUNED MODEL had 100% recall. TUNED model was not selected as best model. \")\n",
    "    final_model_class_name = top_model.__class__.__name__\n",
    "    final_model_report = best_model_report\n",
    "    final_model_file = top_model\n",
    "elif model_reports_tuned.Recall.values[0] > model_reports.sort_values([\"Recall\",\"Precision\"],ascending=False).head(1).Recall.values[0]:\n",
    "    base_recall = model_reports.sort_values([\"Recall\",\"Precision\"],ascending=False).head(1).Recall.values[0]\n",
    "    logging.info(f\"TUNED MODEL had {model_reports_tuned.Recall.values[0]*100} recall and Base model without tuning had {base_recall*100} Recall. TUNED model was selected as best model. \")\n",
    "\n",
    "    final_model_class_name = tuned_model.__class__.__name__\n",
    "    final_model_report = model_to_report_map_tuned[final_model_class_name]\n",
    "    final_model_file = tuned_model\n",
    "else:\n",
    "    base_recall = model_reports.sort_values([\"Recall\",\"Precision\"],ascending=False).head(1).Recall.values[0]\n",
    "    logging.info(f\"TUNED MODEL had {model_reports_tuned.Recall.values[0]*100} recall and Base model without tuning had {base_recall*100} Recall. TUNED model was selected as best model. \")\n",
    "    final_model_class_name = top_model.__class__.__name__\n",
    "    final_model_report = best_model_report\n",
    "    final_model_file = top_model\n",
    "\n",
    "###############################  Save the Report and model    ###############################\n",
    "# Save HTML report of the selected model\n",
    "# final_model_report = model_to_report_map_tuned[tuned_model.__class__.__name__]\n",
    "save_path = f'models/final_selected/{todays_date}/'\n",
    "create_folder_if_not_exists(save_path)\n",
    "final_model_report.write_html(f\"{save_path}{todays_date}_{final_model_class_name}.html\")\n",
    "# bucket = storage.Client().bucket(bucket)\n",
    "filename = f\"{todays_date}_{final_model_class_name}.html\"\n",
    "blob = bucket.blob(f\"{save_path}{filename}\")\n",
    "blob.upload_from_filename(f\"{save_path}{todays_date}_{final_model_class_name}.html\")\n",
    "print(f\"{filename} sucessfully uploaded to GCS bucket!\")\n",
    "\n",
    "\n",
    "model_file_name = '{save_path}{model_type}_{date}'.format(save_path = save_path,\n",
    "                                  model_type = final_model_class_name,    \n",
    "                                  date=datetime.now().strftime(\"%Y-%m-%d\"))                                                                   \n",
    "save_model(final_model_file,model_file_name)\n",
    "filename = 'model.pkl'.format(model_type=final_model_class_name,date=datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "blob = bucket.blob(f\"{save_path}{filename}\")\n",
    "blob.upload_from_filename(f\"{model_file_name}.pkl\")\n",
    "print(f\"{filename} sucessfully uploaded to GCS bucket!\")\n",
    "\n",
    "######################## Save the final Model for Upload Moel componet with Renaming ##############\n",
    "# model.uri = f'gs://{bucket_name}/models/final_selected/{todays_date}/'\n",
    "\n",
    "# save_model(final_model_file,'model.pkl')\n",
    "# final_model_file.save_model(model.path + \".bst\")\n",
    "\n",
    "###################### Output Final Selected Model's HTML Report View ########################\n",
    "\n",
    "model_metrics_report.path = f'gs://{bucket_name}/{save_path}{todays_date}_{final_model_class_name}.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6691a8c1-ffde-417a-9734-d6e2e27a0756",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "    from datetime import datetime\n",
    "    import logging \n",
    "    from pycaret.classification import setup,create_model,tune_model, predict_model,get_config,compare_models,save_model,tune_model\n",
    "    from sklearn.metrics import accuracy_score, roc_auc_score, precision_recall_curve, mean_squared_error, f1_score, precision_score, recall_score, confusion_matrix, roc_curve\n",
    " \n",
    "\n",
    "    ### import data\n",
    "    # CREDENTIALS = google.oauth2.credentials.Credentials(token)\n",
    "    # # import google.oauth2.credentials\n",
    "   \n",
    "    client = bigquery.Client(project=project_id, location='northamerica-northeast1')\n",
    "    storage_client = storage.Client(project=project_id)\n",
    "\n",
    "\n",
    "    # Get utils.py\n",
    "    bucket = storage_client.get_bucket(resources_bucket_name)\n",
    "    blob = bucket.get_blob(f\"{utils_file_path}/{utils_filename}\")\n",
    "    blob.download_to_filename(utils_filename)\n",
    "    blob = bucket.get_blob(f\"{utils_file_path}/{plot_utils_filename}\")\n",
    "    blob.download_to_filename(plot_utils_filename)\n",
    "    \n",
    "    from preprocessing_utils import pre_process_data\n",
    "    from preprocessing_utils import downsampling\n",
    "    from plotly_utils import evaluate_and_save_models,create_folder_if_not_exists,ploty_model_metrics,plotly_feature_importance,plotly_lift_curve, plotly_model_report,plotly_roc, plotly_confusion_matrix,plotly_output_hist,plotly_precision_recall \n",
    "    # specify the path to the training data\n",
    "    training_table = f\"{project_id}.{dataset}.{training_dataset}\"\n",
    "\n",
    "    # generate the query\n",
    "    train_query = '''\n",
    "       SELECT * \n",
    "                FROM `{training_table}`\n",
    "    '''.format(training_table = training_table)\n",
    "   \n",
    "\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "\n",
    "    # create a dataframe with the training data\n",
    "    train_all = client.query(train_query, job_config=job_config).to_dataframe()\n",
    "\n",
    "     ##############  Split train/valid/test based of Dev Training Sample Size   #######################\n",
    "    # training_perc = 0.62\n",
    "    train_df = train_all.sort_values([\"partition_dt\"]).iloc[:int(train_all.shape[0]*training_perc)]\n",
    "\n",
    "\n",
    "    lower_bound = int(train_all.shape[0]*training_perc)\n",
    "    upper_bound = lower_bound + int(train_all.shape[0]*((1-training_perc)/2))\n",
    "    valid_df = train_all.sort_values([\"partition_dt\"]).iloc[lower_bound:upper_bound]\n",
    "\n",
    "    lower_bound = train_df.shape[0] + valid_df.shape[0]\n",
    "    upper_bound = lower_bound + int(train_all.shape[0]*((1-training_perc)/2))\n",
    "    test_df = train_all.sort_values([\"partition_dt\"]).iloc[lower_bound:]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2eea99-a3f0-48ea-8962-115edcaf26d9",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3cda87-dc1f-495e-be67-a3d623c13e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dsl.pipeline(\n",
    "#     # A name for the pipeline.\n",
    "#     name=\"{}-xgb-pipeline\".format(SERVICE_TYPE_NAME),\n",
    "#     description=' pipeline for training {} model'.format(SERVICE_TYPE_NAME)\n",
    "# )\n",
    "def pipeline(\n",
    "        project_id: str = PROJECT_ID,\n",
    "        region: str = REGION,\n",
    "        resource_bucket: str = RESOURCE_BUCKET, \n",
    "        file_bucket: str = FILE_BUCKET\n",
    "    ):\n",
    "    \n",
    "    # ----- create training set --------\n",
    "    bq_create_training_dataset_op = bq_create_dataset(score_date=SCORE_DATE_DASH,\n",
    "                          score_date_delta=SCORE_DATE_DELTA,\n",
    "                          project_id=PROJECT_ID,\n",
    "                          dataset_id=DATASET_ID,\n",
    "                          region=REGION,\n",
    "                          promo_expiry_start=PROMO_EXPIRY_START, \n",
    "                          promo_expiry_end=PROMO_EXPIRY_END, \n",
    "                          v_start_date=SCORE_DATE_MINUS_6_MOS_DASH,\n",
    "                          v_end_date=SCORE_DATE_LAST_MONTH_END_DASH)\n",
    "    \n",
    "    # ----- preprocessing train data --------\n",
    "    preprocess_train_op = preprocess(\n",
    "        pipeline_dataset='bq_ctr_pipeline_dataset', \n",
    "        save_data_path='gs://{}/{}_train.csv.gz'.format(FILE_BUCKET, SERVICE_TYPE),\n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID\n",
    "    )\n",
    "\n",
    "    # preprocess_train_op.set_memory_limit('128G')\n",
    "    # preprocess_train_op.set_cpu_limit('32')\n",
    "\n",
    "    bq_create_training_dataset_op \n",
    "    preprocess_train_op\n",
    "\n",
    "    # ----- create validation set --------\n",
    "    bq_create_validation_dataset_op = bq_create_dataset(score_date=SCORE_DATE_VAL_DASH,\n",
    "                          score_date_delta=SCORE_DATE_VAL_DELTA,\n",
    "                          project_id=PROJECT_ID,\n",
    "                          dataset_id=DATASET_ID,\n",
    "                          region=REGION,\n",
    "                          promo_expiry_start=PROMO_EXPIRY_START_VAL, \n",
    "                          promo_expiry_end=PROMO_EXPIRY_END_VAL, \n",
    "                          v_start_date=SCORE_DATE_VAL_MINUS_6_MOS_DASH,\n",
    "                          v_end_date=SCORE_DATE_VAL_LAST_MONTH_END_DASH)\n",
    "    \n",
    "    # ----- preprocessing validation data --------\n",
    "    preprocess_validation_op = preprocess(\n",
    "        pipeline_dataset='bq_ctr_pipeline_dataset', \n",
    "        save_data_path='gs://{}/{}_validation.csv.gz'.format(FILE_BUCKET, SERVICE_TYPE),\n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID\n",
    "    )\n",
    "\n",
    "    # preprocess_validation_op.set_memory_limit('256G')\n",
    "    # preprocess_validation_op.set_cpu_limit('32')\n",
    "\n",
    "    bq_create_validation_dataset_op\n",
    "    preprocess_validation_op\n",
    "\n",
    "    train_and_save_model_op = train_and_save_model(file_bucket=FILE_BUCKET,\n",
    "                                                   service_type=SERVICE_TYPE,\n",
    "                                                   score_date_dash=SCORE_DATE_DASH,\n",
    "                                                   score_date_val_dash=SCORE_DATE_VAL_DASH,\n",
    "                                                   project_id=PROJECT_ID,\n",
    "                                                   dataset_id=DATASET_ID,\n",
    "                                                   )\n",
    "    \n",
    "    train_and_save_model_op\n",
    "    \n",
    "#     train_and_save_model_op.set_memory_limit('256G')\n",
    "#     train_and_save_model_op.set_cpu_limit('32')\n",
    "\n",
    "#     train_and_save_model_op.after(preprocess_train_op)\n",
    "#     train_and_save_model_op.after(preprocess_validation_op)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2a51c8-4c37-4392-a21a-c4c3279d36d4",
   "metadata": {},
   "source": [
    "### Run the Pipeline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5313e5-b5cd-4a78-9df5-91af24168e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline(project_id = PROJECT_ID,\n",
    "#         region = REGION,\n",
    "#         resource_bucket = RESOURCE_BUCKET,\n",
    "#         file_bucket = FILE_BUCKET)\n",
    "\n",
    "\n",
    "pipeline(project_id = PROJECT_ID,\n",
    "        region = REGION,\n",
    "        resource_bucket = RESOURCE_BUCKET, \n",
    "        file_bucket = FILE_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27465a63-5c3b-4e96-9c08-f9cc5dafde90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kfp.v2 import compiler\n",
    "# from google.cloud.aiplatform import pipeline_jobs\n",
    "\n",
    "# import json\n",
    "\n",
    "# compiler.Compiler().compile(\n",
    "#    pipeline_func=pipeline, package_path=\"pipeline.json\"\n",
    "# )\n",
    "\n",
    "# job = pipeline_jobs.PipelineJob(\n",
    "#                                display_name=PIPELINE_NAME,\n",
    "#                                template_path=\"pipeline.json\",\n",
    "#                                location=REGION,\n",
    "#                                enable_caching=False,\n",
    "#                                pipeline_root = f\"gs://{RESOURCE_BUCKET}\"\n",
    "# )\n",
    "# job.run(\n",
    "#    service_account = f\"bilayer-sa@{PROJECT_ID}.iam.gserviceaccount.com\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382c8f46-4d79-423d-8a73-4fc55432b905",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
