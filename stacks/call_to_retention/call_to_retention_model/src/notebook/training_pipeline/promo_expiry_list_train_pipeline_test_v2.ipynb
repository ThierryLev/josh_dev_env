{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c150da3-6e1e-4f02-a778-bb3b12af9054",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b559b7e6-fe51-49cc-8e9c-832380843832",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output, OutputPath, ClassificationMetrics,\n",
    "                        Metrics, component)\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import google\n",
    "from google.oauth2 import credentials\n",
    "from google.oauth2 import service_account\n",
    "from google.oauth2.service_account import Credentials\n",
    "from google.cloud import storage\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google_cloud_pipeline_components.v1.batch_predict_job import \\\n",
    "    ModelBatchPredictOp as batch_prediction_op\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558ff533-70c8-4421-813e-c567d6bc496b",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f70d721-76e7-4c2e-8153-88b5bbe8ee47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tag cell with parameters\n",
    "PROJECT_ID =  ''\n",
    "BUCKET_NAME=''\n",
    "DATASET_ID = ''\n",
    "RESOURCE_BUCKET = ''\n",
    "FILE_BUCKET = ''\n",
    "REGION = ''\n",
    "MODEL_ID = '5090'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7035200a-de6b-412c-8406-64854b639cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag cell with parameters\n",
    "PROJECT_ID =  'divg-josh-pr-d1cc3a'\n",
    "BUCKET_NAME='divg-josh-pr-d1cc3a-default'\n",
    "DATASET_ID = 'call_to_retention_dataset'\n",
    "RESOURCE_BUCKET = 'divg-josh-pr-d1cc3a-default'\n",
    "FILE_BUCKET = 'divg-josh-pr-d1cc3a-default'\n",
    "MODEL_ID = '5090'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eeed21-e12b-45ee-b1e4-d0c8f3843533",
   "metadata": {},
   "source": [
    "### Service Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65d1668-e1d8-4f58-958f-edc44d06a6b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SERVICE_TYPE = 'call_to_retention'\n",
    "SERVICE_TYPE_NAME = 'call-to-retention'\n",
    "TABLE_ID = 'bq_call_to_retention_targets'\n",
    "REGION = \"northamerica-northeast1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2229e5d1-1362-40ed-aa7d-cb22e41e4960",
   "metadata": {},
   "source": [
    "### Pulumi Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d628b-e759-4c9c-aab5-568c54721aaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "STACK_NAME = 'call_to_retention'\n",
    "TRAIN_PIPELINE_NAME_PATH = 'train_pipeline'\n",
    "PREDICT_PIPELINE_NAME_PATH = 'predict_pipeline'\n",
    "TRAIN_PIPELINE_NAME = 'call-to-retention-train-pipeline' # Same name as pulumi.yaml\n",
    "PREDICT_PIPELINE_NAME = 'call-to-retention-predict-pipeline' # Same name as pulumi.yaml\n",
    "TRAIN_PIPELINE_DESCRIPTION = 'call-to-retention-train-pipeline'\n",
    "PREDICT_PIPELINE_DESCRIPTION = 'call-to-retention-predict-pipeline'\n",
    "REGION = \"northamerica-northeast1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85558b11-be96-49ea-91c5-d17b15c00bf1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Query + Pre-Processing Component Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370852f7-2e23-4ac3-8cd8-b40c39be85ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_QUERIES_PATH = f\"{STACK_NAME}/{TRAIN_PIPELINE_NAME_PATH}/queries/\" \n",
    "TRAIN_UTILS_FILE_PATH = f\"{STACK_NAME}/{TRAIN_PIPELINE_NAME_PATH}/utils\" \n",
    "UTILS_FILENAME = 'utils.py'\n",
    "\n",
    "PROCESSED_SERVING_DATA_TABLENAME = 'processed_serving_data'\n",
    "INPUT_SERVING_DATA_TABLENAME = 'input_serving_data'\n",
    "\n",
    "QUERY_DATE = (date.today() - relativedelta(days=1)).strftime('%Y-%m-%d')\n",
    "TARGET_TABLE_REF = '{}.{}.{}'.format(PROJECT_ID, DATASET_ID, TABLE_ID)\n",
    "\n",
    "QUERIES_PATH = 'call_to_retention/queries/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc0bd98-1d58-491e-bc5e-bbac378c7bbf",
   "metadata": {},
   "source": [
    "### Import Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f442904b-a40a-4ba8-b237-e9894ffd4f26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download required component files to local\n",
    "prefix = f'{STACK_NAME}/{TRAIN_PIPELINE_NAME_PATH}/components/'\n",
    "dl_dir = 'components/'\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(RESOURCE_BUCKET)\n",
    "blobs = bucket.list_blobs(prefix=prefix)  # Get list of files\n",
    "for blob in blobs: # download each file that starts with \"prefix\" into \"dl_dir\"\n",
    "    if blob.name.endswith(\"/\"):\n",
    "        continue\n",
    "    file_split = blob.name.split(prefix)\n",
    "    file_path = f\"{dl_dir}{file_split[-1]}\"\n",
    "    directory = \"/\".join(file_path.split(\"/\")[0:-1])\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    blob.download_to_filename(file_path) \n",
    "\n",
    "# import main pipeline components\n",
    "import components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c315d454-416f-4eed-b12d-1acd5aed1207",
   "metadata": {},
   "source": [
    "### Date Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfad310-a482-4285-aa31-6d9113008be6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scoringDate = date(2022, 9, 1)  # date.today() - relativedelta(days=2)- relativedelta(months=30)\n",
    "valScoringDate = date(2022, 10, 1)  # scoringDate - relativedelta(days=2)\n",
    "\n",
    "# training dates\n",
    "SCORE_DATE = scoringDate.strftime('%Y%m%d')  # date.today().strftime('%Y%m%d')\n",
    "SCORE_DATE_DASH = scoringDate.strftime('%Y-%m-%d')\n",
    "SCORE_DATE_MINUS_6_MOS_DASH = ((scoringDate - relativedelta(months=6)).replace(day=1)).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_LAST_MONTH_START_DASH = (scoringDate.replace(day=1) - timedelta(days=1)).replace(day=1).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_LAST_MONTH_END_DASH = ((scoringDate.replace(day=1)) - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "PROMO_EXPIRY_START = (scoringDate.replace(day=1) + relativedelta(months=3)).replace(day=1).strftime('%Y-%m-%d')\n",
    "PROMO_EXPIRY_END = (scoringDate.replace(day=1) + relativedelta(months=4)).replace(day=1).strftime('%Y-%m-%d')\n",
    "\n",
    "# validation dates\n",
    "SCORE_DATE_VAL = valScoringDate.strftime('%Y%m%d')\n",
    "SCORE_DATE_VAL_DASH = valScoringDate.strftime('%Y-%m-%d')\n",
    "SCORE_DATE_VAL_MINUS_6_MOS_DASH = ((valScoringDate - relativedelta(months=6)).replace(day=1)).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_VAL_LAST_MONTH_START_DASH = (valScoringDate.replace(day=1) - timedelta(days=1)).replace(day=1).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_VAL_LAST_MONTH_END_DASH = ((valScoringDate.replace(day=1)) - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "PROMO_EXPIRY_START_VAL = (valScoringDate.replace(day=1) + relativedelta(months=3)).replace(day=1).strftime('%Y-%m-%d')\n",
    "PROMO_EXPIRY_END_VAL = (valScoringDate.replace(day=1) + relativedelta(months=4)).replace(day=1).strftime('%Y-%m-%d')\n",
    "\n",
    "SCORE_DATE_DELTA = 0\n",
    "SCORE_DATE_VAL_DELTA = 0\n",
    "TICKET_DATE_WINDOW = 30  # Days of ticket data to be queried\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0d12b8-ccad-4c70-8c12-d8837797225c",
   "metadata": {},
   "source": [
    "### bq_create_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324ba175-f34b-4e02-975e-c39fe1084e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "# from kfp.v2.dsl import (Model, Input, component)\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,HTML,\n",
    "                        OutputPath, ClassificationMetrics, Metrics, component)\n",
    "from typing import NamedTuple\n",
    "# Create Training Dataset for training pipeline\n",
    "@component(\n",
    "    base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/wb-platform/pipelines/kubeflow-pycaret:latest\",\n",
    "    output_component_file=\"bq_create_dataset.yaml\",\n",
    ")\n",
    "def bq_create_dataset(score_date: str,\n",
    "                      score_date_delta: int,\n",
    "                      project_id: str,\n",
    "                      dataset_id: str,\n",
    "                      region: str,\n",
    "                      promo_expiry_start: str, \n",
    "                      promo_expiry_end: str, \n",
    "                      v_start_date: str,\n",
    "                      v_end_date: str) -> NamedTuple(\"output\", [(\"col_list\", list)]):\n",
    " \n",
    "    from google.cloud import bigquery\n",
    "    import logging \n",
    "    from datetime import datetime\n",
    "    # For wb\n",
    "    # import google.oauth2.credentials\n",
    "    # CREDENTIALS = google.oauth2.credentials.Credentials(token)\n",
    "    \n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    client = get_gcp_bqclient(project_id)\n",
    "    # client = bigquery.Client(project=project_id, location=region)\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    \n",
    "    # Change dataset / table + sp table name to version in bi-layer\n",
    "    query =\\\n",
    "        f'''\n",
    "            DECLARE score_date DATE DEFAULT \"{score_date}\";\n",
    "            DECLARE promo_expiry_start DATE DEFAULT \"{promo_expiry_start}\";\n",
    "            DECLARE promo_expiry_end DATE DEFAULT \"{promo_expiry_end}\";\n",
    "            DECLARE start_date DATE DEFAULT \"{v_start_date}\";\n",
    "            DECLARE end_date DATE DEFAULT \"{v_end_date}\";\n",
    "        \n",
    "            -- Change dataset / sp name to the version in the bi_layer\n",
    "            CALL {dataset_id}.bq_sp_ctr_pipeline_dataset(score_date, promo_expiry_start, promo_expiry_end, start_date, end_date);\n",
    "\n",
    "            SELECT\n",
    "                *\n",
    "            FROM {dataset_id}.INFORMATION_SCHEMA.PARTITIONS\n",
    "            WHERE table_name='bq_ctr_pipeline_dataset'\n",
    "            \n",
    "        '''\n",
    "    \n",
    "    df = client.query(query, job_config=job_config).to_dataframe()\n",
    "    logging.info(df.to_string())\n",
    "    \n",
    "    logging.info(f\"Loaded {df.total_rows[0]} rows into \\\n",
    "             {df.table_catalog[0]}.{df.table_schema[0]}.{df.table_name[0]} on \\\n",
    "             {datetime.strftime((df.last_modified_time[0]), '%Y-%m-%d %H:%M:%S') } !\")\n",
    "    \n",
    "    ######################################## Save column list_##########################\n",
    "    query =\\\n",
    "        f'''\n",
    "           SELECT\n",
    "                *\n",
    "            FROM {dataset_id}.bq_ctr_pipeline_dataset\n",
    "\n",
    "        '''\n",
    "    \n",
    "    df = client.query(query, job_config=job_config).to_dataframe()\n",
    "    \n",
    "    col_list = list([col for col in df.columns])\n",
    "    return (col_list,)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281c6676-70b0-41a6-b860-aa6d95fec0da",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8623f9-e2fd-406a-b0a7-2ea7c42af66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "        pipeline_dataset: str, \n",
    "        save_data_path: str,\n",
    "        project_id: str,\n",
    "        dataset_id: str\n",
    "):\n",
    "    from google.cloud import bigquery\n",
    "    import pandas as pd\n",
    "    import gc\n",
    "    import time\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    client = get_gcp_bqclient(project_id)\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "\n",
    "    # pipeline_dataset \n",
    "    pipeline_dataset_name = f\"{project_id}.{dataset_id}.{pipeline_dataset}\" \n",
    "    build_df_pipeline_dataset = f'SELECT * FROM `{pipeline_dataset_name}`'\n",
    "    df_pipeline_dataset = client.query(build_df_pipeline_dataset).to_dataframe()\n",
    "    df_pipeline_dataset = df_pipeline_dataset.set_index('ban') \n",
    "\n",
    "    # demo columns\n",
    "    df_pipeline_dataset['demo_urban_flag'] = df_pipeline_dataset.demo_sgname.str.lower().str.contains('urban').fillna(0).astype(int)\n",
    "    df_pipeline_dataset['demo_rural_flag'] = df_pipeline_dataset.demo_sgname.str.lower().str.contains('rural').fillna(0).astype(int)\n",
    "    df_pipeline_dataset['demo_family_flag'] = df_pipeline_dataset.demo_lsname.str.lower().str.contains('families').fillna(0).astype(int)\n",
    "\n",
    "    df_income_dummies = pd.get_dummies(df_pipeline_dataset[['demo_lsname']]) \n",
    "    df_income_dummies.columns = df_income_dummies.columns.str.replace('&', 'and')\n",
    "    df_income_dummies.columns = df_income_dummies.columns.str.replace(' ', '_')\n",
    "\n",
    "    df_pipeline_dataset.drop(columns=['demo_sgname', 'demo_lsname'], axis=1, inplace=True)\n",
    "\n",
    "    df_pipeline_dataset = df_pipeline_dataset.join(df_income_dummies)\n",
    "\n",
    "    df_join = df_pipeline_dataset.copy()\n",
    "\n",
    "    #column name clean-up\n",
    "    df_join.columns = df_join.columns.str.replace(' ', '_')\n",
    "    df_join.columns = df_join.columns.str.replace('-', '_')\n",
    "\n",
    "    df_join.head()\n",
    "\n",
    "    #df_final\n",
    "    df_final = df_join.copy()\n",
    "    del df_join\n",
    "    gc.collect()\n",
    "    print('......df_final done')\n",
    "\n",
    "    for f in df_final.columns:\n",
    "        df_final[f] = list(df_final[f])\n",
    "\n",
    "    df_final.to_csv(save_data_path, index=True, compression='gzip') \n",
    "    del df_final\n",
    "    gc.collect()\n",
    "    print(f'......csv saved in {save_data_path}')\n",
    "    time.sleep(120)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330396ff-dfe9-4c23-9d67-922e9a190bdc",
   "metadata": {},
   "source": [
    "### Train and Save Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f33719-127b-45c7-bccd-dcd3970b003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_model(\n",
    "            file_bucket: str,\n",
    "            service_type: str,\n",
    "            score_date_dash: str,\n",
    "            score_date_val_dash: str,\n",
    "            project_id: str,\n",
    "            dataset_id: str\n",
    "):\n",
    "\n",
    "    import gc\n",
    "    import time\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    from google.cloud import storage\n",
    "    from google.cloud import bigquery\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    def get_lift(prob, y_test, q):\n",
    "        result = pd.DataFrame(columns=['Prob', 'Call_To_Retention'])\n",
    "        result['Prob'] = prob\n",
    "        result['Call_To_Retention'] = y_test\n",
    "        result['Decile'] = pd.qcut(result['Prob'], q, labels=[i for i in range(q, 0, -1)])\n",
    "        add = pd.DataFrame(result.groupby('Decile')['Call_To_Retention'].mean()).reset_index()\n",
    "        add.columns = ['Decile', 'avg_real_call_to_retention_rate']\n",
    "        result = result.merge(add, on='Decile', how='left')\n",
    "        result.sort_values('Decile', ascending=True, inplace=True)\n",
    "        lg = pd.DataFrame(result.groupby('Decile')['Prob'].mean()).reset_index()\n",
    "        lg.columns = ['Decile', 'avg_model_pred_call_to_retention_rate']\n",
    "        lg.sort_values('Decile', ascending=False, inplace=True)\n",
    "        lg['avg_call_to_retention_rate_total'] = result['Call_To_Retention'].mean()\n",
    "        lg = lg.merge(add, on='Decile', how='left')\n",
    "        lg['lift'] = lg['avg_real_call_to_retention_rate'] / lg['avg_call_to_retention_rate_total']\n",
    "\n",
    "        return lg\n",
    "\n",
    "    df_train = pd.read_csv('gs://{}/{}_train.csv.gz'.format(file_bucket, service_type),\n",
    "                           compression='gzip')  \n",
    "    df_test = pd.read_csv('gs://{}/{}_validation.csv.gz'.format(file_bucket, service_type),  \n",
    "                          compression='gzip')\n",
    "\n",
    "    #set up df_train\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    sql_train = ''' SELECT * FROM `{}.{}.bq_call_to_retention_targets` '''.format(project_id, dataset_id) \n",
    "    df_target_train = client.query(sql_train).to_dataframe()\n",
    "    df_target_train = df_target_train.loc[\n",
    "        df_target_train['YEAR_MONTH'] == '-'.join(score_date_dash.split('-')[:2])]  # score_date_dash = '2022-08-31'\n",
    "    df_target_train['ban'] = df_target_train['ban'].astype('int64')\n",
    "    df_target_train = df_target_train.groupby('ban').tail(1)\n",
    "    df_train = df_train.merge(df_target_train[['ban', 'target_ind']], on='ban', how='left')\n",
    "    df_train.rename(columns={'target_ind': 'target'}, inplace=True)\n",
    "    df_train.dropna(subset=['target'], inplace=True)\n",
    "    df_train['target'] = df_train['target'].astype(int)\n",
    "    print(df_train.shape)\n",
    "\n",
    "    #set up df_test\n",
    "    sql_test = ''' SELECT * FROM `{}.{}.bq_call_to_retention_targets` '''.format(project_id, dataset_id) \n",
    "    df_target_test = client.query(sql_test).to_dataframe()\n",
    "    df_target_test = df_target_test.loc[\n",
    "        df_target_test['YEAR_MONTH'] == '-'.join(score_date_val_dash.split('-')[:2])]  # score_date_dash = '2022-09-30'\n",
    "    df_target_test['ban'] = df_target_test['ban'].astype('int64')\n",
    "    df_target_test = df_target_test.groupby('ban').tail(1)\n",
    "    df_test = df_test.merge(df_target_test[['ban', 'target_ind']], on='ban', how='left')\n",
    "    df_test.rename(columns={'target_ind': 'target'}, inplace=True)\n",
    "    df_test.dropna(subset=['target'], inplace=True)\n",
    "    df_test['target'] = df_test['target'].astype(int)\n",
    "    print(df_test.shape)\n",
    "\n",
    "    #set up features (list)\n",
    "    cols_1 = df_train.columns.values\n",
    "    cols_2 = df_test.columns.values\n",
    "    cols = set(cols_1).intersection(set(cols_2))\n",
    "    features = [f for f in cols if f not in ['ban', 'target']]\n",
    "\n",
    "    #train test split\n",
    "    df_train, df_val = train_test_split(df_train, shuffle=True, test_size=0.2, random_state=42,\n",
    "                                        stratify=df_train['target']\n",
    "                                        )\n",
    "\n",
    "    ban_train = df_train['ban']\n",
    "    X_train = df_train[features]\n",
    "    y_train = np.squeeze(df_train['target'].values)\n",
    "\n",
    "    ban_val = df_val['ban']\n",
    "    X_val = df_val[features]\n",
    "    y_val = np.squeeze(df_val['target'].values)\n",
    "\n",
    "    ban_test = df_test['ban']\n",
    "    X_test = df_test[features]\n",
    "    y_test = np.squeeze(df_test['target'].values)\n",
    "\n",
    "    del df_train, df_val, df_test\n",
    "    gc.collect()\n",
    "\n",
    "    # build model and fit in training data\n",
    "    import xgboost as xgb\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        learning_rate=0.01,\n",
    "        n_estimators=100,\n",
    "        max_depth=8,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective='binary:logistic',\n",
    "        nthread=4,\n",
    "        scale_pos_weight=1\n",
    "        # seed=27\n",
    "    )\n",
    "\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    print('xgb training done')\n",
    "\n",
    "    from sklearn.preprocessing import normalize\n",
    "\n",
    "#     #predictions on X_val\n",
    "#     y_pred = xgb_model.predict_proba(X_val, ntree_limit=xgb_model.best_iteration)[:, 1]\n",
    "#     y_pred_label = (y_pred > 0.5).astype(int)\n",
    "#     auc = roc_auc_score(y_val, y_pred_label)\n",
    "#     metrics.log_metric(\"AUC\", auc)\n",
    "\n",
    "    pred_prb = xgb_model.predict_proba(X_test, ntree_limit=xgb_model.best_iteration)[:, 1]\n",
    "    lg = get_lift(pred_prb, y_test, 10)\n",
    "\n",
    "    # save the model in GCS\n",
    "    from datetime import datetime\n",
    "    models_dict = {}\n",
    "    create_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    models_dict['create_time'] = create_time\n",
    "    models_dict['model'] = xgb_model\n",
    "    models_dict['features'] = features\n",
    "    lg.to_csv('gs://{}/lift_on_scoring_data_{}.csv'.format(file_bucket, create_time, index=False))\n",
    "\n",
    "    with open('model_dict.pkl', 'wb') as handle:\n",
    "        pickle.dump(models_dict, handle)\n",
    "    handle.close()\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(file_bucket)\n",
    "\n",
    "    MODEL_PATH = '{}_xgb_models/'.format(service_type)\n",
    "    blob = bucket.blob(MODEL_PATH)\n",
    "    if not blob.exists(storage_client):\n",
    "        blob.upload_from_string('')\n",
    "\n",
    "    model_name_onbkt = '{}{}_models_xgb_{}'.format(MODEL_PATH, service_type, models_dict['create_time'])\n",
    "    blob = bucket.blob(model_name_onbkt)\n",
    "    blob.upload_from_filename('model_dict.pkl')\n",
    "\n",
    "    print(f\"....model loaded to GCS done at {str(create_time)}\")\n",
    "\n",
    "    time.sleep(120)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815bf58f-4aef-4cb2-9dd4-7684e3d34a29",
   "metadata": {},
   "source": [
    "### pycaret_automl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038e8fcd-7a34-43bf-a338-1d251202b149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "project_id = PROJECT_ID\n",
    "dataset_id = DATASET_ID\n",
    "region = REGION\n",
    "service_type = SERVICE_TYPE\n",
    "bucket_name = BUCKET_NAME\n",
    "file_bucket = FILE_BUCKET\n",
    "score_date_dash= SCORE_DATE_DASH\n",
    "score_date_val_dash= SCORE_DATE_VAL_DASH\n",
    "\n",
    "from pycaret.classification import setup,create_model,tune_model, predict_model,get_config,compare_models,save_model,tune_model\n",
    "\n",
    "df_train = pd.read_csv('gs://{}/{}_train.csv.gz'.format(file_bucket, service_type),\n",
    "                       compression='gzip')  \n",
    "df_test = pd.read_csv('gs://{}/{}_validation.csv.gz'.format(file_bucket, service_type),  \n",
    "                      compression='gzip')\n",
    "\n",
    "def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "    token = os.popen('gcloud auth print-access-token').read()\n",
    "    token = re.sub(f'\\n$', '', token)\n",
    "    credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "    bq_client = bigquery.Client(project=project_id)\n",
    "    if use_local_credential:\n",
    "        bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "    return bq_client\n",
    "\n",
    "client = get_gcp_bqclient(project_id)\n",
    "\n",
    "#set up df_train\n",
    "sql_train = ''' SELECT * FROM `{}.{}.bq_call_to_retention_targets` '''.format(project_id, dataset_id) \n",
    "df_target_train = client.query(sql_train).to_dataframe()\n",
    "df_target_train = df_target_train.loc[\n",
    "    df_target_train['YEAR_MONTH'] == '-'.join(score_date_dash.split('-')[:2])]  # score_date_dash = '2022-08-31'\n",
    "df_target_train['ban'] = df_target_train['ban'].astype('int64')\n",
    "df_target_train = df_target_train.groupby('ban').tail(1)\n",
    "df_train = df_train.merge(df_target_train[['ban', 'target_ind']], on='ban', how='left')\n",
    "df_train.rename(columns={'target_ind': 'target'}, inplace=True)\n",
    "df_train.dropna(subset=['target'], inplace=True)\n",
    "df_train['target'] = df_train['target'].astype(int)\n",
    "print(df_train.shape)\n",
    "\n",
    "#set up df_test\n",
    "sql_test = ''' SELECT * FROM `{}.{}.bq_call_to_retention_targets` '''.format(project_id, dataset_id) \n",
    "df_target_test = client.query(sql_test).to_dataframe()\n",
    "df_target_test = df_target_test.loc[\n",
    "    df_target_test['YEAR_MONTH'] == '-'.join(score_date_val_dash.split('-')[:2])]  # score_date_dash = '2022-09-30'\n",
    "df_target_test['ban'] = df_target_test['ban'].astype('int64')\n",
    "df_target_test = df_target_test.groupby('ban').tail(1)\n",
    "df_test = df_test.merge(df_target_test[['ban', 'target_ind']], on='ban', how='left')\n",
    "df_test.rename(columns={'target_ind': 'target'}, inplace=True)\n",
    "df_test.dropna(subset=['target'], inplace=True)\n",
    "df_test['target'] = df_test['target'].astype(int)\n",
    "print(df_test.shape)\n",
    "\n",
    "#set up features (list)\n",
    "cols_1 = df_train.columns.values\n",
    "cols_2 = df_test.columns.values\n",
    "cols = set(cols_1).intersection(set(cols_2))\n",
    "features = [f for f in cols if f not in ['ban', 'target']]\n",
    "\n",
    "#train test split\n",
    "df_train, df_val = train_test_split(df_train, shuffle=True, test_size=0.3, random_state=42,\n",
    "                                    stratify=df_train['target']\n",
    "                                    )\n",
    "\n",
    "train_sampled = df_train.drop(columns=['ban'], axis=1) \n",
    "valid_sampled = df_val.drop(columns=['ban'], axis=1) \n",
    "test_sampled = df_test.drop(columns=['ban'], axis=1) \n",
    "\n",
    "print(train_sampled.columns) \n",
    "print(test_sampled.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14680646-06dd-484a-af5a-a0b08fbe7b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_sampled.shape) \n",
    "print(valid_sampled.shape) \n",
    "print(test_sampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63996de-487e-424a-9b8c-c0b63e7f681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ban_train = df_train['ban']\n",
    "X_train = df_train[features]\n",
    "y_train = np.squeeze(df_train['target'].values)\n",
    "\n",
    "ban_val = df_val['ban']\n",
    "X_val = df_val[features]\n",
    "y_val = np.squeeze(df_val['target'].values)\n",
    "\n",
    "ban_test = df_test['ban']\n",
    "X_test = df_test[features]\n",
    "y_test = np.squeeze(df_test['target'].values)\n",
    "\n",
    "del df_train, df_val, df_test\n",
    "gc.collect()\n",
    "\n",
    "################################ Pycaret Setup initialize  ############################ \n",
    "classification_setup = setup(data=train_sampled, \n",
    "                         # ignore_features=drop_cols,\n",
    "                         test_data = valid_sampled,\n",
    "                         target='target',\n",
    "                         fix_imbalance=False,\n",
    "                         remove_outliers = True,\n",
    "                         normalize=True,\n",
    "                         normalize_method='zscore',\n",
    "                         log_experiment=False,\n",
    "                         remove_multicollinearity=True,\n",
    "                         multicollinearity_threshold=0.95,\n",
    "                         feature_selection=True,\n",
    "                         fold=5,\n",
    "                         fold_shuffle=True,\n",
    "                         session_id=123,\n",
    "                         numeric_features=numeric_features,\n",
    "                         silent=True)\n",
    "\n",
    "### Pycaret top 3 models to analyze\n",
    "best_model = compare_models(include = ['rf','xgboost','lightgbm','et'],errors='raise', n_select=3)\n",
    "\n",
    "# save the model reports and report fig of all top 2 models to GCS\n",
    "todays_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "save_path = f'pycaret/{todays_date}/'\n",
    "model_reports, model_to_report_map = evaluate_and_save_models(models=best_model.copy(), \n",
    "                                     bucket_name=bucket_name,\n",
    "                                     save_path=save_path, \n",
    "                                     test_df=test_sampled,\n",
    "                                     actual_label_str='target',\n",
    "                                     columns = get_config('X_train').columns,\n",
    "                                     save_columns=True,\n",
    "                                     show_report=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6691a8c1-ffde-417a-9734-d6e2e27a0756",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "    from datetime import datetime\n",
    "    import logging \n",
    "    from pycaret.classification import setup,create_model,tune_model, predict_model,get_config,compare_models,save_model,tune_model\n",
    "    from sklearn.metrics import accuracy_score, roc_auc_score, precision_recall_curve, mean_squared_error, f1_score, precision_score, recall_score, confusion_matrix, roc_curve\n",
    " \n",
    "\n",
    "    ### import data\n",
    "    # CREDENTIALS = google.oauth2.credentials.Credentials(token)\n",
    "    # # import google.oauth2.credentials\n",
    "   \n",
    "    client = bigquery.Client(project=project_id, location='northamerica-northeast1')\n",
    "    storage_client = storage.Client(project=project_id)\n",
    "\n",
    "\n",
    "    # Get utils.py\n",
    "    bucket = storage_client.get_bucket(resources_bucket_name)\n",
    "    blob = bucket.get_blob(f\"{utils_file_path}/{utils_filename}\")\n",
    "    blob.download_to_filename(utils_filename)\n",
    "    blob = bucket.get_blob(f\"{utils_file_path}/{plot_utils_filename}\")\n",
    "    blob.download_to_filename(plot_utils_filename)\n",
    "    \n",
    "    from preprocessing_utils import pre_process_data\n",
    "    from preprocessing_utils import downsampling\n",
    "    from plotly_utils import evaluate_and_save_models,create_folder_if_not_exists,ploty_model_metrics,plotly_feature_importance,plotly_lift_curve, plotly_model_report,plotly_roc, plotly_confusion_matrix,plotly_output_hist,plotly_precision_recall \n",
    "    # specify the path to the training data\n",
    "    training_table = f\"{project_id}.{dataset}.{training_dataset}\"\n",
    "\n",
    "    # generate the query\n",
    "    train_query = '''\n",
    "       SELECT * \n",
    "                FROM `{training_table}`\n",
    "    '''.format(training_table = training_table)\n",
    "   \n",
    "\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "\n",
    "    # create a dataframe with the training data\n",
    "    train_all = client.query(train_query, job_config=job_config).to_dataframe()\n",
    "\n",
    "     ##############  Split train/valid/test based of Dev Training Sample Size   #######################\n",
    "    # training_perc = 0.62\n",
    "    train_df = train_all.sort_values([\"partition_dt\"]).iloc[:int(train_all.shape[0]*training_perc)]\n",
    "\n",
    "\n",
    "    lower_bound = int(train_all.shape[0]*training_perc)\n",
    "    upper_bound = lower_bound + int(train_all.shape[0]*((1-training_perc)/2))\n",
    "    valid_df = train_all.sort_values([\"partition_dt\"]).iloc[lower_bound:upper_bound]\n",
    "\n",
    "    lower_bound = train_df.shape[0] + valid_df.shape[0]\n",
    "    upper_bound = lower_bound + int(train_all.shape[0]*((1-training_perc)/2))\n",
    "    test_df = train_all.sort_values([\"partition_dt\"]).iloc[lower_bound:]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2eea99-a3f0-48ea-8962-115edcaf26d9",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3cda87-dc1f-495e-be67-a3d623c13e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dsl.pipeline(\n",
    "#     # A name for the pipeline.\n",
    "#     name=\"{}-xgb-pipeline\".format(SERVICE_TYPE_NAME),\n",
    "#     description=' pipeline for training {} model'.format(SERVICE_TYPE_NAME)\n",
    "# )\n",
    "def pipeline(\n",
    "        project_id: str = PROJECT_ID,\n",
    "        region: str = REGION,\n",
    "        resource_bucket: str = RESOURCE_BUCKET, \n",
    "        file_bucket: str = FILE_BUCKET\n",
    "    ):\n",
    "    \n",
    "    # ----- create training set --------\n",
    "    bq_create_training_dataset_op = bq_create_dataset(score_date=SCORE_DATE_DASH,\n",
    "                          score_date_delta=SCORE_DATE_DELTA,\n",
    "                          project_id=PROJECT_ID,\n",
    "                          dataset_id=DATASET_ID,\n",
    "                          region=REGION,\n",
    "                          promo_expiry_start=PROMO_EXPIRY_START, \n",
    "                          promo_expiry_end=PROMO_EXPIRY_END, \n",
    "                          v_start_date=SCORE_DATE_MINUS_6_MOS_DASH,\n",
    "                          v_end_date=SCORE_DATE_LAST_MONTH_END_DASH)\n",
    "    \n",
    "    # ----- preprocessing train data --------\n",
    "    preprocess_train_op = preprocess(\n",
    "        pipeline_dataset='bq_ctr_pipeline_dataset', \n",
    "        save_data_path='gs://{}/{}_train.csv.gz'.format(FILE_BUCKET, SERVICE_TYPE),\n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID\n",
    "    )\n",
    "\n",
    "    # preprocess_train_op.set_memory_limit('128G')\n",
    "    # preprocess_train_op.set_cpu_limit('32')\n",
    "\n",
    "    bq_create_training_dataset_op \n",
    "    preprocess_train_op\n",
    "\n",
    "    # ----- create validation set --------\n",
    "    bq_create_validation_dataset_op = bq_create_dataset(score_date=SCORE_DATE_VAL_DASH,\n",
    "                          score_date_delta=SCORE_DATE_VAL_DELTA,\n",
    "                          project_id=PROJECT_ID,\n",
    "                          dataset_id=DATASET_ID,\n",
    "                          region=REGION,\n",
    "                          promo_expiry_start=PROMO_EXPIRY_START_VAL, \n",
    "                          promo_expiry_end=PROMO_EXPIRY_END_VAL, \n",
    "                          v_start_date=SCORE_DATE_VAL_MINUS_6_MOS_DASH,\n",
    "                          v_end_date=SCORE_DATE_VAL_LAST_MONTH_END_DASH)\n",
    "    \n",
    "    # ----- preprocessing validation data --------\n",
    "    preprocess_validation_op = preprocess(\n",
    "        pipeline_dataset='bq_ctr_pipeline_dataset', \n",
    "        save_data_path='gs://{}/{}_validation.csv.gz'.format(FILE_BUCKET, SERVICE_TYPE),\n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID\n",
    "    )\n",
    "\n",
    "    # preprocess_validation_op.set_memory_limit('256G')\n",
    "    # preprocess_validation_op.set_cpu_limit('32')\n",
    "\n",
    "    bq_create_validation_dataset_op\n",
    "    preprocess_validation_op\n",
    "\n",
    "    train_and_save_model_op = train_and_save_model(file_bucket=FILE_BUCKET,\n",
    "                                                   service_type=SERVICE_TYPE,\n",
    "                                                   score_date_dash=SCORE_DATE_DASH,\n",
    "                                                   score_date_val_dash=SCORE_DATE_VAL_DASH,\n",
    "                                                   project_id=PROJECT_ID,\n",
    "                                                   dataset_id=DATASET_ID,\n",
    "                                                   )\n",
    "    \n",
    "    train_and_save_model_op\n",
    "    \n",
    "#     train_and_save_model_op.set_memory_limit('256G')\n",
    "#     train_and_save_model_op.set_cpu_limit('32')\n",
    "\n",
    "#     train_and_save_model_op.after(preprocess_train_op)\n",
    "#     train_and_save_model_op.after(preprocess_validation_op)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2a51c8-4c37-4392-a21a-c4c3279d36d4",
   "metadata": {},
   "source": [
    "### Run the Pipeline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5313e5-b5cd-4a78-9df5-91af24168e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline(project_id = PROJECT_ID,\n",
    "#         region = REGION,\n",
    "#         resource_bucket = RESOURCE_BUCKET,\n",
    "#         file_bucket = FILE_BUCKET)\n",
    "\n",
    "\n",
    "pipeline(project_id = PROJECT_ID,\n",
    "        region = REGION,\n",
    "        resource_bucket = RESOURCE_BUCKET, \n",
    "        file_bucket = FILE_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27465a63-5c3b-4e96-9c08-f9cc5dafde90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kfp.v2 import compiler\n",
    "# from google.cloud.aiplatform import pipeline_jobs\n",
    "\n",
    "# import json\n",
    "\n",
    "# compiler.Compiler().compile(\n",
    "#    pipeline_func=pipeline, package_path=\"pipeline.json\"\n",
    "# )\n",
    "\n",
    "# job = pipeline_jobs.PipelineJob(\n",
    "#                                display_name=PIPELINE_NAME,\n",
    "#                                template_path=\"pipeline.json\",\n",
    "#                                location=REGION,\n",
    "#                                enable_caching=False,\n",
    "#                                pipeline_root = f\"gs://{RESOURCE_BUCKET}\"\n",
    "# )\n",
    "# job.run(\n",
    "#    service_account = f\"bilayer-sa@{PROJECT_ID}.iam.gserviceaccount.com\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382c8f46-4d79-423d-8a73-4fc55432b905",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
