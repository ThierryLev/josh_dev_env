{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c150da3-6e1e-4f02-a778-bb3b12af9054",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b559b7e6-fe51-49cc-8e9c-832380843832",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output, OutputPath, ClassificationMetrics,\n",
    "                        Metrics, component)\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import google\n",
    "from google.oauth2 import credentials\n",
    "from google.oauth2 import service_account\n",
    "from google.oauth2.service_account import Credentials\n",
    "from google.cloud import storage\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google_cloud_pipeline_components.v1.batch_predict_job import \\\n",
    "    ModelBatchPredictOp as batch_prediction_op\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558ff533-70c8-4421-813e-c567d6bc496b",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f70d721-76e7-4c2e-8153-88b5bbe8ee47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tag cell with parameters\n",
    "PROJECT_ID =  ''\n",
    "BUCKET_NAME=''\n",
    "DATASET_ID = ''\n",
    "RESOURCE_BUCKET = ''\n",
    "FILE_BUCKET = ''\n",
    "REGION = ''\n",
    "MODEL_ID = '5090'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7035200a-de6b-412c-8406-64854b639cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag cell with parameters\n",
    "PROJECT_ID =  'divg-josh-pr-d1cc3a'\n",
    "BUCKET_NAME='divg-josh-pr-d1cc3a-default'\n",
    "DATASET_ID = 'call_to_retention_dataset'\n",
    "RESOURCE_BUCKET = 'divg-josh-pr-d1cc3a-default'\n",
    "FILE_BUCKET = 'divg-josh-pr-d1cc3a-default'\n",
    "MODEL_ID = '5090'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eeed21-e12b-45ee-b1e4-d0c8f3843533",
   "metadata": {},
   "source": [
    "### Service Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65d1668-e1d8-4f58-958f-edc44d06a6b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SERVICE_TYPE = 'call_to_retention'\n",
    "SERVICE_TYPE_NAME = 'call-to-retention'\n",
    "TABLE_ID = 'bq_call_to_retention_targets'\n",
    "REGION = \"northamerica-northeast1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2229e5d1-1362-40ed-aa7d-cb22e41e4960",
   "metadata": {},
   "source": [
    "### Pulumi Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d628b-e759-4c9c-aab5-568c54721aaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "STACK_NAME = 'call_to_retention'\n",
    "TRAIN_PIPELINE_NAME_PATH = 'train_pipeline'\n",
    "PREDICT_PIPELINE_NAME_PATH = 'predict_pipeline'\n",
    "TRAIN_PIPELINE_NAME = 'call-to-retention-train-pipeline' # Same name as pulumi.yaml\n",
    "PREDICT_PIPELINE_NAME = 'call-to-retention-predict-pipeline' # Same name as pulumi.yaml\n",
    "TRAIN_PIPELINE_DESCRIPTION = 'call-to-retention-train-pipeline'\n",
    "PREDICT_PIPELINE_DESCRIPTION = 'call-to-retention-predict-pipeline'\n",
    "REGION = \"northamerica-northeast1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85558b11-be96-49ea-91c5-d17b15c00bf1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Query + Pre-Processing Component Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370852f7-2e23-4ac3-8cd8-b40c39be85ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_QUERIES_PATH = f\"{STACK_NAME}/{TRAIN_PIPELINE_NAME_PATH}/queries/\" \n",
    "TRAIN_UTILS_FILE_PATH = f\"{STACK_NAME}/{TRAIN_PIPELINE_NAME_PATH}/utils\" \n",
    "UTILS_FILENAME = 'utils.py'\n",
    "\n",
    "PROCESSED_SERVING_DATA_TABLENAME = 'processed_serving_data'\n",
    "INPUT_SERVING_DATA_TABLENAME = 'input_serving_data'\n",
    "\n",
    "QUERY_DATE = (date.today() - relativedelta(days=1)).strftime('%Y-%m-%d')\n",
    "TARGET_TABLE_REF = '{}.{}.{}'.format(PROJECT_ID, DATASET_ID, TABLE_ID)\n",
    "\n",
    "QUERIES_PATH = 'call_to_retention/queries/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc0bd98-1d58-491e-bc5e-bbac378c7bbf",
   "metadata": {},
   "source": [
    "### Import Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f442904b-a40a-4ba8-b237-e9894ffd4f26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download required component files to local\n",
    "prefix = f'{STACK_NAME}/{TRAIN_PIPELINE_NAME_PATH}/components/'\n",
    "dl_dir = 'components/'\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(RESOURCE_BUCKET)\n",
    "blobs = bucket.list_blobs(prefix=prefix)  # Get list of files\n",
    "for blob in blobs: # download each file that starts with \"prefix\" into \"dl_dir\"\n",
    "    if blob.name.endswith(\"/\"):\n",
    "        continue\n",
    "    file_split = blob.name.split(prefix)\n",
    "    file_path = f\"{dl_dir}{file_split[-1]}\"\n",
    "    directory = \"/\".join(file_path.split(\"/\")[0:-1])\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    blob.download_to_filename(file_path) \n",
    "\n",
    "# import main pipeline components\n",
    "import components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c315d454-416f-4eed-b12d-1acd5aed1207",
   "metadata": {},
   "source": [
    "### Date Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfad310-a482-4285-aa31-6d9113008be6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scoringDate = date.today() - relativedelta(days=3)\n",
    "\n",
    "# training dates\n",
    "SCORE_DATE = scoringDate.strftime('%Y%m%d')  # date.today().strftime('%Y%m%d')\n",
    "SCORE_DATE_DASH = scoringDate.strftime('%Y-%m-%d')\n",
    "SCORE_DATE_MINUS_6_MOS_DASH = ((scoringDate - relativedelta(months=6)).replace(day=1)).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_LAST_MONTH_START_DASH = (scoringDate.replace(day=1) - timedelta(days=1)).replace(day=1).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_LAST_MONTH_END_DASH = ((scoringDate.replace(day=1)) - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "#revert these changes after 2023-05-30\n",
    "PROMO_EXPIRY_START = (scoringDate.replace(day=1) + relativedelta(months=4)).replace(day=1).strftime('%Y-%m-%d')\n",
    "PROMO_EXPIRY_END = (scoringDate.replace(day=1) + relativedelta(months=5)).replace(day=1).strftime('%Y-%m-%d')\n",
    "\n",
    "SCORE_DATE_DELTA = 0\n",
    "SCORE_DATE_VAL_DELTA = 0\n",
    "TICKET_DATE_WINDOW = 30  # Days of ticket data to be queried\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc7d16d-2625-4133-927c-671fd35b9877",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMO_EXPIRY_START"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2ddb09-0a15-47ca-a969-3b2282eaa071",
   "metadata": {},
   "source": [
    "### bq_create_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d577db47-9757-4e82-b2e6-3b6bc21a1111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kfp\n",
    "# from kfp import dsl\n",
    "# # from kfp.v2.dsl import (Model, Input, component)\n",
    "# from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,HTML,\n",
    "#                         OutputPath, ClassificationMetrics, Metrics, component)\n",
    "# from typing import NamedTuple\n",
    "# # Create Training Dataset for training pipeline\n",
    "# @component(\n",
    "#     base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/wb-platform/pipelines/kubeflow-pycaret:latest\",\n",
    "#     output_component_file=\"bq_create_dataset.yaml\",\n",
    "# )\n",
    "def bq_create_dataset(score_date: str,\n",
    "                      score_date_delta: int,\n",
    "                      project_id: str,\n",
    "                      dataset_id: str,\n",
    "                      region: str,\n",
    "                      promo_expiry_start: str, \n",
    "                      promo_expiry_end: str, \n",
    "                      v_start_date: str,\n",
    "                      v_end_date: str):\n",
    "# -> NamedTuple(\"output\", [(\"col_list\", list)])\n",
    " \n",
    "    from google.cloud import bigquery\n",
    "    import logging \n",
    "    from datetime import datetime\n",
    "    # For wb\n",
    "    # import google.oauth2.credentials\n",
    "    # CREDENTIALS = google.oauth2.credentials.Credentials(token)\n",
    "    \n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    client = get_gcp_bqclient(project_id)\n",
    "    # client = bigquery.Client(project=project_id, location=region)\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    \n",
    "    # Change dataset / table + sp table name to version in bi-layer\n",
    "    query =\\\n",
    "        f'''\n",
    "            DECLARE score_date DATE DEFAULT \"{score_date}\";\n",
    "            DECLARE promo_expiry_start DATE DEFAULT \"{promo_expiry_start}\";\n",
    "            DECLARE promo_expiry_end DATE DEFAULT \"{promo_expiry_end}\";\n",
    "            DECLARE start_date DATE DEFAULT \"{v_start_date}\";\n",
    "            DECLARE end_date DATE DEFAULT \"{v_end_date}\";\n",
    "        \n",
    "            -- Change dataset / sp name to the version in the bi_layer\n",
    "            CALL {dataset_id}.bq_sp_ctr_pipeline_dataset(score_date, promo_expiry_start, promo_expiry_end, start_date, end_date);\n",
    "\n",
    "            SELECT\n",
    "                *\n",
    "            FROM {dataset_id}.INFORMATION_SCHEMA.PARTITIONS\n",
    "            WHERE table_name='bq_ctr_pipeline_dataset'\n",
    "            \n",
    "        '''\n",
    "    \n",
    "    df = client.query(query, job_config=job_config).to_dataframe()\n",
    "    logging.info(df.to_string())\n",
    "    \n",
    "    logging.info(f\"Loaded {df.total_rows[0]} rows into \\\n",
    "             {df.table_catalog[0]}.{df.table_schema[0]}.{df.table_name[0]} on \\\n",
    "             {datetime.strftime((df.last_modified_time[0]), '%Y-%m-%d %H:%M:%S') } !\")\n",
    "    \n",
    "    ######################################## Save column list_##########################\n",
    "    query =\\\n",
    "        f'''\n",
    "           SELECT\n",
    "                *\n",
    "            FROM {dataset_id}.bq_ctr_pipeline_dataset\n",
    "\n",
    "        '''\n",
    "    \n",
    "    df = client.query(query, job_config=job_config).to_dataframe()\n",
    "    \n",
    "    col_list = list([col for col in df.columns])\n",
    "    return (col_list,)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281c6676-70b0-41a6-b860-aa6d95fec0da",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8623f9-e2fd-406a-b0a7-2ea7c42af66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "        pipeline_dataset: str, \n",
    "        save_data_path: str,\n",
    "        project_id: str,\n",
    "        dataset_id: str\n",
    "):\n",
    "    from google.cloud import bigquery\n",
    "    import pandas as pd\n",
    "    import gc\n",
    "    import time\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    client = get_gcp_bqclient(project_id)\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "\n",
    "    # pipeline_dataset \n",
    "    pipeline_dataset_name = f\"{project_id}.{dataset_id}.{pipeline_dataset}\" \n",
    "    build_df_pipeline_dataset = f'SELECT * FROM `{pipeline_dataset_name}`'\n",
    "    df_pipeline_dataset = client.query(build_df_pipeline_dataset).to_dataframe()\n",
    "    df_pipeline_dataset = df_pipeline_dataset.set_index('ban') \n",
    "\n",
    "    # demo columns\n",
    "    df_pipeline_dataset['demo_urban_flag'] = df_pipeline_dataset.demo_sgname.str.lower().str.contains('urban').fillna(0).astype(int)\n",
    "    df_pipeline_dataset['demo_rural_flag'] = df_pipeline_dataset.demo_sgname.str.lower().str.contains('rural').fillna(0).astype(int)\n",
    "    df_pipeline_dataset['demo_family_flag'] = df_pipeline_dataset.demo_lsname.str.lower().str.contains('families').fillna(0).astype(int)\n",
    "\n",
    "    df_income_dummies = pd.get_dummies(df_pipeline_dataset[['demo_lsname']]) \n",
    "    df_income_dummies.columns = df_income_dummies.columns.str.replace('&', 'and')\n",
    "    df_income_dummies.columns = df_income_dummies.columns.str.replace(' ', '_')\n",
    "\n",
    "    df_pipeline_dataset.drop(columns=['demo_sgname', 'demo_lsname'], axis=1, inplace=True)\n",
    "\n",
    "    df_pipeline_dataset = df_pipeline_dataset.join(df_income_dummies)\n",
    "\n",
    "    df_join = df_pipeline_dataset.copy()\n",
    "\n",
    "    #column name clean-up\n",
    "    df_join.columns = df_join.columns.str.replace(' ', '_')\n",
    "    df_join.columns = df_join.columns.str.replace('-', '_')\n",
    "\n",
    "    df_join.head()\n",
    "\n",
    "    #df_final\n",
    "    df_final = df_join.copy()\n",
    "    del df_join\n",
    "    gc.collect()\n",
    "    print('......df_final done')\n",
    "\n",
    "    for f in df_final.columns:\n",
    "        df_final[f] = list(df_final[f])\n",
    "\n",
    "    df_final.to_csv(save_data_path, index=True, compression='gzip') \n",
    "    del df_final\n",
    "    gc.collect()\n",
    "    print(f'......csv saved in {save_data_path}')\n",
    "    time.sleep(120)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330396ff-dfe9-4c23-9d67-922e9a190bdc",
   "metadata": {},
   "source": [
    "### Batch Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f33719-127b-45c7-bccd-dcd3970b003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_prediction(\n",
    "        project_id: str,\n",
    "        dataset_id: str,\n",
    "        file_bucket: str,\n",
    "        service_type: str,\n",
    "        score_table: str,\n",
    "        score_date_dash: str\n",
    "):\n",
    "    import time\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    from datetime import date\n",
    "    from dateutil.relativedelta import relativedelta\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    MODEL_ID = '5090'\n",
    "    \n",
    "    def if_tbl_exists(bq_client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            bq_client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "\n",
    "    def upsert_table(project_id, dataset_id, table_id, sql, result):\n",
    "        new_values = ',\\n'.join(result.apply(lambda row: row_format(row), axis=1))\n",
    "        new_sql = sql.format(proj_id=project_id, dataset_id=dataset_id, table_id=table_id,\n",
    "                             new_values=new_values)\n",
    "\n",
    "        def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "            token = os.popen('gcloud auth print-access-token').read()\n",
    "            token = re.sub(f'\\n$', '', token)\n",
    "            credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "            bq_client = bigquery.Client(project=project_id)\n",
    "            if use_local_credential:\n",
    "                bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "            return bq_client\n",
    "\n",
    "        bq_client = get_gcp_bqclient(project_id)\n",
    "\n",
    "        # bq_client = bigquery.Client(project=project_id)\n",
    "        \n",
    "        code = bq_client.query(new_sql)\n",
    "        time.sleep(5)\n",
    "\n",
    "    def row_format(row):\n",
    "        values = row.values\n",
    "        new_values = \"\"\n",
    "        v = str(values[0]) if not pd.isnull(values[0]) else 'NULL'\n",
    "        if 'str' in str(type(values[0])):\n",
    "            new_values += f\"'{v}'\"\n",
    "        else:\n",
    "            new_values += f\"{v}\"\n",
    "\n",
    "        for i in range(1, len(values)):\n",
    "            v = str(values[i]) if not pd.isnull(values[i]) else 'NULL'\n",
    "            if 'str' in str(type(values[i])):\n",
    "                new_values += f\",'{v}'\"\n",
    "            else:\n",
    "                new_values += f\",{v}\"\n",
    "        return '(' + new_values + ')'\n",
    "\n",
    "    def generate_sql_file(ll):\n",
    "        s = 'MERGE INTO `{proj_id}.{dataset_id}.{table_id}` a'\n",
    "        s += \" USING UNNEST(\"\n",
    "        s += \"[struct<\"\n",
    "        for i in range(len(ll) - 1):\n",
    "            v = ll[i]\n",
    "            s += \"{} {},\".format(v[0], v[1])\n",
    "        s += \"{} {}\".format(ll[-1][0], ll[-1][1])\n",
    "        s += \">{new_values}]\"\n",
    "        s += \") b\"\n",
    "        s += \" ON a.ban = b.ban and a.score_date = b.score_date\"\n",
    "        s += \" WHEN MATCHED THEN\"\n",
    "        s += \" UPDATE SET \"\n",
    "        s += \"a.{}=b.{},\".format(ll[0][0], ll[0][0])\n",
    "        for i in range(1, len(ll) - 1):\n",
    "            v = ll[i]\n",
    "            s += \"a.{}=b.{},\".format(v[0], v[0])\n",
    "        s += \"a.{}=b.{}\".format(ll[-1][0], ll[-1][0])\n",
    "        s += \" WHEN NOT MATCHED THEN\"\n",
    "        s += \" INSERT(\"\n",
    "        for i in range(len(ll) - 1):\n",
    "            v = ll[i]\n",
    "            s += \"{},\".format(v[0])\n",
    "        s += \"{})\".format(ll[-1][0])\n",
    "        s += \" VALUES(\"\n",
    "        for i in range(len(ll) - 1):\n",
    "            s += \"b.{},\".format(ll[i][0])\n",
    "        s += \"b.{}\".format(ll[-1][0])\n",
    "        s += \")\"\n",
    "\n",
    "        return s\n",
    "\n",
    "    MODEL_PATH = '{}_xgb_models/'.format(service_type)\n",
    "    df_score = pd.read_csv('gs://{}/{}_score.csv.gz'.format(file_bucket, service_type), compression='gzip')\n",
    "    df_score.dropna(subset=['ban'], inplace=True)\n",
    "    df_score.reset_index(drop=True, inplace=True)\n",
    "    print('......scoring data loaded:{}'.format(df_score.shape))\n",
    "    time.sleep(10)\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(file_bucket)\n",
    "    blobs = storage_client.list_blobs(file_bucket, prefix='{}{}_models_xgb_'.format(MODEL_PATH, service_type))\n",
    "\n",
    "    model_lists = []\n",
    "    for blob in blobs:\n",
    "        model_lists.append(blob.name)\n",
    "\n",
    "    blob = bucket.blob(model_lists[-1])\n",
    "    blob_in = blob.download_as_string()\n",
    "    model_dict = pickle.loads(blob_in)\n",
    "    model_xgb = model_dict['model']\n",
    "    features = model_dict['features']\n",
    "    print('...... model loaded')\n",
    "    time.sleep(10)\n",
    "\n",
    "    ll = [('ban', 'string'), ('score_date', 'string'), ('model_id', 'string'), ('score', 'float64')]\n",
    "    sql = generate_sql_file(ll)\n",
    "\n",
    "    df_score['ban'] = df_score['ban'].astype(int)\n",
    "    print('.... scoring for {} promo expiry bans base'.format(len(df_score)))\n",
    "\n",
    "    # get full score to cave into bucket\n",
    "    pred_prob = model_xgb.predict_proba(df_score[features], ntree_limit=model_xgb.best_iteration)[:, 1]\n",
    "    result = pd.DataFrame(columns=['ban', 'score_date', 'model_id', 'score'])\n",
    "    result['score'] = list(pred_prob)\n",
    "    result['score'] = result['score'].fillna(0.0).astype('float64')\n",
    "    result['ban'] = list(df_score['ban'])\n",
    "    result['ban'] = result['ban'].astype('str')\n",
    "    result['score_date'] = score_date_dash\n",
    "    result['model_id'] = MODEL_ID\n",
    "\n",
    "    result.to_csv('gs://{}/ucar/{}_prediction.csv.gz'.format(file_bucket, service_type), compression='gzip',\n",
    "                  index=False)\n",
    "    time.sleep(60)\n",
    "\n",
    "    batch_size = 1000\n",
    "    n_batchs = int(df_score.shape[0] / batch_size) + 1\n",
    "    print('...... will upsert {} batches'.format(n_batchs))\n",
    "\n",
    "    # start batch prediction\n",
    "    all_scores = np.array(result['score'].values)\n",
    "    for i in range(n_batchs):\n",
    "    \n",
    "        s, e = i * batch_size, (i + 1) * batch_size\n",
    "        if e >= df_score.shape[0]:\n",
    "            e = df_score.shape[0]\n",
    "\n",
    "        df_temp = df_score.iloc[s:e]\n",
    "        pred_prob = all_scores[s:e]\n",
    "        batch_result = pd.DataFrame(columns=['ban', 'score_date', 'model_id', 'score'])\n",
    "        batch_result['score'] = list(pred_prob)\n",
    "        batch_result['score'] = batch_result['score'].fillna(0.0).astype('float64')\n",
    "        batch_result['ban'] = list(df_temp['ban'])\n",
    "        batch_result['ban'] = batch_result['ban'].astype('str')\n",
    "        batch_result['score_date'] = score_date_dash\n",
    "        batch_result['model_id'] = MODEL_ID\n",
    "\n",
    "        upsert_table(project_id,\n",
    "                     dataset_id,\n",
    "                     score_table,\n",
    "                     sql,\n",
    "                     batch_result,\n",
    "                     )\n",
    "        if i % 20 == 0:\n",
    "            print('predict for batch {} done'.format(i), end=' ')\n",
    "\n",
    "    time.sleep(120)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1b7f3a-31a7-4737-a763-89a6356648cc",
   "metadata": {},
   "source": [
    "### Post Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e403c5-a7b0-401d-b602-81dff0746149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(\n",
    "        project_id: str,\n",
    "        file_bucket: str,\n",
    "        service_type: str,\n",
    "        score_date_dash: str,\n",
    "):\n",
    "    import time\n",
    "    import pandas as pd\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    MODEL_ID = '5090'\n",
    "    file_name = 'gs://{}/ucar/{}_prediction.csv.gz'.format(file_bucket, service_type)\n",
    "    df_orig = pd.read_csv(file_name, compression='gzip')\n",
    "    df_orig.dropna(subset=['ban'], inplace=True)\n",
    "    df_orig.reset_index(drop=True, inplace=True)\n",
    "    df_orig['scoring_date'] = score_date_dash\n",
    "    df_orig.ban = df_orig.ban.astype(int)\n",
    "    df_orig = df_orig.rename(columns={'ban': 'bus_bacct_num', 'score': 'score_num'})\n",
    "    df_orig.score_num = df_orig.score_num.astype(float)\n",
    "    df_orig['decile_grp_num'] = pd.qcut(df_orig['score_num'], q=10, labels=[i for i in range(10, 0, -1)])\n",
    "    df_orig['percentile_pct'] = df_orig.score_num.rank(pct=True)\n",
    "    df_orig['predict_model_nm'] = 'FFH Call To Retention Model - DIVG'\n",
    "    df_orig['model_type_cd'] = 'FFH'\n",
    "    df_orig['subscriber_no'] = \"\"\n",
    "    df_orig['prod_instnc_resrc_str'] = \"\"\n",
    "    df_orig['service_instnc_id'] = \"\"\n",
    "    df_orig['segment_nm'] = \"\"\n",
    "    df_orig['segment_id'] = \"\"\n",
    "    df_orig['classn_nm'] = \"\"\n",
    "    df_orig['predict_model_id'] = MODEL_ID\n",
    "    df_orig.drop(columns=['model_id', 'score_date'], axis=1, inplace=True)\n",
    "\n",
    "    get_cust_id = \"\"\"\n",
    "    WITH bq_snpsht_max_date AS(\n",
    "    SELECT PARSE_DATE('%Y%m%d', MAX(partition_id)) AS max_date\n",
    "        FROM `cio-datahub-enterprise-pr-183a.ent_cust_cust.INFORMATION_SCHEMA.PARTITIONS` \n",
    "    WHERE table_name = 'bq_prod_instnc_snpsht' \n",
    "        AND partition_id <> '__NULL__'\n",
    "    ),\n",
    "    -- BANs can have multiple Cust ID. Create rank by product type and status, prioritizing ban/cust id with active FFH products\n",
    "    rank_prod_type AS (\n",
    "    SELECT DISTINCT\n",
    "        bacct_bus_bacct_num,\n",
    "        consldt_cust_bus_cust_id AS cust_id,\n",
    "        CASE WHEN pi_prod_instnc_resrc_typ_cd IN ('SING', 'HSIC', 'TTV', 'SMHM', 'STV', 'DIIC') AND pi_prod_instnc_stat_cd = 'A' THEN 1\n",
    "                WHEN pi_prod_instnc_resrc_typ_cd IN ('SING', 'HSIC', 'TTV', 'SMHM', 'STV', 'DIIC') THEN 2\n",
    "                WHEN pi_prod_instnc_stat_cd = 'A' THEN 3\n",
    "                ELSE 4\n",
    "                END AS prod_rank\n",
    "    FROM `cio-datahub-enterprise-pr-183a.ent_cust_cust.bq_prod_instnc_snpsht`\n",
    "    CROSS JOIN bq_snpsht_max_date\n",
    "    WHERE CAST(prod_instnc_ts AS DATE)=bq_snpsht_max_date.max_date\n",
    "    AND bus_prod_instnc_src_id = 1001\n",
    "    ),\n",
    "    --Rank Cust ID\n",
    "    rank_cust_id AS (\n",
    "    SELECT DISTINCT\n",
    "        bacct_bus_bacct_num,\n",
    "        cust_id,\n",
    "        RANK() OVER(PARTITION BY bacct_bus_bacct_num\n",
    "                        ORDER BY prod_rank,\n",
    "                                    cust_id) AS cust_id_rank               \n",
    "    FROM rank_prod_type\n",
    "    )\n",
    "    --Select best cust id\n",
    "    SELECT bacct_bus_bacct_num,\n",
    "        cust_id\n",
    "    FROM rank_cust_id\n",
    "    WHERE cust_id_rank = 1\n",
    "    \"\"\"\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    client = get_gcp_bqclient(project_id)\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    \n",
    "    # client = bigquery.Client(project=project_id)\n",
    "    df_cust = client.query(get_cust_id).to_dataframe()\n",
    "    df_final = df_orig.set_index('bus_bacct_num').join(df_cust.set_index('bacct_bus_bacct_num')).reset_index()\n",
    "    df_final = df_final.rename(columns={'index': 'bus_bacct_num', 'cust_bus_cust_id': 'cust_id'})\n",
    "    df_final = df_final.sort_values(by=['score_num'], ascending=False)\n",
    "    df_final.to_csv(file_name, compression='gzip', index=False)\n",
    "    time.sleep(300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2eea99-a3f0-48ea-8962-115edcaf26d9",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3cda87-dc1f-495e-be67-a3d623c13e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @dsl.pipeline(\n",
    "#     # A name for the pipeline.\n",
    "#     name=\"{}-xgb-pipeline\".format(SERVICE_TYPE_NAME),\n",
    "#     description=' pipeline for training {} model'.format(SERVICE_TYPE_NAME)\n",
    "# )\n",
    "def pipeline(\n",
    "        project_id: str = PROJECT_ID,\n",
    "        region: str = REGION,\n",
    "        resource_bucket: str = RESOURCE_BUCKET,\n",
    "        file_bucket: str = FILE_BUCKET\n",
    "    ):\n",
    "    \n",
    "    # ----- create training set --------\n",
    "    bq_create_scoring_dataset_op = bq_create_dataset(score_date=SCORE_DATE_DASH,\n",
    "                          score_date_delta=SCORE_DATE_DELTA,\n",
    "                          project_id=PROJECT_ID,\n",
    "                          dataset_id=DATASET_ID,\n",
    "                          region=REGION,\n",
    "                          promo_expiry_start=PROMO_EXPIRY_START, \n",
    "                          promo_expiry_end=PROMO_EXPIRY_END, \n",
    "                          v_start_date=SCORE_DATE_MINUS_6_MOS_DASH,\n",
    "                          v_end_date=SCORE_DATE_LAST_MONTH_END_DASH)\n",
    "    \n",
    "    # ----- preprocessing train data --------\n",
    "    preprocess_scoring_op = preprocess(\n",
    "        pipeline_dataset='bq_ctr_pipeline_dataset', \n",
    "        save_data_path='gs://{}/{}_score.csv.gz'.format(FILE_BUCKET, SERVICE_TYPE),\n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID\n",
    "    )\n",
    "\n",
    "    # preprocess_train_op.set_memory_limit('128G')\n",
    "    # preprocess_train_op.set_cpu_limit('32')\n",
    "\n",
    "    bq_create_scoring_dataset_op \n",
    "    preprocess_scoring_op\n",
    "\n",
    "    batch_prediction_op = batch_prediction(\n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID,\n",
    "        file_bucket=FILE_BUCKET,\n",
    "        service_type=SERVICE_TYPE,\n",
    "        score_date_dash=SCORE_DATE_DASH,\n",
    "        score_table='bq_call_to_retention_scores',\n",
    "    )\n",
    "    # batch_prediction_op.set_memory_limit('32G')\n",
    "    # batch_prediction_op.set_cpu_limit('4')\n",
    "\n",
    "    batch_prediction_op\n",
    "\n",
    "    postprocessing_op = postprocess(\n",
    "        project_id=PROJECT_ID,\n",
    "        file_bucket=FILE_BUCKET,\n",
    "        service_type=SERVICE_TYPE,\n",
    "        score_date_dash=SCORE_DATE_DASH,\n",
    "    )\n",
    "    # postprocessing_op.set_memory_limit('16G')\n",
    "    # postprocessing_op.set_cpu_limit('4')\n",
    "\n",
    "    postprocessing_op\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2a51c8-4c37-4392-a21a-c4c3279d36d4",
   "metadata": {},
   "source": [
    "### Run the Pipeline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5313e5-b5cd-4a78-9df5-91af24168e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline(project_id = PROJECT_ID,\n",
    "#         region = REGION,\n",
    "#         resource_bucket = RESOURCE_BUCKET,\n",
    "#         file_bucket = FILE_BUCKET)\n",
    "\n",
    "\n",
    "pipeline(project_id = PROJECT_ID,\n",
    "        region = REGION,\n",
    "        resource_bucket = RESOURCE_BUCKET, \n",
    "        file_bucket = FILE_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27465a63-5c3b-4e96-9c08-f9cc5dafde90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kfp.v2 import compiler\n",
    "# from google.cloud.aiplatform import pipeline_jobs\n",
    "\n",
    "# import json\n",
    "\n",
    "# compiler.Compiler().compile(\n",
    "#    pipeline_func=pipeline, package_path=\"pipeline.json\"\n",
    "# )\n",
    "\n",
    "# job = pipeline_jobs.PipelineJob(\n",
    "#                                display_name=PIPELINE_NAME,\n",
    "#                                template_path=\"pipeline.json\",\n",
    "#                                location=REGION,\n",
    "#                                enable_caching=False,\n",
    "#                                pipeline_root = f\"gs://{RESOURCE_BUCKET}\"\n",
    "# )\n",
    "# job.run(\n",
    "#    service_account = f\"bilayer-sa@{PROJECT_ID}.iam.gserviceaccount.com\"\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
