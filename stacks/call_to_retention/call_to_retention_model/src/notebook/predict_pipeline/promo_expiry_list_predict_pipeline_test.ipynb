{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c150da3-6e1e-4f02-a778-bb3b12af9054",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b559b7e6-fe51-49cc-8e9c-832380843832",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output, OutputPath, ClassificationMetrics,\n",
    "                        Metrics, component)\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import google\n",
    "from google.oauth2 import credentials\n",
    "from google.oauth2 import service_account\n",
    "from google.oauth2.service_account import Credentials\n",
    "from google.cloud import storage\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google_cloud_pipeline_components.v1.batch_predict_job import \\\n",
    "    ModelBatchPredictOp as batch_prediction_op\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558ff533-70c8-4421-813e-c567d6bc496b",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f70d721-76e7-4c2e-8153-88b5bbe8ee47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tag cell with parameters\n",
    "PROJECT_ID =  ''\n",
    "BUCKET_NAME=''\n",
    "DATASET_ID = ''\n",
    "RESOURCE_BUCKET = ''\n",
    "FILE_BUCKET = ''\n",
    "REGION = ''\n",
    "MODEL_ID = '5090'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7035200a-de6b-412c-8406-64854b639cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag cell with parameters\n",
    "PROJECT_ID =  'divg-josh-pr-d1cc3a'\n",
    "BUCKET_NAME='divg-josh-pr-d1cc3a-default'\n",
    "DATASET_ID = 'call_to_retention_dataset'\n",
    "RESOURCE_BUCKET = 'divg-josh-pr-d1cc3a-default'\n",
    "FILE_BUCKET = 'divg-josh-pr-d1cc3a-default'\n",
    "MODEL_ID = '5090'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eeed21-e12b-45ee-b1e4-d0c8f3843533",
   "metadata": {},
   "source": [
    "### Service Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65d1668-e1d8-4f58-958f-edc44d06a6b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SERVICE_TYPE = 'call_to_retention'\n",
    "SERVICE_TYPE_NAME = 'call-to-retention'\n",
    "TABLE_ID = 'bq_call_to_retention_targets'\n",
    "REGION = \"northamerica-northeast1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2229e5d1-1362-40ed-aa7d-cb22e41e4960",
   "metadata": {},
   "source": [
    "### Pulumi Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d628b-e759-4c9c-aab5-568c54721aaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "STACK_NAME = 'call_to_retention'\n",
    "TRAIN_PIPELINE_NAME_PATH = 'train_pipeline'\n",
    "PREDICT_PIPELINE_NAME_PATH = 'predict_pipeline'\n",
    "TRAIN_PIPELINE_NAME = 'call-to-retention-train-pipeline' # Same name as pulumi.yaml\n",
    "PREDICT_PIPELINE_NAME = 'call-to-retention-predict-pipeline' # Same name as pulumi.yaml\n",
    "TRAIN_PIPELINE_DESCRIPTION = 'call-to-retention-train-pipeline'\n",
    "PREDICT_PIPELINE_DESCRIPTION = 'call-to-retention-predict-pipeline'\n",
    "REGION = \"northamerica-northeast1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85558b11-be96-49ea-91c5-d17b15c00bf1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Query + Pre-Processing Component Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370852f7-2e23-4ac3-8cd8-b40c39be85ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_QUERIES_PATH = f\"{STACK_NAME}/{TRAIN_PIPELINE_NAME_PATH}/queries/\" \n",
    "TRAIN_UTILS_FILE_PATH = f\"{STACK_NAME}/{TRAIN_PIPELINE_NAME_PATH}/utils\" \n",
    "UTILS_FILENAME = 'utils.py'\n",
    "\n",
    "PROCESSED_SERVING_DATA_TABLENAME = 'processed_serving_data'\n",
    "INPUT_SERVING_DATA_TABLENAME = 'input_serving_data'\n",
    "\n",
    "QUERY_DATE = (date.today() - relativedelta(days=1)).strftime('%Y-%m-%d')\n",
    "TARGET_TABLE_REF = '{}.{}.{}'.format(PROJECT_ID, DATASET_ID, TABLE_ID)\n",
    "\n",
    "QUERIES_PATH = 'call_to_retention/queries/'\n",
    "\n",
    "#Query Paths\n",
    "ACCOUNT_PROMO_EXPIRY_LIST_QUERY_PATH = QUERIES_PATH + 'create_input_account_promo_expiry_list_query.sql'\n",
    "ACCOUNT_CONSL_QUERY_PATH = QUERIES_PATH + 'create_input_account_consl_query.sql'\n",
    "ACCOUNT_FFH_BILLING_QUERY_PATH = QUERIES_PATH + 'create_input_account_ffh_billing_query.sql'\n",
    "ACCOUNT_FFH_DISCOUNTS_QUERY_PATH = QUERIES_PATH + 'create_input_account_ffh_discounts_query.sql'\n",
    "ACCOUNT_HS_USAGE_QUERY_PATH = QUERIES_PATH + 'create_input_account_hs_usage_query.sql'\n",
    "ACCOUNT_DEMO_INCOME_QUERY_PATH = QUERIES_PATH + 'create_input_account_demo_income_query.sql'\n",
    "ACCOUNT_GPON_COPPER_QUERY_PATH = QUERIES_PATH + 'create_input_account_gpon_copper_query.sql'\n",
    "ACCOUNT_PRICE_PLAN_QUERY_PATH = QUERIES_PATH + 'create_input_account_price_plan_query.sql'\n",
    "ACCOUNT_CLCKSTRM_TELUS_QUERY_PATH = QUERIES_PATH + 'create_input_account_clckstrm_telus_query.sql'\n",
    "ACCOUNT_CALL_HISTORY_QUERY_PATH = QUERIES_PATH + 'create_input_account_call_history_query.sql'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc0bd98-1d58-491e-bc5e-bbac378c7bbf",
   "metadata": {},
   "source": [
    "### Import Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f442904b-a40a-4ba8-b237-e9894ffd4f26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download required component files to local\n",
    "prefix = f'{STACK_NAME}/{TRAIN_PIPELINE_NAME_PATH}/components/'\n",
    "dl_dir = 'components/'\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(RESOURCE_BUCKET)\n",
    "blobs = bucket.list_blobs(prefix=prefix)  # Get list of files\n",
    "for blob in blobs: # download each file that starts with \"prefix\" into \"dl_dir\"\n",
    "    if blob.name.endswith(\"/\"):\n",
    "        continue\n",
    "    file_split = blob.name.split(prefix)\n",
    "    file_path = f\"{dl_dir}{file_split[-1]}\"\n",
    "    directory = \"/\".join(file_path.split(\"/\")[0:-1])\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    blob.download_to_filename(file_path) \n",
    "\n",
    "# import main pipeline components\n",
    "import components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c315d454-416f-4eed-b12d-1acd5aed1207",
   "metadata": {},
   "source": [
    "### Date Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfad310-a482-4285-aa31-6d9113008be6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scoringDate = date(2022, 4, 1)  # date.today() - relativedelta(days=2)- relativedelta(months=30)\n",
    "valScoringDate = date(2022, 5, 1)  # scoringDate - relativedelta(days=2)\n",
    "\n",
    "# training views\n",
    "PROMO_EXPIRY_LIST_VIEW_NAME = '{}_pipeline_promo_expiry_list_data_training_bi_layer'.format(SERVICE_TYPE)  \n",
    "CONSL_VIEW_NAME = '{}_pipeline_consl_data_training_bi_layer'.format(SERVICE_TYPE)  \n",
    "FFH_BILLING_VIEW_NAME = '{}_pipeline_ffh_billing_data_training_bi_layer'.format(SERVICE_TYPE)  \n",
    "FFH_DISCOUNTS_VIEW_NAME = '{}_pipeline_ffh_discounts_data_training_bi_layer'.format(SERVICE_TYPE)  \n",
    "HS_USAGE_VIEW_NAME = '{}_pipeline_hs_usage_data_training_bi_layer'.format(SERVICE_TYPE)  \n",
    "DEMO_INCOME_VIEW_NAME = '{}_pipeline_demo_income_data_training_bi_layer'.format(SERVICE_TYPE)  \n",
    "GPON_COPPER_VIEW_NAME = '{}_pipeline_gpon_copper_data_training_bi_layer'.format(SERVICE_TYPE)  \n",
    "PRICE_PLAN_VIEW_NAME = '{}_pipeline_price_plan_data_training_bi_layer'.format(SERVICE_TYPE)  \n",
    "CLCKSTRM_TELUS_VIEW_NAME = '{}_pipeline_clckstrm_telus_training_bi_layer'.format(SERVICE_TYPE)\n",
    "CALL_HISTORY_VIEW_NAME = '{}_pipeline_call_history_data_training_bi_layer'.format(SERVICE_TYPE)  \n",
    "\n",
    "# validation views\n",
    "PROMO_EXPIRY_LIST_VIEW_VALIDATION_NAME = '{}_pipeline_promo_expiry_list_data_validation_bi_layer'.format(SERVICE_TYPE)  \n",
    "CONSL_VIEW_VALIDATION_NAME = '{}_pipeline_consl_data_validation_bi_layer'.format(SERVICE_TYPE)  \n",
    "FFH_BILLING_VIEW_VALIDATION_NAME = '{}_pipeline_ffh_billing_data_validation_bi_layer'.format(SERVICE_TYPE)  \n",
    "FFH_DISCOUNTS_VIEW_VALIDATION_NAME = '{}_pipeline_ffh_discounts_data_validation_bi_layer'.format(SERVICE_TYPE)  \n",
    "HS_USAGE_VIEW_VALIDATION_NAME = '{}_pipeline_hs_usage_data_validation_bi_layer'.format(SERVICE_TYPE)  \n",
    "DEMO_INCOME_VIEW_VALIDATION_NAME = '{}_pipeline_demo_income_data_validation_bi_layer'.format(SERVICE_TYPE)  \n",
    "GPON_COPPER_VIEW_VALIDATION_NAME = '{}_pipeline_gpon_copper_data_validation_bi_layer'.format(SERVICE_TYPE)  \n",
    "PRICE_PLAN_VIEW_VALIDATION_NAME = '{}_pipeline_price_plan_data_validation_bi_layer'.format(SERVICE_TYPE)  \n",
    "CLCKSTRM_TELUS_VIEW_VALIDATION_NAME = '{}_pipeline_clckstrm_telus_validation_bi_layer'.format(SERVICE_TYPE)\n",
    "CALL_HISTORY_VIEW_VALIDATION_NAME = '{}_pipeline_call_history_data_validation_bi_layer'.format(SERVICE_TYPE)  \n",
    "\n",
    "# training dates\n",
    "SCORE_DATE = scoringDate.strftime('%Y%m%d')  # date.today().strftime('%Y%m%d')\n",
    "SCORE_DATE_DASH = scoringDate.strftime('%Y-%m-%d')\n",
    "SCORE_DATE_MINUS_6_MOS_DASH = ((scoringDate - relativedelta(months=6)).replace(day=1)).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_THIS_MONTH_START_DASH = scoringDate.replace(day=1)\n",
    "SCORE_DATE_THIS_MONTH_END_DASH = (((scoringDate.replace(day=1)) + relativedelta(months=1)).replace(day=1) - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_LAST_MONTH_START_DASH = (scoringDate.replace(day=1) - timedelta(days=1)).replace(day=1).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_LAST_MONTH_END_DASH = ((scoringDate.replace(day=1)) - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_LAST_MONTH_YEAR = ((scoringDate.replace(day=1)) - timedelta(days=1)).year\n",
    "SCORE_DATE_LAST_MONTH_MONTH = ((scoringDate.replace(day=1)) - timedelta(days=1)).month\n",
    "PROMO_EXPIRY_START = (scoringDate.replace(day=1) + relativedelta(months=3)).replace(day=1).strftime('%Y%m%d')\n",
    "PROMO_EXPIRY_END = (scoringDate.replace(day=1) + relativedelta(months=4)).replace(day=1).strftime('%Y%m%d')\n",
    "\n",
    "# validation dates\n",
    "SCORE_DATE_VAL = valScoringDate.strftime('%Y%m%d')\n",
    "SCORE_DATE_VAL_DASH = valScoringDate.strftime('%Y-%m-%d')\n",
    "SCORE_DATE_VAL_MINUS_6_MOS_DASH = ((valScoringDate - relativedelta(months=6)).replace(day=1)).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_VAL_THIS_MONTH_START_DASH = valScoringDate.replace(day=1)\n",
    "SCORE_DATE_VAL_THIS_MONTH_END_DASH = (((valScoringDate.replace(day=1)) + relativedelta(months=1)).replace(day=1) - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_VAL_LAST_MONTH_START_DASH = (valScoringDate.replace(day=1) - timedelta(days=1)).replace(day=1).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_VAL_LAST_MONTH_END_DASH = ((valScoringDate.replace(day=1)) - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_VAL_LAST_MONTH_YEAR = ((valScoringDate.replace(day=1)) - timedelta(days=1)).year\n",
    "SCORE_DATE_VAL_LAST_MONTH_MONTH = ((valScoringDate.replace(day=1)) - timedelta(days=1)).month\n",
    "PROMO_EXPIRY_START_VAL = (valScoringDate.replace(day=1) + relativedelta(months=3)).replace(day=1).strftime('%Y%m%d')\n",
    "PROMO_EXPIRY_END_VAL = (valScoringDate.replace(day=1) + relativedelta(months=4)).replace(day=1).strftime('%Y%m%d')\n",
    "\n",
    "SCORE_DATE_DELTA = 0\n",
    "SCORE_DATE_VAL_DELTA = 0\n",
    "TICKET_DATE_WINDOW = 30  # Days of ticket data to be queried\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc9b8dc-9863-4e0b-b5e2-40adfbbbf69e",
   "metadata": {},
   "source": [
    "### 1.create_input_account_promo_expiry_list_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02852157-aa1e-4d9f-897b-1de7f225ccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_account_promo_expiry_list_view(view_name: str,\n",
    "                                           score_date: str,\n",
    "                                           score_date_delta: str,\n",
    "                                           dataset_id: str,\n",
    "                                           project_id: str,\n",
    "                                           region: str,\n",
    "                                           resource_bucket: str,\n",
    "                                           query_path: str, \n",
    "                                           promo_expiry_start: str, \n",
    "                                           promo_expiry_end: str\n",
    "                                           ):\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    bq_client = get_gcp_bqclient(project_id)\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    dataset = bq_client.dataset(dataset_id)\n",
    "    table_ref = dataset.table(view_name)\n",
    "\n",
    "    # load query from .txt file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(resource_bucket)\n",
    "    blob = bucket.get_blob(query_path)\n",
    "    content = blob.download_as_string()\n",
    "    content = str(content, 'utf-8')\n",
    "\n",
    "    def if_tbl_exists(client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "\n",
    "    if if_tbl_exists(bq_client, table_ref):\n",
    "        bq_client.delete_table(table_ref)\n",
    "\n",
    "    create_base_feature_set_query = content.format(score_date=score_date,\n",
    "                                                   score_date_delta=score_date_delta,\n",
    "                                                   view_name=view_name,\n",
    "                                                   dataset_id=dataset_id,\n",
    "                                                   project_id=project_id,\n",
    "                                                   promo_expiry_start=promo_expiry_start, \n",
    "                                                   promo_expiry_end=promo_expiry_end\n",
    "                                                   )\n",
    "    shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "    base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "    base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "    base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "    base_feature_set_view = bq_client.create_table(base_feature_set_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fddfece-062c-48e9-a5f4-cff98e47ebb1",
   "metadata": {},
   "source": [
    "### 2.create_input_account_consl_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb809ffb-e6a1-4268-8f89-06944489d0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_account_consl_view(view_name: str,\n",
    "                                    score_date: str,\n",
    "                                    score_date_delta: str,\n",
    "                                    project_id: str,\n",
    "                                    dataset_id: str,\n",
    "                                    region: str,\n",
    "                                    resource_bucket: str,\n",
    "                                    query_path: str,\n",
    "                                    ):\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "\n",
    "    def if_tbl_exists(client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    bq_client = get_gcp_bqclient(project_id)\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    dataset = bq_client.dataset(dataset_id)\n",
    "    table_ref = dataset.table(view_name)\n",
    "\n",
    "    # load query from .txt file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(resource_bucket)\n",
    "    blob = bucket.get_blob(query_path)\n",
    "    content = blob.download_as_string()\n",
    "    content = str(content, 'utf-8')\n",
    "\n",
    "    if if_tbl_exists(bq_client, table_ref):\n",
    "        bq_client.delete_table(table_ref)\n",
    "\n",
    "    # content = open(query_path, 'r').read()\n",
    "\n",
    "    create_base_feature_set_query = content.format(score_date=score_date,\n",
    "                                                   score_date_delta=score_date_delta,\n",
    "                                                   view_name=view_name,\n",
    "                                                   dataset_id=dataset_id,\n",
    "                                                   project_id=project_id,\n",
    "                                                   )\n",
    "    shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "    base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "    base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "    base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "    base_feature_set_view = bq_client.create_table(base_feature_set_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b450c36c-0fe4-4d33-a20a-d9676faf1711",
   "metadata": {},
   "source": [
    "### 3.create_input_account_ffh_billing_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63071292-8a10-4ac4-8b15-f4a705c97e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_account_ffh_billing_view(view_name: str,\n",
    "                                          v_report_date: str,\n",
    "                                          v_start_date: str,\n",
    "                                          v_end_date: str,\n",
    "                                          v_bill_year: str,\n",
    "                                          v_bill_month: str,\n",
    "                                          dataset_id: str,\n",
    "                                          project_id: str,\n",
    "                                          region: str,\n",
    "                                          resource_bucket: str,\n",
    "                                          query_path: str\n",
    "                                          ):\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    bq_client = get_gcp_bqclient(project_id)\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    dataset = bq_client.dataset(dataset_id)\n",
    "    table_ref = dataset.table(view_name)\n",
    "\n",
    "    # load query from .txt file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(resource_bucket)\n",
    "    blob = bucket.get_blob(query_path)\n",
    "    content = blob.download_as_string()\n",
    "    content = str(content, 'utf-8')\n",
    "\n",
    "    def if_tbl_exists(client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "\n",
    "    if if_tbl_exists(bq_client, table_ref):\n",
    "        bq_client.delete_table(table_ref)\n",
    "\n",
    "    create_base_feature_set_query = content.format(v_report_date=v_report_date,\n",
    "                                                   v_start_date=v_start_date,\n",
    "                                                   v_end_date=v_end_date,\n",
    "                                                   v_bill_year=v_bill_year,\n",
    "                                                   v_bill_month=v_bill_month,\n",
    "                                                   )\n",
    "\n",
    "    shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "    base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "    base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "    base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "    base_feature_set_view = bq_client.create_table(base_feature_set_view)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c3ef7e-efaa-4206-a831-a671257fa73a",
   "metadata": {},
   "source": [
    "### 4.create_input_account_ffh_discounts_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9117918-6c4f-40fb-96c5-ec26d5a3a627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_account_ffh_discounts_view(view_name: str,\n",
    "                                        score_date: str,\n",
    "                                        score_date_delta: str,\n",
    "                                        project_id: str,\n",
    "                                        dataset_id: str,\n",
    "                                        region: str,\n",
    "                                        resource_bucket: str,\n",
    "                                        query_path: str,\n",
    "                                        ):\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "\n",
    "    def if_tbl_exists(client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    bq_client = get_gcp_bqclient(project_id)\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    dataset = bq_client.dataset(dataset_id)\n",
    "    table_ref = dataset.table(view_name)\n",
    "\n",
    "    # load query from .txt file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(resource_bucket)\n",
    "    blob = bucket.get_blob(query_path)\n",
    "    content = blob.download_as_string()\n",
    "    content = str(content, 'utf-8')\n",
    "    \n",
    "    if if_tbl_exists(bq_client, table_ref):\n",
    "        bq_client.delete_table(table_ref)\n",
    "\n",
    "    # content = open(query_path, 'r').read()\n",
    "\n",
    "    create_base_feature_set_query = content.format(score_date=score_date,\n",
    "                                                   score_date_delta=score_date_delta, \n",
    "                                                   view_name=view_name,\n",
    "                                                   dataset_id=dataset_id,\n",
    "                                                   project_id=project_id\n",
    "                                                   )\n",
    "    shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "    base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "    base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "    base_feature_set_view.view_query = create_base_feature_set_query\n",
    "    base_feature_set_view = bq_client.create_table(base_feature_set_view)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcac8e47-12fd-4483-bf7c-778e423b4799",
   "metadata": {},
   "source": [
    "### 5.create_input_account_hs_usage_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee66702-4817-4452-ad4a-07775d2aa02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_account_hs_usage_view(view_name: str,\n",
    "                                       v_report_date: str,\n",
    "                                       v_start_date: str,\n",
    "                                       v_end_date: str,\n",
    "                                       v_bill_year: str,\n",
    "                                       v_bill_month: str,\n",
    "                                       dataset_id: str,\n",
    "                                       project_id: str,\n",
    "                                       region: str,\n",
    "                                       resource_bucket: str,\n",
    "                                       query_path: str\n",
    "                                       ):\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    bq_client = get_gcp_bqclient(project_id)\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    dataset = bq_client.dataset(dataset_id)\n",
    "    table_ref = dataset.table(view_name)\n",
    "\n",
    "    # load query from .txt file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(resource_bucket)\n",
    "    blob = bucket.get_blob(query_path)\n",
    "    content = blob.download_as_string()\n",
    "    content = str(content, 'utf-8')\n",
    "\n",
    "    def if_tbl_exists(client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "\n",
    "    if if_tbl_exists(bq_client, table_ref):\n",
    "        bq_client.delete_table(table_ref)\n",
    "\n",
    "    create_base_feature_set_query = content.format(v_report_date=v_report_date,\n",
    "                                                   v_start_date=v_start_date,\n",
    "                                                   v_end_date=v_end_date,\n",
    "                                                   v_bill_year=v_bill_year,\n",
    "                                                   v_bill_month=v_bill_month,\n",
    "                                                   )\n",
    "\n",
    "    shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "    base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "    base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "    base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "    base_feature_set_view = bq_client.create_table(base_feature_set_view)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec90cef-a04a-410c-8976-8b0bd71dd0ad",
   "metadata": {},
   "source": [
    "### 6.create_input_account_demo_income_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f68587-e7ab-4764-b767-4da55c51f162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_account_demo_income_view(view_name: str,\n",
    "                                          score_date: str,\n",
    "                                          score_date_delta: str,\n",
    "                                          dataset_id: str,\n",
    "                                          project_id: str,\n",
    "                                          region: str,\n",
    "                                          resource_bucket: str,\n",
    "                                          query_path: str\n",
    "                                          ):\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    bq_client = get_gcp_bqclient(project_id)\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    dataset = bq_client.dataset(dataset_id)\n",
    "    table_ref = dataset.table(view_name)\n",
    "\n",
    "    # load query from .txt file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(resource_bucket)\n",
    "    blob = bucket.get_blob(query_path)\n",
    "    content = blob.download_as_string()\n",
    "    content = str(content, 'utf-8')\n",
    "\n",
    "    def if_tbl_exists(client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "\n",
    "    if if_tbl_exists(bq_client, table_ref):\n",
    "        bq_client.delete_table(table_ref)\n",
    "\n",
    "    create_base_feature_set_query = content.format(score_date=score_date,\n",
    "                                                   score_date_delta=score_date_delta,\n",
    "                                                   project_id=project_id,\n",
    "                                                   dataset_id='common_dataset',\n",
    "                                                   )\n",
    "\n",
    "    shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "    base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "    base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "    base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "    base_feature_set_view = bq_client.create_table(base_feature_set_view)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0148a-0a7a-4fa5-8e53-7ef7bd91eda6",
   "metadata": {},
   "source": [
    "### 7.create_input_account_gpon_copper_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2190f37-a3d3-423d-a2f4-f8f5e4832835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_account_gpon_copper_view(view_name: str,\n",
    "                                          score_date: str,\n",
    "                                          score_date_delta: str,\n",
    "                                          dataset_id: str,\n",
    "                                          project_id: str,\n",
    "                                          region: str,\n",
    "                                          resource_bucket: str,\n",
    "                                          query_path: str\n",
    "                                          ):\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    bq_client = get_gcp_bqclient(project_id)\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    dataset = bq_client.dataset(dataset_id)\n",
    "    table_ref = dataset.table(view_name)\n",
    "\n",
    "    # load query from .txt file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(resource_bucket)\n",
    "    blob = bucket.get_blob(query_path)\n",
    "    content = blob.download_as_string()\n",
    "    content = str(content, 'utf-8')\n",
    "\n",
    "    def if_tbl_exists(client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "\n",
    "    if if_tbl_exists(bq_client, table_ref):\n",
    "        bq_client.delete_table(table_ref)\n",
    "\n",
    "    create_base_feature_set_query = content.format(score_date=score_date,\n",
    "                                                   score_date_delta=score_date_delta,\n",
    "                                                   )\n",
    "\n",
    "    shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "    base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "    base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "    base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "    base_feature_set_view = bq_client.create_table(base_feature_set_view)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4637e1d7-d125-454d-a1e4-cb00057c9bd3",
   "metadata": {},
   "source": [
    "### 8.create_input_account_price_plan_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67990674-ff9e-4274-b881-ac9fee47b5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_account_price_plan_view(view_name: str,\n",
    "                                        score_date: str,\n",
    "                                        score_date_delta: str,\n",
    "                                        project_id: str,\n",
    "                                        dataset_id: str,\n",
    "                                        region: str,\n",
    "                                        resource_bucket: str,\n",
    "                                        query_path: str,\n",
    "                                        ):\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "\n",
    "    def if_tbl_exists(client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    bq_client = get_gcp_bqclient(project_id)\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    dataset = bq_client.dataset(dataset_id)\n",
    "    table_ref = dataset.table(view_name)\n",
    "\n",
    "    # load query from .txt file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(resource_bucket)\n",
    "    blob = bucket.get_blob(query_path)\n",
    "    content = blob.download_as_string()\n",
    "    content = str(content, 'utf-8')\n",
    "\n",
    "    if if_tbl_exists(bq_client, table_ref):\n",
    "        bq_client.delete_table(table_ref)\n",
    "\n",
    "    # content = open(query_path, 'r').read()\n",
    "\n",
    "    create_base_feature_set_query = content.format(score_date=score_date,\n",
    "                                                   score_date_delta=score_date_delta,\n",
    "                                                   view_name=view_name,\n",
    "                                                   dataset_id=dataset_id,\n",
    "                                                   project_id=project_id,\n",
    "                                                   )\n",
    "    shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "    base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "    base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "    base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "    base_feature_set_view = bq_client.create_table(base_feature_set_view)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4f1056-61de-4aa9-9167-66a2a9400f24",
   "metadata": {},
   "source": [
    "### 9.create_input_account_clckstrm_telus_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf4fc55-03b6-4680-b52b-e3f8f664685e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_account_clckstrm_telus_view(view_name: str,\n",
    "                                    score_date: str,\n",
    "                                    score_date_delta: str,\n",
    "                                    project_id: str,\n",
    "                                    dataset_id: str,\n",
    "                                    region: str,\n",
    "                                    resource_bucket: str,\n",
    "                                    query_path: str,\n",
    "                                    ):\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "\n",
    "    def if_tbl_exists(client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    bq_client = get_gcp_bqclient(project_id)\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    dataset = bq_client.dataset(dataset_id)\n",
    "    table_ref = dataset.table(view_name)\n",
    "\n",
    "    # load query from .txt file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(resource_bucket)\n",
    "    blob = bucket.get_blob(query_path)\n",
    "    content = blob.download_as_string()\n",
    "    content = str(content, 'utf-8')\n",
    "\n",
    "    if if_tbl_exists(bq_client, table_ref):\n",
    "        bq_client.delete_table(table_ref)\n",
    "\n",
    "    # content = open(query_path, 'r').read()\n",
    "\n",
    "    create_base_feature_set_query = content.format(score_date=score_date,\n",
    "                                                   score_date_delta=score_date_delta,\n",
    "                                                   view_name=view_name,\n",
    "                                                   dataset_id=dataset_id,\n",
    "                                                   project_id=project_id,\n",
    "                                                   )\n",
    "    shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "    base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "    base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "    base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "    base_feature_set_view = bq_client.create_table(base_feature_set_view)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df2a9dd-3c80-4dcd-a55d-adce24f144de",
   "metadata": {},
   "source": [
    "### 10.create_input_account_call_history_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f4a761-e3d4-402f-87d9-6a4ed21ff3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_account_call_history_view(view_name: str,\n",
    "                                        score_date: str,\n",
    "                                        score_date_delta: str,\n",
    "                                        project_id: str,\n",
    "                                        dataset_id: str,\n",
    "                                        region: str,\n",
    "                                        resource_bucket: str,\n",
    "                                        query_path: str,\n",
    "                                        ):\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "\n",
    "    def if_tbl_exists(client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    bq_client = get_gcp_bqclient(project_id)\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    dataset = bq_client.dataset(dataset_id)\n",
    "    table_ref = dataset.table(view_name)\n",
    "\n",
    "    # load query from .txt file\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(resource_bucket)\n",
    "    blob = bucket.get_blob(query_path)\n",
    "    content = blob.download_as_string()\n",
    "    content = str(content, 'utf-8')\n",
    "\n",
    "    if if_tbl_exists(bq_client, table_ref):\n",
    "        bq_client.delete_table(table_ref)\n",
    "\n",
    "    # content = open(query_path, 'r').read()\n",
    "\n",
    "    create_base_feature_set_query = content.format(score_date=score_date,\n",
    "                                                   score_date_delta=score_date_delta,\n",
    "                                                   view_name=view_name,\n",
    "                                                   dataset_id=dataset_id,\n",
    "                                                   project_id=project_id,\n",
    "                                                   )\n",
    "    shared_dataset_ref = bq_client.dataset(dataset_id)\n",
    "    base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "    base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "    base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "    base_feature_set_view = bq_client.create_table(base_feature_set_view)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281c6676-70b0-41a6-b860-aa6d95fec0da",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8623f9-e2fd-406a-b0a7-2ea7c42af66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "        promo_expiry_list_view: str, \n",
    "        account_consl_view: str, \n",
    "        account_bill_view: str, \n",
    "        account_discounts_view: str, \n",
    "        hs_usage_view: str, \n",
    "        demo_income_view: str, \n",
    "        gpon_copper_view: str, \n",
    "        price_plan_view: str, \n",
    "        clckstrm_telus_view: str, \n",
    "        call_history_view: str, \n",
    "        save_data_path: str,\n",
    "        project_id: str,\n",
    "        dataset_id: str\n",
    "):\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    import pandas as pd\n",
    "    import gc\n",
    "    import time\n",
    "\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    client = get_gcp_bqclient(project_id)\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    \n",
    "    #1.df_promo_expiry_list\n",
    "    promo_expiry_list_set = f\"{project_id}.{dataset_id}.{promo_expiry_list_view}\" \n",
    "    build_df_promo_expiry_list = '''SELECT * FROM `{promo_expiry_list_set}`'''.format(promo_expiry_list_set=promo_expiry_list_set)\n",
    "    df_promo_expiry_list = client.query(build_df_promo_expiry_list).to_dataframe()\n",
    "    df_promo_expiry_list = df_promo_expiry_list.set_index('ban')\n",
    "    df_join = df_promo_expiry_list.copy()\n",
    "    print('......df_promo_expiry_list done')\n",
    "    \n",
    "    #2.df_consl\n",
    "    consl_data_set = f\"{project_id}.{dataset_id}.{account_consl_view}\" \n",
    "    build_df_consl = '''SELECT * FROM `{consl_data_set}`'''.format(consl_data_set=consl_data_set)\n",
    "    df_consl = client.query(build_df_consl).to_dataframe()\n",
    "    df_mix = df_consl[[\n",
    "        'ban',\n",
    "        'customer_tenure', \n",
    "        'product_mix_all',\n",
    "        'hsic_count',\n",
    "        'ttv_count',\n",
    "        'sing_count',\n",
    "        'mob_count',\n",
    "        'shs_count',\n",
    "        'new_hsic_ind',\n",
    "        'new_ttv_ind',\n",
    "        'new_sing_ind',\n",
    "        'new_c_ind',\n",
    "        'new_smhm_ind',\n",
    "        'mnh_ind'\n",
    "    ]]\n",
    "    df_mix = df_mix.drop_duplicates(subset=['ban']).set_index('ban').add_prefix('productMix_').fillna(0)\n",
    "    df_join = df_join.join(df_mix)\n",
    "    del df_mix\n",
    "    gc.collect()\n",
    "    print('......df_consl done')\n",
    "    \n",
    "    #3.df_bill\n",
    "    bill_data_set = f\"{project_id}.{dataset_id}.{account_bill_view}\" \n",
    "    build_df_bill = '''SELECT * FROM `{bill_data_set}`'''.format(bill_data_set=bill_data_set)\n",
    "    df_bill = client.query(build_df_bill).to_dataframe() \n",
    "    df_bill = df_bill.set_index('ban').add_prefix('ffhBill_')\n",
    "    df_join = df_join.join(df_bill).fillna(0) \n",
    "    del df_bill\n",
    "    gc.collect()\n",
    "    print('......df_bill done')\n",
    "    \n",
    "    #4.df_discounts\n",
    "    discounts_data_set = f\"{project_id}.{dataset_id}.{account_discounts_view}\" \n",
    "    build_df_discounts = '''SELECT * FROM `{discounts_data_set}`'''.format(discounts_data_set=discounts_data_set)\n",
    "    df_discounts = client.query(build_df_discounts).to_dataframe() \n",
    "    df_discounts = df_discounts.set_index('ban').add_prefix('ffhDiscounts_')\n",
    "    df_join = df_join.join(df_discounts).fillna(0) \n",
    "    del df_discounts\n",
    "    gc.collect()\n",
    "    print('......df_discounts done')\n",
    "\n",
    "    #5.df_hs_usage\n",
    "    hs_usage_data_set = f\"{project_id}.{dataset_id}.{hs_usage_view}\" \n",
    "    build_df_hs_usage = '''SELECT * FROM `{hs_usage_data_set}`'''.format(hs_usage_data_set=hs_usage_data_set)\n",
    "    df_hs_usage = client.query(build_df_hs_usage).to_dataframe() \n",
    "    df_hs_usage = df_hs_usage.set_index('ban').add_prefix('hsiaUsage_')\n",
    "    df_join = df_join.join(df_hs_usage).fillna(0) \n",
    "    del df_hs_usage\n",
    "    gc.collect()\n",
    "    print('......df_hs_usage done')\n",
    "\n",
    "    #6.df_income\n",
    "    demo_income_data_set = f\"{project_id}.{dataset_id}.{demo_income_view}\" \n",
    "    build_df_demo_income = '''SELECT * FROM `{demo_income_data_set}`'''.format(demo_income_data_set=demo_income_data_set)\n",
    "    df_income = client.query(build_df_demo_income).to_dataframe()\n",
    "    df_income = df_income.set_index('ban')\n",
    "    df_income['demo_urban_flag'] = df_income.demo_sgname.str.lower().str.contains('urban').fillna(0).astype(int)\n",
    "    df_income['demo_rural_flag'] = df_income.demo_sgname.str.lower().str.contains('rural').fillna(0).astype(int)\n",
    "    df_income['demo_family_flag'] = df_income.demo_lsname.str.lower().str.contains('families').fillna(0).astype(int)\n",
    "    df_income_dummies = pd.get_dummies(df_income[['demo_lsname']])\n",
    "    df_income_dummies.columns = df_income_dummies.columns.str.replace('&', 'and')\n",
    "    df_income_dummies.columns = df_income_dummies.columns.str.replace(' ', '_')\n",
    "    df_income = df_income[['demo_avg_income', 'demo_urban_flag', 'demo_rural_flag', 'demo_family_flag']].join(\n",
    "        df_income_dummies)\n",
    "    df_income.demo_avg_income = df_income.demo_avg_income.astype(float)\n",
    "    df_income.demo_avg_income = df_income.demo_avg_income.fillna(df_income.demo_avg_income.median())\n",
    "    df_group_income = df_income.groupby('ban').agg('mean')\n",
    "    df_group_income = df_group_income.add_prefix('demographics_')\n",
    "    df_join = df_join.join(df_group_income.fillna(df_group_income.median()))\n",
    "    del df_group_income\n",
    "    del df_income\n",
    "    gc.collect()\n",
    "    print('......df_income done')\n",
    "\n",
    "    #7.df_gpon_copper\n",
    "    gpon_copper_data_set = f\"{project_id}.{dataset_id}.{gpon_copper_view}\"\n",
    "    build_df_gpon_copper = '''SELECT * FROM `{gpon_copper_data_set}`'''.format(gpon_copper_data_set=gpon_copper_data_set)\n",
    "    df_gpon_copper = client.query(build_df_gpon_copper).to_dataframe()\n",
    "    df_gpon_copper = df_gpon_copper.set_index('ban')\n",
    "    df_join = df_join.join(df_gpon_copper.add_prefix('infra_')).fillna(0)\n",
    "    del df_gpon_copper\n",
    "    gc.collect()\n",
    "    print('......df_gpon_copper done')\n",
    "\n",
    "    #8.df_price_plan\n",
    "    price_plan_data_set = f\"{project_id}.{dataset_id}.{price_plan_view}\"\n",
    "    build_df_price_plan = '''SELECT * FROM `{price_plan_data_set}`'''.format(price_plan_data_set=price_plan_data_set)\n",
    "    df_price_plan = client.query(build_df_price_plan).to_dataframe()\n",
    "    df_price_plan = df_price_plan.set_index('ban')\n",
    "    df_pp_dummies = pd.get_dummies(df_price_plan[['price_plan']])\n",
    "    df_pp_dummies.columns = df_pp_dummies.columns.str.replace('&', 'and')\n",
    "    df_pp_dummies.columns = df_pp_dummies.columns.str.replace(' ', '_')\n",
    "    df_price_plan = df_price_plan.join(df_pp_dummies)\n",
    "    df_price_plan.drop(columns=['price_plan'], axis=1, inplace=True)\n",
    "    print(df_price_plan.columns)\n",
    "    df_join = df_join.join(df_price_plan.add_prefix('infra_')).fillna(0)\n",
    "    del df_price_plan\n",
    "    gc.collect()\n",
    "    print('......df_price_plan done')\n",
    "\n",
    "    #9.df_clckstrm_telus\n",
    "    clckstrm_telus_data_set = f\"{project_id}.{dataset_id}.{clckstrm_telus_view}\" \n",
    "    build_df_clckstrm_telus = '''SELECT * FROM `{clckstrm_telus_data_set}`'''.format(clckstrm_telus_data_set=clckstrm_telus_data_set)\n",
    "    df_clckstrm_telus = client.query(build_df_clckstrm_telus).to_dataframe() \n",
    "    df_clckstrm_telus = df_clckstrm_telus.set_index('ban').add_prefix('clckstrmData_')\n",
    "    df_join = df_join.join(df_clckstrm_telus).fillna(0) \n",
    "    del df_clckstrm_telus\n",
    "    gc.collect()\n",
    "    print('......df_clckstrm_telus done')\n",
    "\n",
    "    #10.df_call_history\n",
    "    call_history_data_set = f\"{project_id}.{dataset_id}.{call_history_view}\" \n",
    "    build_df_call_history = '''SELECT * FROM `{call_history_data_set}`'''.format(call_history_data_set=call_history_data_set)\n",
    "    df_call_history = client.query(build_df_call_history).to_dataframe() \n",
    "    df_call_history = df_call_history.set_index('ban').add_prefix('callHistory_')\n",
    "    df_join = df_join.join(df_call_history)\n",
    "    df_join[['callHistory_frequency', 'callHistory_have_called']] = df_join[['callHistory_frequency', 'callHistory_have_called']].fillna(0)\n",
    "    df_join[['callHistory_recency']] = df_join[['callHistory_recency']].fillna(999)\n",
    "    del df_call_history\n",
    "    gc.collect()\n",
    "    print('......df_call_history done')\n",
    "\n",
    "    #column name clean-up\n",
    "    df_join.columns = df_join.columns.str.replace(' ', '_')\n",
    "    df_join.columns = df_join.columns.str.replace('-', '_')\n",
    "\n",
    "    #df_final\n",
    "    df_final = df_join.copy()\n",
    "    del df_join\n",
    "    gc.collect()\n",
    "    print('......df_final done')\n",
    "\n",
    "    for f in df_final.columns:\n",
    "        df_final[f] = list(df_final[f])\n",
    "\n",
    "    df_final.to_csv(save_data_path, index=True, compression='gzip') \n",
    "    del df_final\n",
    "    gc.collect()\n",
    "    print(f'......csv saved in {save_data_path}')\n",
    "    time.sleep(120)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330396ff-dfe9-4c23-9d67-922e9a190bdc",
   "metadata": {},
   "source": [
    "### Train and Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f33719-127b-45c7-bccd-dcd3970b003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_model(\n",
    "            resource_bucket: str,\n",
    "            service_type: str,\n",
    "            score_date_dash: str,\n",
    "            score_date_val_dash: str,\n",
    "            project_id: str,\n",
    "            dataset_id: str\n",
    "            # metrics: Output[Metrics],\n",
    "            # metricsc: Output[ClassificationMetrics]\n",
    "):\n",
    "\n",
    "    import gc\n",
    "    import time\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    from google.cloud import storage\n",
    "    from google.cloud import bigquery\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    def get_lift(prob, y_test, q):\n",
    "        result = pd.DataFrame(columns=['Prob', 'Call_To_Retention'])\n",
    "        result['Prob'] = prob\n",
    "        result['Call_To_Retention'] = y_test\n",
    "        result['Decile'] = pd.qcut(result['Prob'], q, labels=[i for i in range(q, 0, -1)])\n",
    "        add = pd.DataFrame(result.groupby('Decile')['Call_To_Retention'].mean()).reset_index()\n",
    "        add.columns = ['Decile', 'avg_real_call_to_retention_rate']\n",
    "        result = result.merge(add, on='Decile', how='left')\n",
    "        result.sort_values('Decile', ascending=True, inplace=True)\n",
    "        lg = pd.DataFrame(result.groupby('Decile')['Prob'].mean()).reset_index()\n",
    "        lg.columns = ['Decile', 'avg_model_pred_call_to_retention_rate']\n",
    "        lg.sort_values('Decile', ascending=False, inplace=True)\n",
    "        lg['avg_call_to_retention_rate_total'] = result['Call_To_Retention'].mean()\n",
    "        lg = lg.merge(add, on='Decile', how='left')\n",
    "        lg['lift'] = lg['avg_real_call_to_retention_rate'] / lg['avg_call_to_retention_rate_total']\n",
    "\n",
    "        return lg\n",
    "\n",
    "    df_train = pd.read_csv('gs://{}/{}_train.csv.gz'.format(resource_bucket, service_type),\n",
    "                           compression='gzip')  \n",
    "    df_test = pd.read_csv('gs://{}/{}_validation.csv.gz'.format(resource_bucket, service_type),  \n",
    "                          compression='gzip')\n",
    "\n",
    "    #set up df_train\n",
    "    def get_gcp_bqclient(project_id, use_local_credential=True):\n",
    "        token = os.popen('gcloud auth print-access-token').read()\n",
    "        token = re.sub(f'\\n$', '', token)\n",
    "        credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        if use_local_credential:\n",
    "            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "        return bq_client\n",
    "\n",
    "    client = get_gcp_bqclient(project_id)\n",
    "\n",
    "    # bq_client = bigquery.Client(project=project_id)\n",
    "    \n",
    "    sql_train = ''' SELECT * FROM `{}.{}.bq_call_to_retention_targets` '''.format(project_id, dataset_id) \n",
    "    df_target_train = client.query(sql_train).to_dataframe()\n",
    "    df_target_train = df_target_train.loc[\n",
    "        df_target_train['YEAR_MONTH'] == '-'.join(score_date_dash.split('-')[:2])]  # score_date_dash = '2022-08-31'\n",
    "    df_target_train['ban'] = df_target_train['ban'].astype('int64')\n",
    "    df_target_train = df_target_train.groupby('ban').tail(1)\n",
    "    df_train = df_train.merge(df_target_train[['ban', 'target_ind']], on='ban', how='left')\n",
    "    df_train.rename(columns={'target_ind': 'target'}, inplace=True)\n",
    "    df_train.dropna(subset=['target'], inplace=True)\n",
    "    df_train['target'] = df_train['target'].astype(int)\n",
    "    print(df_train.shape)\n",
    "\n",
    "    #set up df_test\n",
    "    sql_test = ''' SELECT * FROM `{}.{}.bq_call_to_retention_targets` '''.format(project_id, dataset_id) \n",
    "    df_target_test = client.query(sql_test).to_dataframe()\n",
    "    df_target_test = df_target_test.loc[\n",
    "        df_target_test['YEAR_MONTH'] == '-'.join(score_date_val_dash.split('-')[:2])]  # score_date_dash = '2022-09-30'\n",
    "    df_target_test['ban'] = df_target_test['ban'].astype('int64')\n",
    "    df_target_test = df_target_test.groupby('ban').tail(1)\n",
    "    df_test = df_test.merge(df_target_test[['ban', 'target_ind']], on='ban', how='left')\n",
    "    df_test.rename(columns={'target_ind': 'target'}, inplace=True)\n",
    "    df_test.dropna(subset=['target'], inplace=True)\n",
    "    df_test['target'] = df_test['target'].astype(int)\n",
    "    print(df_test.shape)\n",
    "\n",
    "    #set up features (list)\n",
    "    cols_1 = df_train.columns.values\n",
    "    cols_2 = df_test.columns.values\n",
    "    cols = set(cols_1).intersection(set(cols_2))\n",
    "    features = [f for f in cols if f not in ['ban', 'target']]\n",
    "\n",
    "    #train test split\n",
    "    df_train, df_val = train_test_split(df_train, shuffle=True, test_size=0.2, random_state=42,\n",
    "                                        stratify=df_train['target']\n",
    "                                        )\n",
    "\n",
    "    ban_train = df_train['ban']\n",
    "    X_train = df_train[features]\n",
    "    y_train = np.squeeze(df_train['target'].values)\n",
    "\n",
    "    ban_val = df_val['ban']\n",
    "    X_val = df_val[features]\n",
    "    y_val = np.squeeze(df_val['target'].values)\n",
    "\n",
    "    ban_test = df_test['ban']\n",
    "    X_test = df_test[features]\n",
    "    y_test = np.squeeze(df_test['target'].values)\n",
    "\n",
    "    del df_train, df_val, df_test\n",
    "    gc.collect()\n",
    "\n",
    "    # build model and fit in training data\n",
    "    import xgboost as xgb\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        learning_rate=0.01,\n",
    "        n_estimators=100,\n",
    "        max_depth=8,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective='binary:logistic',\n",
    "        nthread=4,\n",
    "        scale_pos_weight=1\n",
    "        # seed=27\n",
    "    )\n",
    "\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    print('xgb training done')\n",
    "\n",
    "    from sklearn.preprocessing import normalize\n",
    "\n",
    "    #predictions on X_val\n",
    "    y_pred = xgb_model.predict_proba(X_val, ntree_limit=xgb_model.best_iteration)[:, 1]\n",
    "    y_pred_label = (y_pred > 0.5).astype(int)\n",
    "    auc = roc_auc_score(y_val, y_pred_label)\n",
    "    metrics.log_metric(\"AUC\", auc)\n",
    "\n",
    "    pred_prb = xgb_model.predict_proba(X_test, ntree_limit=xgb_model.best_iteration)[:, 1]\n",
    "    lg = get_lift(pred_prb, y_test, 10)\n",
    "    lg.to_csv('gs://{}/lift_on_scoring_data_{}.csv'.format(resource_bucket, create_time, index=False))\n",
    "\n",
    "    # save the model in GCS\n",
    "    from datetime import datetime\n",
    "    models_dict = {}\n",
    "    create_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    models_dict['create_time'] = create_time\n",
    "    models_dict['model'] = xgb_model\n",
    "    models_dict['features'] = features\n",
    "\n",
    "    with open('model_dict.pkl', 'wb') as handle:\n",
    "        pickle.dump(models_dict, handle)\n",
    "    handle.close()\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(resource_bucket)\n",
    "\n",
    "    MODEL_PATH = '{}_xgb_models/'.format(service_type)\n",
    "    blob = bucket.blob(MODEL_PATH)\n",
    "    if not blob.exists(storage_client):\n",
    "        blob.upload_from_string('')\n",
    "\n",
    "    model_name_onbkt = '{}{}_models_xgb_{}'.format(MODEL_PATH, service_type, models_dict['create_time'])\n",
    "    blob = bucket.blob(model_name_onbkt)\n",
    "    blob.upload_from_filename('model_dict.pkl')\n",
    "\n",
    "    print(f\"....model loaded to GCS done at {str(create_time)}\")\n",
    "\n",
    "    time.sleep(120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9754fb-807c-470c-8c1f-c55093725b34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e2eea99-a3f0-48ea-8962-115edcaf26d9",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3cda87-dc1f-495e-be67-a3d623c13e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @dsl.pipeline(\n",
    "#     # A name for the pipeline.\n",
    "#     name=\"{}-xgb-pipeline\".format(SERVICE_TYPE_NAME),\n",
    "#     description=' pipeline for training {} model'.format(SERVICE_TYPE_NAME)\n",
    "# )\n",
    "def pipeline(\n",
    "        project_id: str = PROJECT_ID,\n",
    "        region: str = REGION,\n",
    "        resource_bucket: str = RESOURCE_BUCKET\n",
    "        # file_bucket: str = FILE_BUCKET\n",
    "    ):\n",
    "    # ------------- train view ops ---------------\n",
    "    #1.create_input_account_promo_expiry_list_view\n",
    "    create_input_account_promo_expiry_list_view_op = create_input_account_promo_expiry_list_view(\n",
    "        view_name=PROMO_EXPIRY_LIST_VIEW_NAME,\n",
    "        score_date=SCORE_DATE,\n",
    "        score_date_delta=SCORE_DATE_DELTA,\n",
    "        promo_expiry_start = PROMO_EXPIRY_START, \n",
    "        promo_expiry_end = PROMO_EXPIRY_END,\n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID,\n",
    "        region=REGION,\n",
    "        resource_bucket=RESOURCE_BUCKET,\n",
    "        query_path=ACCOUNT_PROMO_EXPIRY_LIST_QUERY_PATH\n",
    "    )\n",
    "    # create_input_account_promo_expiry_list_view_op.set_memory_limit('16G')\n",
    "    # create_input_account_promo_expiry_list_view_op.set_cpu_limit('4')\n",
    "\n",
    "    #2.create_input_account_consl_view\n",
    "    create_input_account_consl_view_op = create_input_account_consl_view(\n",
    "        view_name=CONSL_VIEW_NAME,\n",
    "        score_date=SCORE_DATE,\n",
    "        score_date_delta=SCORE_DATE_DELTA,\n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID,\n",
    "        region=REGION,\n",
    "        resource_bucket=RESOURCE_BUCKET,\n",
    "        query_path=ACCOUNT_CONSL_QUERY_PATH\n",
    "    )\n",
    "    # create_input_account_consl_view_op.set_memory_limit('16G')\n",
    "    # create_input_account_consl_view_op.set_cpu_limit('4')\n",
    "\n",
    "    #3.create_input_account_ffh_billing_view\n",
    "    create_input_account_ffh_billing_view_op = create_input_account_ffh_billing_view(\n",
    "        v_report_date=SCORE_DATE_DASH,\n",
    "        v_start_date=SCORE_DATE_MINUS_6_MOS_DASH,\n",
    "        v_end_date=SCORE_DATE_LAST_MONTH_END_DASH,\n",
    "        v_bill_year=SCORE_DATE_LAST_MONTH_YEAR,\n",
    "        v_bill_month=SCORE_DATE_LAST_MONTH_MONTH,\n",
    "        view_name=FFH_BILLING_VIEW_NAME,\n",
    "        dataset_id=DATASET_ID,\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        resource_bucket=RESOURCE_BUCKET,\n",
    "        query_path=ACCOUNT_FFH_BILLING_QUERY_PATH \n",
    "    )\n",
    "\n",
    "    # create_input_account_ffh_billing_view_op.set_memory_limit('16G')\n",
    "    # create_input_account_ffh_billing_view_op.set_cpu_limit('4')\n",
    "\n",
    "    #4.create_input_account_ffh_discounts_view\n",
    "    create_input_account_ffh_discounts_view_op = create_input_account_ffh_discounts_view(\n",
    "        view_name=FFH_DISCOUNTS_VIEW_NAME,\n",
    "        score_date=SCORE_DATE,\n",
    "        score_date_delta=SCORE_DATE_DELTA,\n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID,\n",
    "        region=REGION,\n",
    "        resource_bucket=RESOURCE_BUCKET,\n",
    "        query_path=ACCOUNT_FFH_DISCOUNTS_QUERY_PATH\n",
    "    )\n",
    "\n",
    "    # create_input_account_ffh_discounts_view_op.set_memory_limit('16G')\n",
    "    # create_input_account_ffh_discounts_view_op.set_cpu_limit('4')\n",
    "\n",
    "    #5.create_input_account_hs_usage_view\n",
    "    create_input_account_hs_usage_view_op = create_input_account_hs_usage_view(\n",
    "        v_report_date=SCORE_DATE_DASH,\n",
    "        v_start_date=SCORE_DATE_MINUS_6_MOS_DASH,\n",
    "        v_end_date=SCORE_DATE_LAST_MONTH_END_DASH,\n",
    "        v_bill_year=SCORE_DATE_LAST_MONTH_YEAR,\n",
    "        v_bill_month=SCORE_DATE_LAST_MONTH_MONTH,\n",
    "        view_name=HS_USAGE_VIEW_NAME,\n",
    "        dataset_id=DATASET_ID,\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        resource_bucket=RESOURCE_BUCKET,\n",
    "        query_path=ACCOUNT_HS_USAGE_QUERY_PATH \n",
    "    )\n",
    "\n",
    "    # create_input_account_hs_usage_view_op.set_memory_limit('16G')\n",
    "    # create_input_account_hs_usage_view_op.set_cpu_limit('4')\n",
    "\n",
    "    #6.create_input_account_demo_income_view\n",
    "    create_input_account_demo_income_view_op = create_input_account_demo_income_view(\n",
    "        score_date=SCORE_DATE,\n",
    "        score_date_delta=SCORE_DATE_DELTA,\n",
    "        view_name=DEMO_INCOME_VIEW_NAME ,\n",
    "        dataset_id=DATASET_ID,\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        resource_bucket=RESOURCE_BUCKET,\n",
    "        query_path=ACCOUNT_DEMO_INCOME_QUERY_PATH \n",
    "    )\n",
    "\n",
    "    # create_input_account_demo_income_view_op.set_memory_limit('16G')\n",
    "    # create_input_account_demo_income_view_op.set_cpu_limit('4')\n",
    "\n",
    "    #7.create_input_account_gpon_copper_view\n",
    "    create_input_account_gpon_copper_view_op = create_input_account_gpon_copper_view(\n",
    "        score_date=SCORE_DATE,\n",
    "        score_date_delta=SCORE_DATE_DELTA,\n",
    "        view_name=GPON_COPPER_VIEW_NAME,\n",
    "        dataset_id=DATASET_ID,\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        resource_bucket=RESOURCE_BUCKET,\n",
    "        query_path=ACCOUNT_GPON_COPPER_QUERY_PATH \n",
    "    )\n",
    "\n",
    "    # create_input_account_gpon_copper_view_op.set_memory_limit('16G')\n",
    "    # create_input_account_gpon_copper_view_op.set_cpu_limit('4')\n",
    "\n",
    "    #8.create_input_account_price_plan_view\n",
    "    create_input_account_price_plan_view_op = create_input_account_price_plan_view(\n",
    "        score_date=SCORE_DATE,\n",
    "        score_date_delta=SCORE_DATE_DELTA,\n",
    "        view_name=PRICE_PLAN_VIEW_NAME ,\n",
    "        dataset_id=DATASET_ID,\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        resource_bucket=RESOURCE_BUCKET,\n",
    "        query_path=ACCOUNT_PRICE_PLAN_QUERY_PATH \n",
    "    )\n",
    "\n",
    "    # create_input_account_price_plan_view_op.set_memory_limit('16G')\n",
    "    # create_input_account_price_plan_view_op.set_cpu_limit('4')\n",
    "\n",
    "    #9.create_input_account_clckstrm_telus_view\n",
    "    create_input_account_clckstrm_telus_view_op = create_input_account_clckstrm_telus_view(\n",
    "        score_date=SCORE_DATE,\n",
    "        score_date_delta=SCORE_DATE_DELTA,\n",
    "        view_name=CLCKSTRM_TELUS_VIEW_NAME,\n",
    "        dataset_id=DATASET_ID,\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        resource_bucket=RESOURCE_BUCKET,\n",
    "        query_path=ACCOUNT_CLCKSTRM_TELUS_QUERY_PATH\n",
    "    )\n",
    "\n",
    "    # create_input_account_clckstrm_telus_view_op.set_memory_limit('16G')\n",
    "    # create_input_account_clckstrm_telus_view_op.set_cpu_limit('4')\n",
    "\n",
    "    #10.create_input_account_call_history_view\n",
    "    create_input_account_call_history_view_op = create_input_account_call_history_view(\n",
    "        score_date=SCORE_DATE,\n",
    "        score_date_delta=SCORE_DATE_DELTA,\n",
    "        view_name=CALL_HISTORY_VIEW_NAME,\n",
    "        dataset_id=DATASET_ID,\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        resource_bucket=RESOURCE_BUCKET,\n",
    "        query_path=ACCOUNT_CALL_HISTORY_QUERY_PATH \n",
    "    )\n",
    "\n",
    "    # create_input_account_call_history_view_op.set_memory_limit('16G')\n",
    "    # create_input_account_call_history_view_op.set_cpu_limit('4')\n",
    "\n",
    "    # ----- preprocessing train data --------\n",
    "    preprocess_train_op = preprocess(\n",
    "        promo_expiry_list_view = PROMO_EXPIRY_LIST_VIEW_NAME, \n",
    "        account_consl_view=CONSL_VIEW_NAME,\n",
    "        account_bill_view=FFH_BILLING_VIEW_NAME,\n",
    "        account_discounts_view=FFH_DISCOUNTS_VIEW_NAME, \n",
    "        hs_usage_view=HS_USAGE_VIEW_NAME,\n",
    "        demo_income_view=DEMO_INCOME_VIEW_NAME,\n",
    "        gpon_copper_view=GPON_COPPER_VIEW_NAME,\n",
    "        price_plan_view=PRICE_PLAN_VIEW_NAME,\n",
    "        clckstrm_telus_view=CLCKSTRM_TELUS_VIEW_NAME, \n",
    "        call_history_view=CALL_HISTORY_VIEW_NAME, \n",
    "        save_data_path='gs://{}/{}_train.csv.gz'.format(RESOURCE_BUCKET, SERVICE_TYPE),\n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID\n",
    "    )\n",
    "\n",
    "    # preprocess_train_op.set_memory_limit('128G')\n",
    "    # preprocess_train_op.set_cpu_limit('32')\n",
    "\n",
    "    create_input_account_promo_expiry_list_view_op\n",
    "    create_input_account_consl_view_op\n",
    "    create_input_account_ffh_billing_view_op\n",
    "    create_input_account_ffh_discounts_view_op\n",
    "    create_input_account_hs_usage_view_op\n",
    "    create_input_account_demo_income_view_op\n",
    "    create_input_account_gpon_copper_view_op\n",
    "    create_input_account_price_plan_view_op\n",
    "    create_input_account_clckstrm_telus_view_op\n",
    "    create_input_account_call_history_view_op\n",
    "    preprocess_train_op\n",
    "\n",
    "    # --------------- validation view ops ---------------\n",
    "    #1.create_input_account_promo_expiry_list_view\n",
    "    create_input_account_promo_expiry_list_validation_view_op = create_input_account_promo_expiry_list_view(\n",
    "        view_name=PROMO_EXPIRY_LIST_VIEW_VALIDATION_NAME,\n",
    "        score_date=SCORE_DATE_VAL,\n",
    "        score_date_delta=SCORE_DATE_VAL_DELTA,\n",
    "        promo_expiry_start = PROMO_EXPIRY_START_VAL, \n",
    "        promo_expiry_end = PROMO_EXPIRY_END_VAL, \n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID,\n",
    "        region=REGION,\n",
    "        resource_bucket=RESOURCE_BUCKET,\n",
    "        query_path=ACCOUNT_PROMO_EXPIRY_LIST_QUERY_PATH\n",
    "    )\n",
    "    # create_input_account_promo_expiry_list_validation_view_op.set_memory_limit('16G')\n",
    "    # create_input_account_promo_expiry_list_validation_view_op.set_cpu_limit('4')\n",
    "\n",
    "    #2.create_input_account_consl_view\n",
    "    create_input_account_consl_validation_view_op = create_input_account_consl_view(\n",
    "        view_name=CONSL_VIEW_VALIDATION_NAME, \n",
    "        score_date=SCORE_DATE_VAL,\n",
    "        score_date_delta=SCORE_DATE_VAL_DELTA,\n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID,\n",
    "        region=REGION,\n",
    "        resource_bucket=RESOURCE_BUCKET,\n",
    "        query_path=ACCOUNT_CONSL_QUERY_PATH\n",
    "    )\n",
    "    # create_input_account_consl_validation_view_op.set_memory_limit('16G')\n",
    "    # create_input_account_consl_validation_view_op.set_cpu_limit('4')\n",
    "\n",
    "    #3.create_input_account_ffh_billing_view\n",
    "    create_input_account_ffh_billing_validation_view_op = create_input_account_ffh_billing_view(\n",
    "        v_report_date=SCORE_DATE_VAL_DASH,\n",
    "        v_start_date=SCORE_DATE_VAL_MINUS_6_MOS_DASH,\n",
    "        v_end_date=SCORE_DATE_VAL_LAST_MONTH_END_DASH,\n",
    "        v_bill_year=SCORE_DATE_VAL_LAST_MONTH_YEAR,\n",
    "        v_bill_month=SCORE_DATE_VAL_LAST_MONTH_MONTH,\n",
    "        view_name=FFH_BILLING_VIEW_VALIDATION_NAME,\n",
    "        dataset_id=DATASET_ID,\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        resource_bucket=RESOURCE_BUCKET,\n",
    "        query_path=ACCOUNT_FFH_BILLING_QUERY_PATH \n",
    "    )\n",
    "\n",
    "    # create_input_account_ffh_billing_validation_view_op.set_memory_limit('16G')\n",
    "    # create_input_account_ffh_billing_validation_view_op.set_cpu_limit('4')\n",
    "\n",
    "    #4.create_input_account_ffh_discounts_view\n",
    "    create_input_account_ffh_discounts_validation_view_op = create_input_account_ffh_discounts_view(\n",
    "        view_name=FFH_DISCOUNTS_VIEW_VALIDATION_NAME, \n",
    "        score_date=SCORE_DATE_VAL,\n",
    "        score_date_delta=SCORE_DATE_VAL_DELTA,\n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID,\n",
    "        region=REGION,\n",
    "        resource_bucket=RESOURCE_BUCKET,\n",
    "        query_path=ACCOUNT_FFH_DISCOUNTS_QUERY_PATH\n",
    "    )\n",
    "\n",
    "    # create_input_account_ffh_discounts_validation_view_op.set_memory_limit('16G')\n",
    "    # create_input_account_ffh_discounts_validation_view_op.set_cpu_limit('4')\n",
    "\n",
    "    #5.create_input_account_hs_usage_view\n",
    "    create_input_account_hs_usage_validation_view_op = create_input_account_hs_usage_view(\n",
    "        v_report_date=SCORE_DATE_VAL_DASH,\n",
    "        v_start_date=SCORE_DATE_VAL_MINUS_6_MOS_DASH,\n",
    "        v_end_date=SCORE_DATE_VAL_LAST_MONTH_END_DASH,\n",
    "        v_bill_year=SCORE_DATE_VAL_LAST_MONTH_YEAR,\n",
    "        v_bill_month=SCORE_DATE_VAL_LAST_MONTH_MONTH,\n",
    "        view_name=HS_USAGE_VIEW_VALIDATION_NAME,\n",
    "        dataset_id=DATASET_ID,\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        resource_bucket=RESOURCE_BUCKET,\n",
    "        query_path=ACCOUNT_HS_USAGE_QUERY_PATH \n",
    "    )\n",
    "\n",
    "    # create_input_account_hs_usage_validation_view_op.set_memory_limit('16G')\n",
    "    # create_input_account_hs_usage_validation_view_op.set_cpu_limit('4')\n",
    "\n",
    "    #6.create_input_account_demo_income_view\n",
    "    create_input_account_demo_income_validation_view_op = create_input_account_demo_income_view(\n",
    "        score_date=SCORE_DATE_VAL,\n",
    "        score_date_delta=SCORE_DATE_VAL_DELTA,\n",
    "        view_name=DEMO_INCOME_VIEW_VALIDATION_NAME,\n",
    "        dataset_id=DATASET_ID,\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        resource_bucket=RESOURCE_BUCKET,\n",
    "        query_path=ACCOUNT_DEMO_INCOME_QUERY_PATH\n",
    "    )\n",
    "\n",
    "    # create_input_account_demo_income_validation_view_op.set_memory_limit('16G')\n",
    "    # create_input_account_demo_income_validation_view_op.set_cpu_limit('4')\n",
    "\n",
    "    #7.create_input_account_gpon_copper_view\n",
    "    create_input_account_gpon_copper_validation_view_op = create_input_account_gpon_copper_view(\n",
    "        score_date=SCORE_DATE_VAL,\n",
    "        score_date_delta=SCORE_DATE_VAL_DELTA,\n",
    "        view_name=GPON_COPPER_VIEW_VALIDATION_NAME,\n",
    "        dataset_id=DATASET_ID,\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        resource_bucket=RESOURCE_BUCKET,\n",
    "        query_path=ACCOUNT_GPON_COPPER_QUERY_PATH\n",
    "    )\n",
    "\n",
    "    # create_input_account_gpon_copper_validation_view_op.set_memory_limit('16G')\n",
    "    # create_input_account_gpon_copper_validation_view_op.set_cpu_limit('4')\n",
    "\n",
    "    #8.create_input_account_price_plan_view\n",
    "    create_input_account_price_plan_validation_view_op = create_input_account_price_plan_view(\n",
    "        score_date=SCORE_DATE_VAL,\n",
    "        score_date_delta=SCORE_DATE_VAL_DELTA,\n",
    "        view_name=PRICE_PLAN_VIEW_VALIDATION_NAME,\n",
    "        dataset_id=DATASET_ID,\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        resource_bucket=RESOURCE_BUCKET,\n",
    "        query_path=ACCOUNT_PRICE_PLAN_QUERY_PATH\n",
    "    )\n",
    "\n",
    "    # create_input_account_price_plan_validation_view_op.set_memory_limit('16G')\n",
    "    # create_input_account_price_plan_validation_view_op.set_cpu_limit('4')\n",
    "\n",
    "    #9.create_input_account_clckstrm_telus_view\n",
    "    create_input_account_clckstrm_telus_validation_view_op = create_input_account_clckstrm_telus_view(\n",
    "        score_date=SCORE_DATE_VAL,\n",
    "        score_date_delta=SCORE_DATE_VAL_DELTA,\n",
    "        view_name=CLCKSTRM_TELUS_VIEW_VALIDATION_NAME,\n",
    "        dataset_id=DATASET_ID,\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        resource_bucket=RESOURCE_BUCKET,\n",
    "        query_path=ACCOUNT_CLCKSTRM_TELUS_QUERY_PATH\n",
    "    )\n",
    "\n",
    "    # create_input_account_clckstrm_telus_validation_view_op.set_memory_limit('16G')\n",
    "    # create_input_account_clckstrm_telus_validation_view_op.set_cpu_limit('4')\n",
    "\n",
    "    #10.create_input_account_call_history_view\n",
    "    create_input_account_call_history_validation_view_op = create_input_account_call_history_view(\n",
    "        score_date=SCORE_DATE_VAL,\n",
    "        score_date_delta=SCORE_DATE_VAL_DELTA,\n",
    "        view_name=CALL_HISTORY_VIEW_VALIDATION_NAME,\n",
    "        dataset_id=DATASET_ID,\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        resource_bucket=RESOURCE_BUCKET,\n",
    "        query_path=ACCOUNT_CALL_HISTORY_QUERY_PATH \n",
    "    )\n",
    "\n",
    "    # create_input_account_call_history_validation_view_op.set_memory_limit('16G')\n",
    "    # create_input_account_call_history_validation_view_op.set_cpu_limit('4')\n",
    "\n",
    "    # ----- preprocessing validation data --------\n",
    "    preprocess_validation_op = preprocess(\n",
    "        promo_expiry_list_view = PROMO_EXPIRY_LIST_VIEW_VALIDATION_NAME, \n",
    "        account_consl_view=CONSL_VIEW_VALIDATION_NAME,\n",
    "        account_bill_view=FFH_BILLING_VIEW_VALIDATION_NAME,\n",
    "        account_discounts_view=FFH_DISCOUNTS_VIEW_VALIDATION_NAME, \n",
    "        hs_usage_view=HS_USAGE_VIEW_VALIDATION_NAME,\n",
    "        demo_income_view=DEMO_INCOME_VIEW_VALIDATION_NAME,\n",
    "        gpon_copper_view=GPON_COPPER_VIEW_VALIDATION_NAME,\n",
    "        price_plan_view=PRICE_PLAN_VIEW_VALIDATION_NAME,\n",
    "        clckstrm_telus_view=CLCKSTRM_TELUS_VIEW_VALIDATION_NAME, \n",
    "        call_history_view=CALL_HISTORY_VIEW_VALIDATION_NAME, \n",
    "        save_data_path='gs://{}/{}_validation.csv.gz'.format(RESOURCE_BUCKET, SERVICE_TYPE),\n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID\n",
    "    )\n",
    "\n",
    "    # preprocess_validation_op.set_memory_limit('256G')\n",
    "    # preprocess_validation_op.set_cpu_limit('32')\n",
    "\n",
    "    create_input_account_promo_expiry_list_validation_view_op\n",
    "    create_input_account_consl_validation_view_op\n",
    "    create_input_account_ffh_billing_validation_view_op\n",
    "    create_input_account_ffh_discounts_validation_view_op\n",
    "    create_input_account_hs_usage_validation_view_op\n",
    "    create_input_account_demo_income_validation_view_op\n",
    "    create_input_account_gpon_copper_validation_view_op\n",
    "    create_input_account_price_plan_validation_view_op\n",
    "    create_input_account_clckstrm_telus_validation_view_op\n",
    "    create_input_account_call_history_validation_view_op\n",
    "    preprocess_train_op\n",
    "\n",
    "    train_and_save_model_op = train_and_save_model(resource_bucket=RESOURCE_BUCKET,\n",
    "                                                   service_type=SERVICE_TYPE,\n",
    "                                                   score_date_dash=SCORE_DATE_DASH,\n",
    "                                                   score_date_val_dash=SCORE_DATE_VAL_DASH,\n",
    "                                                   project_id=PROJECT_ID,\n",
    "                                                   dataset_id=DATASET_ID,\n",
    "                                                   )\n",
    "    \n",
    "    train_and_save_model_op\n",
    "    \n",
    "#     train_and_save_model_op.set_memory_limit('256G')\n",
    "#     train_and_save_model_op.set_cpu_limit('32')\n",
    "\n",
    "#     train_and_save_model_op.after(preprocess_train_op)\n",
    "#     train_and_save_model_op.after(preprocess_validation_op)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2a51c8-4c37-4392-a21a-c4c3279d36d4",
   "metadata": {},
   "source": [
    "### Run the Pipeline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5313e5-b5cd-4a78-9df5-91af24168e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline(project_id = PROJECT_ID,\n",
    "#         region = REGION,\n",
    "#         resource_bucket = RESOURCE_BUCKET,\n",
    "#         file_bucket = FILE_BUCKET)\n",
    "\n",
    "\n",
    "pipeline(project_id = PROJECT_ID,\n",
    "        region = REGION,\n",
    "        resource_bucket = RESOURCE_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27465a63-5c3b-4e96-9c08-f9cc5dafde90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kfp.v2 import compiler\n",
    "# from google.cloud.aiplatform import pipeline_jobs\n",
    "\n",
    "# import json\n",
    "\n",
    "# compiler.Compiler().compile(\n",
    "#    pipeline_func=pipeline, package_path=\"pipeline.json\"\n",
    "# )\n",
    "\n",
    "# job = pipeline_jobs.PipelineJob(\n",
    "#                                display_name=PIPELINE_NAME,\n",
    "#                                template_path=\"pipeline.json\",\n",
    "#                                location=REGION,\n",
    "#                                enable_caching=False,\n",
    "#                                pipeline_root = f\"gs://{RESOURCE_BUCKET}\"\n",
    "# )\n",
    "# job.run(\n",
    "#    service_account = f\"bilayer-sa@{PROJECT_ID}.iam.gserviceaccount.com\"\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
