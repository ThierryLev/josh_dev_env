name: Bq create dataset
inputs:
- {name: score_date, type: String}
- {name: score_date_delta, type: Integer}
- {name: project_id, type: String}
- {name: dataset_id, type: String}
- {name: region, type: String}
- {name: promo_expiry_start, type: String}
- {name: promo_expiry_end, type: String}
- {name: v_start_date, type: String}
- {name: v_end_date, type: String}
outputs:
- {name: col_list, type: JsonArray}
implementation:
  container:
    image: northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/wb-platform/pipelines/kubeflow-pycaret:latest
    command:
    - sh
    - -c
    - |2

      if ! [ -x "$(command -v pip)" ]; then
          python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
      fi

      PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.11' && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp -d)
      printf "%s" "$0" > "$program_path/ephemeral_component.py"
      python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
    - "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing\
      \ import *\n\ndef bq_create_dataset(score_date: str,\n                     \
      \ score_date_delta: int,\n                      project_id: str,\n         \
      \             dataset_id: str,\n                      region: str,\n       \
      \               promo_expiry_start: str, \n                      promo_expiry_end:\
      \ str, \n                      v_start_date: str,\n                      v_end_date:\
      \ str) -> NamedTuple(\"output\", [(\"col_list\", list)]):\n\n    from google.cloud\
      \ import bigquery\n    import logging \n    from datetime import datetime\n\
      \    # For wb\n    # import google.oauth2.credentials\n    # CREDENTIALS = google.oauth2.credentials.Credentials(token)\n\
      \n    def get_gcp_bqclient(project_id, use_local_credential=True):\n       \
      \ token = os.popen('gcloud auth print-access-token').read()\n        token =\
      \ re.sub(f'\\n$', '', token)\n        credentials = google.oauth2.credentials.Credentials(token)\n\
      \n        bq_client = bigquery.Client(project=project_id)\n        if use_local_credential:\n\
      \            bq_client = bigquery.Client(project=project_id, credentials=credentials)\n\
      \        return bq_client\n\n    client = get_gcp_bqclient(project_id)\n   \
      \ # client = bigquery.Client(project=project_id, location=region)\n    job_config\
      \ = bigquery.QueryJobConfig()\n\n    # Change dataset / table + sp table name\
      \ to version in bi-layer\n    query =\\\n        f'''\n            DECLARE score_date\
      \ DATE DEFAULT \"{score_date}\";\n            DECLARE promo_expiry_start DATE\
      \ DEFAULT \"{promo_expiry_start}\";\n            DECLARE promo_expiry_end DATE\
      \ DEFAULT \"{promo_expiry_end}\";\n            DECLARE start_date DATE DEFAULT\
      \ \"{v_start_date}\";\n            DECLARE end_date DATE DEFAULT \"{v_end_date}\"\
      ;\n\n            -- Change dataset / sp name to the version in the bi_layer\n\
      \            CALL {dataset_id}.bq_sp_ctr_pipeline_dataset(score_date, promo_expiry_start,\
      \ promo_expiry_end, start_date, end_date);\n\n            SELECT\n         \
      \       *\n            FROM {dataset_id}.INFORMATION_SCHEMA.PARTITIONS\n   \
      \         WHERE table_name='bq_ctr_pipeline_dataset'\n\n        '''\n\n    df\
      \ = client.query(query, job_config=job_config).to_dataframe()\n    logging.info(df.to_string())\n\
      \n    logging.info(f\"Loaded {df.total_rows[0]} rows into \\\n             {df.table_catalog[0]}.{df.table_schema[0]}.{df.table_name[0]}\
      \ on \\\n             {datetime.strftime((df.last_modified_time[0]), '%Y-%m-%d\
      \ %H:%M:%S') } !\")\n\n    ######################################## Save column\
      \ list_##########################\n    query =\\\n        f'''\n           SELECT\n\
      \                *\n            FROM {dataset_id}.bq_ctr_pipeline_dataset\n\n\
      \        '''\n\n    df = client.query(query, job_config=job_config).to_dataframe()\n\
      \n    col_list = list([ col for col in df.column_name])\n    return (col_list,)\n\
      \n"
    args:
    - --executor_input
    - {executorInput: null}
    - --function_to_execute
    - bq_create_dataset
