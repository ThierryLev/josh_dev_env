{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c150da3-6e1e-4f02-a778-bb3b12af9054",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b559b7e6-fe51-49cc-8e9c-832380843832",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output, OutputPath, ClassificationMetrics,\n",
    "                        Metrics, component)\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import google\n",
    "from google.oauth2 import credentials\n",
    "from google.oauth2 import service_account\n",
    "from google.oauth2.service_account import Credentials\n",
    "from google.cloud import storage\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google_cloud_pipeline_components.v1.batch_predict_job import \\\n",
    "    ModelBatchPredictOp as batch_prediction_op\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558ff533-70c8-4421-813e-c567d6bc496b",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f70d721-76e7-4c2e-8153-88b5bbe8ee47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tag cell with parameters\n",
    "PROJECT_ID =  ''\n",
    "BUCKET_NAME=''\n",
    "DATASET_ID = ''\n",
    "RESOURCE_BUCKET = ''\n",
    "FILE_BUCKET = ''\n",
    "REGION = ''\n",
    "MODEL_ID = '5090'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7035200a-de6b-412c-8406-64854b639cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag cell with parameters\n",
    "PROJECT_ID =  'divg-josh-pr-d1cc3a'\n",
    "BUCKET_NAME='divg-josh-pr-d1cc3a-default'\n",
    "DATASET_ID = 'call_to_retention_dataset'\n",
    "RESOURCE_BUCKET = 'divg-josh-pr-d1cc3a-default'\n",
    "FILE_BUCKET = 'divg-josh-pr-d1cc3a-default'\n",
    "MODEL_ID = '5090'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eeed21-e12b-45ee-b1e4-d0c8f3843533",
   "metadata": {},
   "source": [
    "### Service Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65d1668-e1d8-4f58-958f-edc44d06a6b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SERVICE_TYPE = 'call_to_retention'\n",
    "SERVICE_TYPE_NAME = 'call-to-retention'\n",
    "TABLE_ID = 'bq_call_to_retention_targets'\n",
    "REGION = \"northamerica-northeast1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2229e5d1-1362-40ed-aa7d-cb22e41e4960",
   "metadata": {},
   "source": [
    "### Pulumi Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d628b-e759-4c9c-aab5-568c54721aaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "STACK_NAME = 'call_to_retention'\n",
    "TRAIN_PIPELINE_NAME_PATH = 'train_pipeline'\n",
    "PREDICT_PIPELINE_NAME_PATH = 'predict_pipeline'\n",
    "TRAIN_PIPELINE_NAME = 'call-to-retention-train-pipeline' # Same name as pulumi.yaml\n",
    "PREDICT_PIPELINE_NAME = 'call-to-retention-predict-pipeline' # Same name as pulumi.yaml\n",
    "TRAIN_PIPELINE_DESCRIPTION = 'call-to-retention-train-pipeline'\n",
    "PREDICT_PIPELINE_DESCRIPTION = 'call-to-retention-predict-pipeline'\n",
    "REGION = \"northamerica-northeast1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85558b11-be96-49ea-91c5-d17b15c00bf1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Query + Pre-Processing Component Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370852f7-2e23-4ac3-8cd8-b40c39be85ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_QUERIES_PATH = f\"{STACK_NAME}/{TRAIN_PIPELINE_NAME_PATH}/queries/\" \n",
    "TRAIN_UTILS_FILE_PATH = f\"{STACK_NAME}/{TRAIN_PIPELINE_NAME_PATH}/utils\" \n",
    "UTILS_FILENAME = 'utils.py'\n",
    "\n",
    "PROCESSED_SERVING_DATA_TABLENAME = 'processed_serving_data'\n",
    "INPUT_SERVING_DATA_TABLENAME = 'input_serving_data'\n",
    "\n",
    "QUERY_DATE = (date.today() - relativedelta(days=1)).strftime('%Y-%m-%d')\n",
    "TARGET_TABLE_REF = '{}.{}.{}'.format(PROJECT_ID, DATASET_ID, TABLE_ID)\n",
    "\n",
    "QUERIES_PATH = 'call_to_retention/queries/'\n",
    "\n",
    "#Query Paths\n",
    "ACCOUNT_PROMO_EXPIRY_LIST_QUERY_PATH = QUERIES_PATH + 'create_input_account_promo_expiry_list_query.sql'\n",
    "ACCOUNT_CONSL_QUERY_PATH = QUERIES_PATH + 'create_input_account_consl_query.sql'\n",
    "ACCOUNT_FFH_BILLING_QUERY_PATH = QUERIES_PATH + 'create_input_account_ffh_billing_query.sql'\n",
    "ACCOUNT_FFH_DISCOUNTS_QUERY_PATH = QUERIES_PATH + 'create_input_account_ffh_discounts_query.sql'\n",
    "ACCOUNT_HS_USAGE_QUERY_PATH = QUERIES_PATH + 'create_input_account_hs_usage_query.sql'\n",
    "ACCOUNT_DEMO_INCOME_QUERY_PATH = QUERIES_PATH + 'create_input_account_demo_income_query.sql'\n",
    "ACCOUNT_GPON_COPPER_QUERY_PATH = QUERIES_PATH + 'create_input_account_gpon_copper_query.sql'\n",
    "ACCOUNT_PRICE_PLAN_QUERY_PATH = QUERIES_PATH + 'create_input_account_price_plan_query.sql'\n",
    "ACCOUNT_CLCKSTRM_TELUS_QUERY_PATH = QUERIES_PATH + 'create_input_account_clckstrm_telus_query.sql'\n",
    "ACCOUNT_CALL_HISTORY_QUERY_PATH = QUERIES_PATH + 'create_input_account_call_history_query.sql'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc0bd98-1d58-491e-bc5e-bbac378c7bbf",
   "metadata": {},
   "source": [
    "### Import Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f442904b-a40a-4ba8-b237-e9894ffd4f26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download required component files to local\n",
    "prefix = f'{STACK_NAME}/{TRAIN_PIPELINE_NAME_PATH}/components/'\n",
    "dl_dir = 'components/'\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(RESOURCE_BUCKET)\n",
    "blobs = bucket.list_blobs(prefix=prefix)  # Get list of files\n",
    "for blob in blobs: # download each file that starts with \"prefix\" into \"dl_dir\"\n",
    "    if blob.name.endswith(\"/\"):\n",
    "        continue\n",
    "    file_split = blob.name.split(prefix)\n",
    "    file_path = f\"{dl_dir}{file_split[-1]}\"\n",
    "    directory = \"/\".join(file_path.split(\"/\")[0:-1])\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    blob.download_to_filename(file_path) \n",
    "\n",
    "# import main pipeline components\n",
    "import components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c315d454-416f-4eed-b12d-1acd5aed1207",
   "metadata": {},
   "source": [
    "### Date Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfad310-a482-4285-aa31-6d9113008be6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scoringDate = date(2022, 4, 1)  # date.today() - relativedelta(days=2)- relativedelta(months=30)\n",
    "valScoringDate = date(2022, 5, 1)  # scoringDate - relativedelta(days=2)\n",
    "\n",
    "# training views\n",
    "PROMO_EXPIRY_LIST_VIEW_NAME = '{}_pipeline_promo_expiry_list_data_training_bi_layer'.format(SERVICE_TYPE)  \n",
    "CONSL_VIEW_NAME = '{}_pipeline_consl_data_training_bi_layer'.format(SERVICE_TYPE)  \n",
    "FFH_BILLING_VIEW_NAME = '{}_pipeline_ffh_billing_data_training_bi_layer'.format(SERVICE_TYPE)  \n",
    "FFH_DISCOUNTS_VIEW_NAME = '{}_pipeline_ffh_discounts_data_training_bi_layer'.format(SERVICE_TYPE)  \n",
    "HS_USAGE_VIEW_NAME = '{}_pipeline_hs_usage_data_training_bi_layer'.format(SERVICE_TYPE)  \n",
    "DEMO_INCOME_VIEW_NAME = '{}_pipeline_demo_income_data_training_bi_layer'.format(SERVICE_TYPE)  \n",
    "GPON_COPPER_VIEW_NAME = '{}_pipeline_gpon_copper_data_training_bi_layer'.format(SERVICE_TYPE)  \n",
    "PRICE_PLAN_VIEW_NAME = '{}_pipeline_price_plan_data_training_bi_layer'.format(SERVICE_TYPE)  \n",
    "CLCKSTRM_TELUS_VIEW_NAME = '{}_pipeline_clckstrm_telus_training_bi_layer'.format(SERVICE_TYPE)\n",
    "CALL_HISTORY_VIEW_NAME = '{}_pipeline_call_history_data_training_bi_layer'.format(SERVICE_TYPE)  \n",
    "\n",
    "# validation views\n",
    "PROMO_EXPIRY_LIST_VIEW_VALIDATION_NAME = '{}_pipeline_promo_expiry_list_data_validation_bi_layer'.format(SERVICE_TYPE)  \n",
    "CONSL_VIEW_VALIDATION_NAME = '{}_pipeline_consl_data_validation_bi_layer'.format(SERVICE_TYPE)  \n",
    "FFH_BILLING_VIEW_VALIDATION_NAME = '{}_pipeline_ffh_billing_data_validation_bi_layer'.format(SERVICE_TYPE)  \n",
    "FFH_DISCOUNTS_VIEW_VALIDATION_NAME = '{}_pipeline_ffh_discounts_data_validation_bi_layer'.format(SERVICE_TYPE)  \n",
    "HS_USAGE_VIEW_VALIDATION_NAME = '{}_pipeline_hs_usage_data_validation_bi_layer'.format(SERVICE_TYPE)  \n",
    "DEMO_INCOME_VIEW_VALIDATION_NAME = '{}_pipeline_demo_income_data_validation_bi_layer'.format(SERVICE_TYPE)  \n",
    "GPON_COPPER_VIEW_VALIDATION_NAME = '{}_pipeline_gpon_copper_data_validation_bi_layer'.format(SERVICE_TYPE)  \n",
    "PRICE_PLAN_VIEW_VALIDATION_NAME = '{}_pipeline_price_plan_data_validation_bi_layer'.format(SERVICE_TYPE)  \n",
    "CLCKSTRM_TELUS_VIEW_VALIDATION_NAME = '{}_pipeline_clckstrm_telus_validation_bi_layer'.format(SERVICE_TYPE)\n",
    "CALL_HISTORY_VIEW_VALIDATION_NAME = '{}_pipeline_call_history_data_validation_bi_layer'.format(SERVICE_TYPE)  \n",
    "\n",
    "# training dates\n",
    "SCORE_DATE = scoringDate.strftime('%Y%m%d')  # date.today().strftime('%Y%m%d')\n",
    "SCORE_DATE_DASH = scoringDate.strftime('%Y-%m-%d')\n",
    "SCORE_DATE_MINUS_6_MOS_DASH = ((scoringDate - relativedelta(months=6)).replace(day=1)).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_THIS_MONTH_START_DASH = scoringDate.replace(day=1)\n",
    "SCORE_DATE_THIS_MONTH_END_DASH = (((scoringDate.replace(day=1)) + relativedelta(months=1)).replace(day=1) - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_LAST_MONTH_START_DASH = (scoringDate.replace(day=1) - timedelta(days=1)).replace(day=1).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_LAST_MONTH_END_DASH = ((scoringDate.replace(day=1)) - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_LAST_MONTH_YEAR = ((scoringDate.replace(day=1)) - timedelta(days=1)).year\n",
    "SCORE_DATE_LAST_MONTH_MONTH = ((scoringDate.replace(day=1)) - timedelta(days=1)).month\n",
    "PROMO_EXPIRY_START = (scoringDate.replace(day=1) + relativedelta(months=3)).replace(day=1).strftime('%Y%m%d')\n",
    "PROMO_EXPIRY_END = (scoringDate.replace(day=1) + relativedelta(months=4)).replace(day=1).strftime('%Y%m%d')\n",
    "\n",
    "# validation dates\n",
    "SCORE_DATE_VAL = valScoringDate.strftime('%Y%m%d')\n",
    "SCORE_DATE_VAL_DASH = valScoringDate.strftime('%Y-%m-%d')\n",
    "SCORE_DATE_VAL_MINUS_6_MOS_DASH = ((valScoringDate - relativedelta(months=6)).replace(day=1)).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_VAL_THIS_MONTH_START_DASH = valScoringDate.replace(day=1)\n",
    "SCORE_DATE_VAL_THIS_MONTH_END_DASH = (((valScoringDate.replace(day=1)) + relativedelta(months=1)).replace(day=1) - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_VAL_LAST_MONTH_START_DASH = (valScoringDate.replace(day=1) - timedelta(days=1)).replace(day=1).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_VAL_LAST_MONTH_END_DASH = ((valScoringDate.replace(day=1)) - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_VAL_LAST_MONTH_YEAR = ((valScoringDate.replace(day=1)) - timedelta(days=1)).year\n",
    "SCORE_DATE_VAL_LAST_MONTH_MONTH = ((valScoringDate.replace(day=1)) - timedelta(days=1)).month\n",
    "PROMO_EXPIRY_START_VAL = (valScoringDate.replace(day=1) + relativedelta(months=3)).replace(day=1).strftime('%Y%m%d')\n",
    "PROMO_EXPIRY_END_VAL = (valScoringDate.replace(day=1) + relativedelta(months=4)).replace(day=1).strftime('%Y%m%d')\n",
    "\n",
    "SCORE_DATE_DELTA = 0\n",
    "SCORE_DATE_VAL_DELTA = 0\n",
    "TICKET_DATE_WINDOW = 30  # Days of ticket data to be queried\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330396ff-dfe9-4c23-9d67-922e9a190bdc",
   "metadata": {},
   "source": [
    "### Train and Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f33719-127b-45c7-bccd-dcd3970b003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_model(\n",
    "            file_bucket: str,\n",
    "            service_type: str,\n",
    "            score_date_dash: str,\n",
    "            score_date_val_dash: str,\n",
    "            project_id: str,\n",
    "            dataset_id: str\n",
    "):\n",
    "\n",
    "    import gc\n",
    "    import time\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    from google.cloud import storage\n",
    "    from google.cloud import bigquery\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    def get_lift(prob, y_test, q):\n",
    "        result = pd.DataFrame(columns=['Prob', 'Call_To_Retention'])\n",
    "        result['Prob'] = prob\n",
    "        result['Call_To_Retention'] = y_test\n",
    "        result['Decile'] = pd.qcut(result['Prob'], q, labels=[i for i in range(q, 0, -1)])\n",
    "        add = pd.DataFrame(result.groupby('Decile')['Call_To_Retention'].mean()).reset_index()\n",
    "        add.columns = ['Decile', 'avg_real_call_to_retention_rate']\n",
    "        result = result.merge(add, on='Decile', how='left')\n",
    "        result.sort_values('Decile', ascending=True, inplace=True)\n",
    "        lg = pd.DataFrame(result.groupby('Decile')['Prob'].mean()).reset_index()\n",
    "        lg.columns = ['Decile', 'avg_model_pred_call_to_retention_rate']\n",
    "        lg.sort_values('Decile', ascending=False, inplace=True)\n",
    "        lg['avg_call_to_retention_rate_total'] = result['Call_To_Retention'].mean()\n",
    "        lg = lg.merge(add, on='Decile', how='left')\n",
    "        lg['lift'] = lg['avg_real_call_to_retention_rate'] / lg['avg_call_to_retention_rate_total']\n",
    "\n",
    "        return lg\n",
    "\n",
    "    df_train = pd.read_csv('gs://{}/{}_train.csv.gz'.format(file_bucket, service_type),\n",
    "                           compression='gzip')  \n",
    "    df_test = pd.read_csv('gs://{}/{}_validation.csv.gz'.format(file_bucket, service_type),  \n",
    "                          compression='gzip')\n",
    "\n",
    "    #set up df_train\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    sql_train = ''' SELECT * FROM `{}.{}.bq_call_to_retention_targets` '''.format(project_id, dataset_id) \n",
    "    df_target_train = client.query(sql_train).to_dataframe()\n",
    "    df_target_train = df_target_train.loc[\n",
    "        df_target_train['YEAR_MONTH'] == '-'.join(score_date_dash.split('-')[:2])]  # score_date_dash = '2022-08-31'\n",
    "    df_target_train['ban'] = df_target_train['ban'].astype('int64')\n",
    "    df_target_train = df_target_train.groupby('ban').tail(1)\n",
    "    df_train = df_train.merge(df_target_train[['ban', 'target_ind']], on='ban', how='left')\n",
    "    df_train.rename(columns={'target_ind': 'target'}, inplace=True)\n",
    "    df_train.dropna(subset=['target'], inplace=True)\n",
    "    df_train['target'] = df_train['target'].astype(int)\n",
    "    print(df_train.shape)\n",
    "\n",
    "    #set up df_test\n",
    "    sql_test = ''' SELECT * FROM `{}.{}.bq_call_to_retention_targets` '''.format(project_id, dataset_id) \n",
    "    df_target_test = client.query(sql_test).to_dataframe()\n",
    "    df_target_test = df_target_test.loc[\n",
    "        df_target_test['YEAR_MONTH'] == '-'.join(score_date_val_dash.split('-')[:2])]  # score_date_dash = '2022-09-30'\n",
    "    df_target_test['ban'] = df_target_test['ban'].astype('int64')\n",
    "    df_target_test = df_target_test.groupby('ban').tail(1)\n",
    "    df_test = df_test.merge(df_target_test[['ban', 'target_ind']], on='ban', how='left')\n",
    "    df_test.rename(columns={'target_ind': 'target'}, inplace=True)\n",
    "    df_test.dropna(subset=['target'], inplace=True)\n",
    "    df_test['target'] = df_test['target'].astype(int)\n",
    "    print(df_test.shape)\n",
    "\n",
    "    #set up features (list)\n",
    "    cols_1 = df_train.columns.values\n",
    "    cols_2 = df_test.columns.values\n",
    "    cols = set(cols_1).intersection(set(cols_2))\n",
    "    features = [f for f in cols if f not in ['ban', 'target']]\n",
    "\n",
    "    #train test split\n",
    "    df_train, df_val = train_test_split(df_train, shuffle=True, test_size=0.2, random_state=42,\n",
    "                                        stratify=df_train['target']\n",
    "                                        )\n",
    "\n",
    "    ban_train = df_train['ban']\n",
    "    X_train = df_train[features]\n",
    "    y_train = np.squeeze(df_train['target'].values)\n",
    "\n",
    "    ban_val = df_val['ban']\n",
    "    X_val = df_val[features]\n",
    "    y_val = np.squeeze(df_val['target'].values)\n",
    "\n",
    "    ban_test = df_test['ban']\n",
    "    X_test = df_test[features]\n",
    "    y_test = np.squeeze(df_test['target'].values)\n",
    "\n",
    "    del df_train, df_val, df_test\n",
    "    gc.collect()\n",
    "\n",
    "    # build model and fit in training data\n",
    "    import xgboost as xgb\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        learning_rate=0.01,\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective='binary:logistic',\n",
    "        nthread=4,\n",
    "        scale_pos_weight=1\n",
    "        # seed=27\n",
    "    )\n",
    "\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    print('xgb training done')\n",
    "\n",
    "    from sklearn.preprocessing import normalize\n",
    "\n",
    "#     #predictions on X_val\n",
    "#     y_pred = xgb_model.predict_proba(X_val, ntree_limit=xgb_model.best_iteration)[:, 1]\n",
    "#     y_pred_label = (y_pred > 0.5).astype(int)\n",
    "#     auc = roc_auc_score(y_val, y_pred_label)\n",
    "#     metrics.log_metric(\"AUC\", auc)\n",
    "\n",
    "    pred_prb = xgb_model.predict_proba(X_test, ntree_limit=xgb_model.best_iteration)[:, 1]\n",
    "    lg = get_lift(pred_prb, y_test, 10)\n",
    "\n",
    "    # save the model in GCS\n",
    "    from datetime import datetime\n",
    "    models_dict = {}\n",
    "    create_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    models_dict['create_time'] = create_time\n",
    "    models_dict['model'] = xgb_model\n",
    "    models_dict['features'] = features\n",
    "    lg.to_csv('gs://{}/lift_on_scoring_data_{}.csv'.format(file_bucket, create_time, index=False))\n",
    "\n",
    "    with open('model_dict.pkl', 'wb') as handle:\n",
    "        pickle.dump(models_dict, handle)\n",
    "    handle.close()\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(file_bucket)\n",
    "\n",
    "    MODEL_PATH = '{}_xgb_models/'.format(service_type)\n",
    "    blob = bucket.blob(MODEL_PATH)\n",
    "    if not blob.exists(storage_client):\n",
    "        blob.upload_from_string('')\n",
    "\n",
    "    model_name_onbkt = '{}{}_models_xgb_{}'.format(MODEL_PATH, service_type, models_dict['create_time'])\n",
    "    blob = bucket.blob(model_name_onbkt)\n",
    "    blob.upload_from_filename('model_dict.pkl')\n",
    "\n",
    "    print(f\"....model loaded to GCS done at {str(create_time)}\")\n",
    "\n",
    "    time.sleep(120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9754fb-807c-470c-8c1f-c55093725b34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e2eea99-a3f0-48ea-8962-115edcaf26d9",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3cda87-dc1f-495e-be67-a3d623c13e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @dsl.pipeline(\n",
    "#     # A name for the pipeline.\n",
    "#     name=\"{}-xgb-pipeline\".format(SERVICE_TYPE_NAME),\n",
    "#     description=' pipeline for training {} model'.format(SERVICE_TYPE_NAME)\n",
    "# )\n",
    "def pipeline(\n",
    "        project_id: str = PROJECT_ID,\n",
    "        region: str = REGION,\n",
    "        resource_bucket: str = RESOURCE_BUCKET, \n",
    "        file_bucket: str = FILE_BUCKET\n",
    "    ):\n",
    "    train_and_save_model_op = train_and_save_model(file_bucket=FILE_BUCKET,\n",
    "                                                   service_type=SERVICE_TYPE,\n",
    "                                                   score_date_dash=SCORE_DATE_DASH,\n",
    "                                                   score_date_val_dash=SCORE_DATE_VAL_DASH,\n",
    "                                                   project_id=PROJECT_ID,\n",
    "                                                   dataset_id=DATASET_ID,\n",
    "                                                   )\n",
    "    \n",
    "    train_and_save_model_op\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2a51c8-4c37-4392-a21a-c4c3279d36d4",
   "metadata": {},
   "source": [
    "### Run the Pipeline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5313e5-b5cd-4a78-9df5-91af24168e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline(project_id = PROJECT_ID,\n",
    "#         region = REGION,\n",
    "#         resource_bucket = RESOURCE_BUCKET,\n",
    "#         file_bucket = FILE_BUCKET)\n",
    "\n",
    "pipeline(project_id = PROJECT_ID,\n",
    "        region = REGION,\n",
    "        resource_bucket = RESOURCE_BUCKET\n",
    "        , file_bucket = FILE_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27465a63-5c3b-4e96-9c08-f9cc5dafde90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kfp.v2 import compiler\n",
    "# from google.cloud.aiplatform import pipeline_jobs\n",
    "\n",
    "# import json\n",
    "\n",
    "# compiler.Compiler().compile(\n",
    "#    pipeline_func=pipeline, package_path=\"pipeline.json\"\n",
    "# )\n",
    "\n",
    "# job = pipeline_jobs.PipelineJob(\n",
    "#                                display_name=PIPELINE_NAME,\n",
    "#                                template_path=\"pipeline.json\",\n",
    "#                                location=REGION,\n",
    "#                                enable_caching=False,\n",
    "#                                pipeline_root = f\"gs://{RESOURCE_BUCKET}\"\n",
    "# )\n",
    "# job.run(\n",
    "#    service_account = f\"bilayer-sa@{PROJECT_ID}.iam.gserviceaccount.com\"\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
