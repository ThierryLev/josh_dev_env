{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c150da3-6e1e-4f02-a778-bb3b12af9054",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b559b7e6-fe51-49cc-8e9c-832380843832",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output, OutputPath, ClassificationMetrics,\n",
    "                        Metrics, component)\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import google\n",
    "from google.oauth2 import credentials\n",
    "from google.oauth2 import service_account\n",
    "from google.oauth2.service_account import Credentials\n",
    "from google.cloud import storage\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google_cloud_pipeline_components.v1.batch_predict_job import \\\n",
    "    ModelBatchPredictOp as batch_prediction_op\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558ff533-70c8-4421-813e-c567d6bc496b",
   "metadata": {},
   "source": [
    "### YAML Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f70d721-76e7-4c2e-8153-88b5bbe8ee47",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#tag cell with parameters\n",
    "PROJECT_ID =  ''\n",
    "BUCKET_NAME=''\n",
    "DATASET_ID = ''\n",
    "RESOURCE_BUCKET = ''\n",
    "FILE_BUCKET = ''\n",
    "REGION = ''\n",
    "MODEL_ID = '9999'\n",
    "MODEL_NAME = 'campaign_data_delivery'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb9c0d1-697a-4e34-92d5-125c106d6c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag cell with parameters\n",
    "PROJECT_ID =  'divg-josh-pr-d1cc3a'\n",
    "BUCKET_NAME='divg-josh-pr-d1cc3a-default'\n",
    "DATASET_ID = 'call_to_retention_dataset'\n",
    "RESOURCE_BUCKET = 'divg-josh-pr-d1cc3a-default'\n",
    "FILE_BUCKET = 'divg-josh-pr-d1cc3a-default'\n",
    "MODEL_ID = '5090'\n",
    "MODEL_NAME = 'call_to_retention'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eeed21-e12b-45ee-b1e4-d0c8f3843533",
   "metadata": {},
   "source": [
    "### Service Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65d1668-e1d8-4f58-958f-edc44d06a6b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SERVICE_TYPE = 'campaign_data_delivery'\n",
    "SERVICE_TYPE_NAME = 'campaign-data-delivery'\n",
    "TABLE_ID = 'bq_campaign_records_hcr'\n",
    "REGION = \"northamerica-northeast1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2229e5d1-1362-40ed-aa7d-cb22e41e4960",
   "metadata": {},
   "source": [
    "### Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d628b-e759-4c9c-aab5-568c54721aaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "STACK_NAME = 'campaign_data_delivery'\n",
    "SERVING_PIPELINE_NAME_PATH = 'campaign_data_delivery/serving_pipeline'\n",
    "SERVING_PIPELINE_NAME = 'campaign-data-delivery-serving-pipeline' # Same name as pulumi.yaml\n",
    "SERVING_PIPELINE_DESCRIPTION = 'campaign-data-delivery-serving-pipeline'\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET_NAME}\"\n",
    "REGION = \"northamerica-northeast1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b1da86-930b-4529-b7ab-7c14716ea3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATASET_TABLE_NAME = 'bq_campaign_data_element'\n",
    "TRAINING_DATASET_SP_NAME = 'bq_sp_campaign_data_delivery_hcr_em'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11873e8-e816-44a1-b8b4-10f4e88ec67b",
   "metadata": {},
   "source": [
    "### Utils Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fcf8b8-93e4-4c2e-9483-8b08e933a827",
   "metadata": {},
   "outputs": [],
   "source": [
    "UTILS_STACK_NAME = 'utils' \n",
    "UTILS_NAME_PATH = 'common_functions'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc0bd98-1d58-491e-bc5e-bbac378c7bbf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f442904b-a40a-4ba8-b237-e9894ffd4f26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download required component files to local\n",
    "prefix = f'{STACK_NAME}/{TRAIN_PIPELINE_NAME_PATH}/components/'\n",
    "dl_dir = 'components/'\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(RESOURCE_BUCKET)\n",
    "blobs = bucket.list_blobs(prefix=prefix)  # Get list of files\n",
    "for blob in blobs: # download each file that starts with \"prefix\" into \"dl_dir\"\n",
    "    if blob.name.endswith(\"/\"):\n",
    "        continue\n",
    "    file_split = blob.name.split(prefix)\n",
    "    file_path = f\"{dl_dir}{file_split[-1]}\"\n",
    "    directory = \"/\".join(file_path.split(\"/\")[0:-1])\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    blob.download_to_filename(file_path) \n",
    "\n",
    "# import main pipeline components\n",
    "from components.bq_create_dataset import bq_create_dataset\n",
    "from components.preprocess import preprocess\n",
    "from components.train_and_save_model import train_and_save_model\n",
    "from components.upload_model import upload_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a28d37-27fd-40a0-9de7-204a493bc68f",
   "metadata": {},
   "source": [
    "### Import Pipeline Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f41079-202c-4620-9266-579114420308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download required component files to local\n",
    "prefix = f'{UTILS_STACK_NAME}/{UTILS_NAME_PATH}/gcp_utils/'\n",
    "dl_dir = 'gcp_utils/'\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(RESOURCE_BUCKET)\n",
    "blobs = bucket.list_blobs(prefix=prefix)  # Get list of files\n",
    "for blob in blobs: # download each file that starts with \"prefix\" into \"dl_dir\"\n",
    "    if blob.name.endswith(\"/\"):\n",
    "        continue\n",
    "    file_split = blob.name.split(prefix)\n",
    "    file_path = f\"{dl_dir}{file_split[-1]}\"\n",
    "    directory = \"/\".join(file_path.split(\"/\")[0:-1])\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    blob.download_to_filename(file_path) \n",
    "\n",
    "from gcp_utils.download_data_from_gcs import download_data_from_gcs\n",
    "from gcp_utils.export_dataframe_to_bq import export_dataframe_to_bq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb0c622-d24e-49f3-8b6b-6b9a60c147b6",
   "metadata": {},
   "source": [
    "### Import Pipeline Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c696f267-f8ab-43cf-ab7b-1b6f5e31c927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download required component files to local\n",
    "prefix = f'{STACK_NAME}/{TRAIN_PIPELINE_NAME_PATH}/utils/'\n",
    "dl_dir = 'utils/'\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(RESOURCE_BUCKET)\n",
    "blobs = bucket.list_blobs(prefix=prefix)  # Get list of files\n",
    "for blob in blobs: # download each file that starts with \"prefix\" into \"dl_dir\"\n",
    "    if blob.name.endswith(\"/\"):\n",
    "        continue\n",
    "    file_split = blob.name.split(prefix)\n",
    "    file_path = f\"{dl_dir}{file_split[-1]}\"\n",
    "    directory = \"/\".join(file_path.split(\"/\")[0:-1])\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    blob.download_to_filename(file_path) \n",
    "\n",
    "from utils.monitoring import generate_data_stats\n",
    "from utils.monitoring import validate_stats \n",
    "from utils.monitoring import visualize_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81073643-52b2-416e-a840-2ad2c198fa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from google.cloud import storage \n",
    "\n",
    "def download_data_from_gcs(bucket_name, gcs_path, local_path): \n",
    "        \n",
    "    \"\"\"\n",
    "    Download data from gcs bucket to local path. \n",
    "    \n",
    "    Args: \n",
    "        bucket_name: The name of the GCS bucket where your file is located. \n",
    "        gcs_path: The full GCS path (including the filename) to download files from. \n",
    "        local_path: The name of the downloaded file in your local path. \n",
    "\n",
    "    Returns: \n",
    "        None \n",
    "        \n",
    "    Example: \n",
    "        download_data_from_gcs('divg-josh-pr-d1cc3a-default', 'downloads/telus_rwrd_redemption_analysis_2018.csv', 'telus_rwrd_redemption_analysis_2018.csv')\n",
    "    \"\"\"\n",
    "    \n",
    "    bucket = storage.Client().bucket(bucket_name) \n",
    "    blob = bucket.blob(gcs_path) \n",
    "    blob.download_to_filename(local_path) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8824433-7181-40ab-a522-d1fde9bbc5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "def export_dataframe_to_bq(df, client, table_id='', schema_list=[], generate_schema=True, write='overwrite'): \n",
    "    \n",
    "    \"\"\"\n",
    "    Load the input dataframe to a table in bigquery. \n",
    "    \n",
    "    Args: \n",
    "        df (DataFrame): A DataFrame that you want to export to BQ.\n",
    "        client: A BigQuery client instance. e.g. client = bigquery.Client(project=project_id). \n",
    "        table_id (string): A string with dataset and table name. i.e 'ttv_churn_dataset.bq_tv_churn_score'. \n",
    "        schema_list (list of string, optional): List of the SchemaFields if provided, otherwise the function can generate it for you. \n",
    "        generate_schema (bool, optional): True (if True, the function will provide schema for you) or False (provide your own list) \n",
    "        write (string, optional):  if 'overwrite', the function will overwrite the existing table\n",
    "                                   if 'append', it will append to the existing table \n",
    "                                   \n",
    "    Returns: \n",
    "        None \n",
    "        \n",
    "    Example: \n",
    "        export_dataframe_to_bq(final_df, bq_client, 'call_to_retention_dataset.bq_call_to_retention_scores_temp')\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    dtype_bq_mapping = { \n",
    "        np.dtype('int64'): 'INTEGER', \n",
    "        np.dtype('float64'): 'FLOAT', \n",
    "        np.dtype('float32'): 'FLOAT', \n",
    "        np.dtype('object'): 'STRING', \n",
    "        np.dtype('bool'): 'BOOLEAN', \n",
    "        np.dtype('datetime64[ns]'): 'DATE', \n",
    "        pd.Int64Dtype(): 'INTEGER' \n",
    "    } \n",
    "    \n",
    "    if write == 'overwrite': \n",
    "        write_type = 'WRITE_TRUNCATE' \n",
    "    else: \n",
    "        write_type = 'WRITE_APPEND' \n",
    "        \n",
    "    if len(schema_list) == 0: \n",
    "        generate_schema = True \n",
    "    else: \n",
    "        generate_schema = False\n",
    "        \n",
    "    try: \n",
    "        if generate_schema == True: \n",
    "            schema_list = [] \n",
    "            for column in df.columns: \n",
    "                schema_list.append(bigquery.SchemaField(column, dtype_bq_mapping[df.dtypes[column]], mode='NULLABLE')) \n",
    "\n",
    "        # Sending to bigquery \n",
    "        job_config = bigquery.LoadJobConfig(schema=schema_list, write_disposition=write_type) \n",
    "        job = client.load_table_from_dataframe(df, table_id, job_config=job_config) \n",
    "        job.result() \n",
    "        table = client.get_table(table_id) # Make an API request \n",
    "        print(\"Loaded {} rows and {} columns to {}\".format(table.num_rows, len(table.schema), table_id)) \n",
    "\n",
    "    except NameError as e: \n",
    "        print(f\"Error : {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c315d454-416f-4eed-b12d-1acd5aed1207",
   "metadata": {},
   "source": [
    "### Date Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfad310-a482-4285-aa31-6d9113008be6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scoringDate = date.today() - relativedelta(days=1)\n",
    "\n",
    "# training dates\n",
    "SCORE_DATE = scoringDate.strftime('%Y%m%d')  # date.today().strftime('%Y%m%d')\n",
    "SCORE_DATE_DASH = scoringDate.strftime('%Y-%m-%d')\n",
    "SCORE_DATE_MINUS_6_MOS_DASH = ((scoringDate - relativedelta(months=6)).replace(day=1)).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_LAST_MONTH_START_DASH = (scoringDate.replace(day=1) - timedelta(days=1)).replace(day=1).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_LAST_MONTH_END_DASH = ((scoringDate.replace(day=1)) - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "#revert these changes after 2023-05-30\n",
    "PROMO_EXPIRY_START = (scoringDate.replace(day=1) + relativedelta(months=3)).replace(day=1).strftime('%Y-%m-%d')\n",
    "PROMO_EXPIRY_END = (scoringDate.replace(day=1) + relativedelta(months=4)).replace(day=1).strftime('%Y-%m-%d')\n",
    "\n",
    "SCORE_DATE_DELTA = 0\n",
    "SCORE_DATE_VAL_DELTA = 0\n",
    "TICKET_DATE_WINDOW = 30  # Days of ticket data to be queried\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09097ee1-13cd-409b-af0f-5c78f73aaa1c",
   "metadata": {},
   "source": [
    "### Model Monitoring Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f70c28-5070-45fc-bdf7-89112c5d2ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_MONITORING_STACK_NAME = 'util'\n",
    "MODEL_MONITORING_PATH = 'pipeline_utils'\n",
    "TRAINING_PIPELINE_NAME_PATH = 'call_to_retention_model/training_pipeline'\n",
    "SERVING_PIPELINE_NAME_PATH = 'call_to_retention_model/serving_pipeline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32fad49-8603-4190-9a66-aaf2dcdcf845",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = date.today()\n",
    "\n",
    "# BQ table where training data is stored\n",
    "INPUT_TRAINING_DATA_TABLE_PATH = f\"{PROJECT_ID}.{DATASET_ID}.{TRAINING_DATASET_TABLE_NAME}\"\n",
    "INPUT_TRAINING_DATA_CSV_PATH = 'gs://{}/{}_train_monitoring.csv'.format(FILE_BUCKET, SERVICE_TYPE)\n",
    "\n",
    "# BQ dataset where monitoring stats are stored\n",
    "MODEL_MONITORING_DATASET = \"call_to_retention_dataset\"\n",
    "\n",
    "# Paths to statistics artifacts in GCS\n",
    "TRAINING_STATISTICS_PATH = f\"gs://{FILE_BUCKET}/{STACK_NAME}/{TRAINING_PIPELINE_NAME_PATH}/training_statistics/training_statistics_{today}\"\n",
    "TRAINING_STATS_PREFIX = f\"{STACK_NAME}/statistics/training_statistics\"\n",
    "TRAINING_STATISTICS_OUTPUT_PATH = f\"gs://{FILE_BUCKET}/{STACK_NAME}/statistics/training_statistics_{today}\" \n",
    "\n",
    "ANOMALIES_PATH = f\"gs://{FILE_BUCKET}/{STACK_NAME}/anomalies/anomalies_{today}\"\n",
    "PREDICTION_ANOMALIES_PATH = f\"gs://{FILE_BUCKET}/{STACK_NAME}/anomalies/prediction_anomalies_{today}\"\n",
    "PREDICTION_STATS_PATH = f\"gs://{FILE_BUCKET}/{STACK_NAME}/statistics/prediction_statistics_{today}\"\n",
    "PREDICTION_STATS_PREFIX = f\"{FILE_BUCKET}/statistics/prediction_statistics\"\n",
    "\n",
    "# Paths to schemas in GCS\n",
    "SCHEMA_PATH = f'gs://{FILE_BUCKET}/{MODEL_NAME}/schemas/training_stats_schema_{today}'\n",
    "# SATISTICS_PATH = f'gs://{FILE_BUCKET}/{MODEL_NAME}/schemas/training_statistics_{today}'\n",
    "# Thresholds for anomalies\n",
    "ANOMALY_THRESHOLDS_PATH = f\"{STACK_NAME}/{TRAINING_PIPELINE_NAME_PATH}/training_statistics/anomaly_thresholds.json\" #same path structure as utils reading from bucket\n",
    "\n",
    "# Filters for predictions monitoring\n",
    "DATE_COL = 'partition_date'\n",
    "DATE_FILTER = str(today)\n",
    "TABLE_BLOCK_SAMPLE = 1 # no sampling\n",
    "ROW_SAMPLE = 1 # no sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39afbc1f-a625-482c-b1bf-885cf808b47d",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "import google\n",
    "from google.cloud import bigquery\n",
    "from datetime import datetime\n",
    "import logging \n",
    "import os \n",
    "import re \n",
    "from google.oauth2 import credentials\n",
    "import google.oauth2.credentials\n",
    "\n",
    "token = !gcloud auth print-access-token\n",
    "token_str = token[0]\n",
    "\n",
    "CREDENTIALS = google.oauth2.credentials.Credentials(token_str) # get credentials from token\n",
    "    \n",
    "client = bigquery.Client(project=PROJECT_ID, credentials=CREDENTIALS)\n",
    "\n",
    "download_data_from_gcs(BUCKET_NAME, 'pyspark/pyspark_part-00000-3dcf5006-05e0-4b8b-b4d6-46019400f59d-c000.csv', 'campaign_records_hcr.csv')\n",
    "\n",
    "df = pd.read_csv('campaign_records_hcr.csv')\n",
    "df = df[df['camp_inhome'].apply(lambda x: len(str(x)) >= 14)] \n",
    "df['camp_inhome'] = pd.to_datetime(df['camp_inhome']).dt.strftime('%Y-%m-%d')\n",
    "df['camp_inhome'] = pd.to_datetime(df['camp_inhome']).dt.date\n",
    "\n",
    "schema_list = [bigquery.SchemaField('camp_inhome', 'DATE', 'NULLABLE', None, None, (), None), bigquery.SchemaField('camp_id', 'STRING', 'NULLABLE', None, None, (), None), bigquery.SchemaField('bacct_num', 'INTEGER', 'NULLABLE', None, None, (), None), bigquery.SchemaField('release_code_desc', 'STRING', 'NULLABLE', None, None, (), None), bigquery.SchemaField('camp_email', 'STRING', 'NULLABLE', None, None, (), None), bigquery.SchemaField('call_result', 'STRING', 'NULLABLE', None, None, (), None), bigquery.SchemaField('record_exhausted_ind', 'INTEGER', 'NULLABLE', None, None, (), None), bigquery.SchemaField('attempts', 'INTEGER', 'NULLABLE', None, None, (), None), bigquery.SchemaField('snet_premise_type_cd', 'STRING', 'NULLABLE', None, None, (), None)]\n",
    "\n",
    "export_dataframe_to_bq(df, client, 'divg_compaign_element.bq_campaign_records_hcr', schema_list=schema_list, generate_schema=False, write='overwrite') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2eea99-a3f0-48ea-8962-115edcaf26d9",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5e82db-20c8-4480-bb48-b44d2ce005ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports\n",
    "from kfp.v2 import compiler\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "@dsl.pipeline(\n",
    "    name=SERVING_PIPELINE_NAME, \n",
    "    description=SERVING_PIPELINE_DESCRIPTION\n",
    "    )\n",
    "def pipeline(\n",
    "        project_id: str = PROJECT_ID,\n",
    "        region: str = REGION,\n",
    "        resource_bucket: str = RESOURCE_BUCKET, \n",
    "        file_bucket: str = FILE_BUCKET\n",
    "    ):\n",
    "    \n",
    "    from datetime import datetime\n",
    "    update_ts = datetime.now()\n",
    "    update_ts_str = update_ts.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # ----- run campaign data delivery stored procedure --------\n",
    "    bq_run_cdd_sp_op = bq_run_cdd_sp(\n",
    "        score_date_dash=score_date_dash, \n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID, \n",
    "        region=REGION\n",
    "    )\n",
    "\n",
    "    bq_run_cdd_sp_op.set_memory_limit('32G')\n",
    "    bq_run_cdd_sp_op.set_cpu_limit('4')\n",
    "    \n",
    "    bq_run_cdd_sp_op\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2a51c8-4c37-4392-a21a-c4c3279d36d4",
   "metadata": {},
   "source": [
    "### Run the Pipeline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27465a63-5c3b-4e96-9c08-f9cc5dafde90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "import json\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "   pipeline_func=pipeline, package_path=\"pipeline.json\"\n",
    ")\n",
    "\n",
    "job = pipeline_jobs.PipelineJob(\n",
    "                                   display_name=TRAIN_PIPELINE_NAME,\n",
    "                                   template_path=\"pipeline.json\",\n",
    "                                   location=REGION,\n",
    "                                   enable_caching=False,\n",
    "                                   pipeline_root = PIPELINE_ROOT\n",
    "                                )\n",
    "job.run(service_account = f\"bilayer-sa@{PROJECT_ID}.iam.gserviceaccount.com\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
