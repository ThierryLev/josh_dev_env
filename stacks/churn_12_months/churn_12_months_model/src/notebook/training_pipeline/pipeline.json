{
  "pipelineSpec": {
    "components": {
      "comp-bq-create-dataset": {
        "executorLabel": "exec-bq-create-dataset",
        "inputDefinitions": {
          "parameters": {
            "dataset_id": {
              "type": "STRING"
            },
            "environment": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "region": {
              "type": "STRING"
            },
            "score_date": {
              "type": "STRING"
            },
            "score_date_delta": {
              "type": "INT"
            },
            "token": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "parameters": {
            "col_list": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-bq-create-dataset-2": {
        "executorLabel": "exec-bq-create-dataset-2",
        "inputDefinitions": {
          "parameters": {
            "dataset_id": {
              "type": "STRING"
            },
            "environment": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "region": {
              "type": "STRING"
            },
            "score_date": {
              "type": "STRING"
            },
            "score_date_delta": {
              "type": "INT"
            },
            "token": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "parameters": {
            "col_list": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-preprocess": {
        "executorLabel": "exec-preprocess",
        "inputDefinitions": {
          "parameters": {
            "dataset_id": {
              "type": "STRING"
            },
            "pipeline_dataset": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "save_data_path": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-preprocess-2": {
        "executorLabel": "exec-preprocess-2",
        "inputDefinitions": {
          "parameters": {
            "dataset_id": {
              "type": "STRING"
            },
            "pipeline_dataset": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "save_data_path": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-train-and-save-model": {
        "executorLabel": "exec-train-and-save-model",
        "inputDefinitions": {
          "parameters": {
            "dataset_id": {
              "type": "STRING"
            },
            "file_bucket": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "score_date_dash": {
              "type": "STRING"
            },
            "score_date_val_dash": {
              "type": "STRING"
            },
            "service_type": {
              "type": "STRING"
            },
            "token": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "metrics": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            },
            "metricsc": {
              "artifactType": {
                "schemaTitle": "system.ClassificationMetrics",
                "schemaVersion": "0.0.1"
              }
            },
            "model": {
              "artifactType": {
                "schemaTitle": "system.Model",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "col_list": {
              "type": "STRING"
            },
            "model_uri": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-upload-model": {
        "executorLabel": "exec-upload-model",
        "inputDefinitions": {
          "artifacts": {
            "model": {
              "artifactType": {
                "schemaTitle": "system.Model",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "col_list": {
              "type": "STRING"
            },
            "model_name": {
              "type": "STRING"
            },
            "model_uri": {
              "type": "STRING"
            },
            "prediction_image": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "region": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "vertex_model": {
              "artifactType": {
                "schemaTitle": "system.Model",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "model_uri": {
              "type": "STRING"
            }
          }
        }
      }
    },
    "deploymentSpec": {
      "executors": {
        "exec-bq-create-dataset": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "bq_create_dataset"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.18' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef bq_create_dataset(score_date: str\n                      , score_date_delta: int\n                      , project_id: str\n                      , dataset_id: str\n                      , region: str\n                      , environment: str\n                      , token: str\n                      ) -> NamedTuple(\"output\", [(\"col_list\", list)]):\n\n    from google.cloud import bigquery\n    import logging \n    from datetime import datetime\n\n    #### For wb\n    import google.oauth2.credentials\n    CREDENTIALS = google.oauth2.credentials.Credentials(token)\n\n    client = bigquery.Client(project=project_id, credentials=CREDENTIALS)\n    job_config = bigquery.QueryJobConfig()\n\n#     #### For prod \n#     client = bigquery.Client(project=project_id)\n#     job_config = bigquery.QueryJobConfig()\n\n    # Change dataset / table + sp table name to version in bi-layer\n    query =\\\n        f'''\n            DECLARE score_date DATE DEFAULT \"{score_date}\";\n\n            -- Change dataset / sp name to the version in the bi_layer\n            CALL {dataset_id}.bq_sp_c12m_{environment}_dataset(score_date);\n\n            SELECT\n                *\n            FROM {dataset_id}.INFORMATION_SCHEMA.PARTITIONS\n            WHERE table_name='bq_c12m_{environment}_dataset'\n\n        '''\n\n    df = client.query(query, job_config=job_config).to_dataframe()\n    logging.info(df.to_string())\n\n    logging.info(f\"Loaded {df.total_rows[0]} rows into \\\n             {df.table_catalog[0]}.{df.table_schema[0]}.{df.table_name[0]} on \\\n             {datetime.strftime((df.last_modified_time[0]), '%Y-%m-%d %H:%M:%S') } !\")\n\n    ######################################## Save column list_##########################\n    query =\\\n        f'''\n           SELECT\n                *\n            FROM {dataset_id}.bq_c12m_{environment}_dataset\n\n        '''\n\n    df = client.query(query, job_config=job_config).to_dataframe()\n\n    col_list = list([col for col in df.columns])\n    return (col_list,)\n\n"
            ],
            "image": "northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/kfp-pycaret-slim:latest",
            "resources": {
              "cpuLimit": 4.0,
              "memoryLimit": 32.0
            }
          }
        },
        "exec-bq-create-dataset-2": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "bq_create_dataset"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.18' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef bq_create_dataset(score_date: str\n                      , score_date_delta: int\n                      , project_id: str\n                      , dataset_id: str\n                      , region: str\n                      , environment: str\n                      , token: str\n                      ) -> NamedTuple(\"output\", [(\"col_list\", list)]):\n\n    from google.cloud import bigquery\n    import logging \n    from datetime import datetime\n\n    #### For wb\n    import google.oauth2.credentials\n    CREDENTIALS = google.oauth2.credentials.Credentials(token)\n\n    client = bigquery.Client(project=project_id, credentials=CREDENTIALS)\n    job_config = bigquery.QueryJobConfig()\n\n#     #### For prod \n#     client = bigquery.Client(project=project_id)\n#     job_config = bigquery.QueryJobConfig()\n\n    # Change dataset / table + sp table name to version in bi-layer\n    query =\\\n        f'''\n            DECLARE score_date DATE DEFAULT \"{score_date}\";\n\n            -- Change dataset / sp name to the version in the bi_layer\n            CALL {dataset_id}.bq_sp_c12m_{environment}_dataset(score_date);\n\n            SELECT\n                *\n            FROM {dataset_id}.INFORMATION_SCHEMA.PARTITIONS\n            WHERE table_name='bq_c12m_{environment}_dataset'\n\n        '''\n\n    df = client.query(query, job_config=job_config).to_dataframe()\n    logging.info(df.to_string())\n\n    logging.info(f\"Loaded {df.total_rows[0]} rows into \\\n             {df.table_catalog[0]}.{df.table_schema[0]}.{df.table_name[0]} on \\\n             {datetime.strftime((df.last_modified_time[0]), '%Y-%m-%d %H:%M:%S') } !\")\n\n    ######################################## Save column list_##########################\n    query =\\\n        f'''\n           SELECT\n                *\n            FROM {dataset_id}.bq_c12m_{environment}_dataset\n\n        '''\n\n    df = client.query(query, job_config=job_config).to_dataframe()\n\n    col_list = list([col for col in df.columns])\n    return (col_list,)\n\n"
            ],
            "image": "northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/kfp-pycaret-slim:latest",
            "resources": {
              "cpuLimit": 4.0,
              "memoryLimit": 32.0
            }
          }
        },
        "exec-preprocess": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "preprocess"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.18' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef preprocess(\n        pipeline_dataset: str, \n        save_data_path: str,\n        project_id: str,\n        dataset_id: str\n):\n    from google.cloud import bigquery\n    import pandas as pd\n    import gc\n    import time\n\n    client = bigquery.Client(project=project_id)\n\n    # pipeline_dataset \n    pipeline_dataset_name = f\"{project_id}.{dataset_id}.{pipeline_dataset}\" \n    build_df_pipeline_dataset = f'SELECT * FROM `{pipeline_dataset_name}`'\n    df_pipeline_dataset = client.query(build_df_pipeline_dataset).to_dataframe()\n    df_pipeline_dataset = df_pipeline_dataset.set_index('ban') \n\n    # demo columns\n    df_pipeline_dataset['demo_urban_flag'] = df_pipeline_dataset.demo_sgname.str.lower().str.contains('urban').fillna(0).astype(int)\n    df_pipeline_dataset['demo_rural_flag'] = df_pipeline_dataset.demo_sgname.str.lower().str.contains('rural').fillna(0).astype(int)\n    df_pipeline_dataset['demo_family_flag'] = df_pipeline_dataset.demo_lsname.str.lower().str.contains('families').fillna(0).astype(int)\n\n    df_income_dummies = pd.get_dummies(df_pipeline_dataset[['demo_lsname']]) \n    df_income_dummies.columns = df_income_dummies.columns.str.replace('&', 'and')\n    df_income_dummies.columns = df_income_dummies.columns.str.replace(' ', '_')\n\n    df_pipeline_dataset.drop(columns=['demo_sgname', 'demo_lsname'], axis=1, inplace=True)\n\n    df_pipeline_dataset = df_pipeline_dataset.join(df_income_dummies)\n\n    df_join = df_pipeline_dataset.copy()\n\n    #column name clean-up\n    df_join.columns = df_join.columns.str.replace(' ', '_')\n    df_join.columns = df_join.columns.str.replace('-', '_')\n\n    #df_final\n    df_final = df_join.copy()\n    del df_join\n    gc.collect()\n    print('......df_final done')\n\n    for f in df_final.columns:\n        df_final[f] = list(df_final[f])\n\n    df_final.to_csv(save_data_path, index=True, compression='gzip') \n    del df_final\n    gc.collect()\n    print(f'......csv saved in {save_data_path}')\n    time.sleep(120)\n\n"
            ],
            "image": "northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/kfp-pycaret-slim:latest",
            "resources": {
              "cpuLimit": 8.0,
              "memoryLimit": 64.0
            }
          }
        },
        "exec-preprocess-2": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "preprocess"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.18' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef preprocess(\n        pipeline_dataset: str, \n        save_data_path: str,\n        project_id: str,\n        dataset_id: str\n):\n    from google.cloud import bigquery\n    import pandas as pd\n    import gc\n    import time\n\n    client = bigquery.Client(project=project_id)\n\n    # pipeline_dataset \n    pipeline_dataset_name = f\"{project_id}.{dataset_id}.{pipeline_dataset}\" \n    build_df_pipeline_dataset = f'SELECT * FROM `{pipeline_dataset_name}`'\n    df_pipeline_dataset = client.query(build_df_pipeline_dataset).to_dataframe()\n    df_pipeline_dataset = df_pipeline_dataset.set_index('ban') \n\n    # demo columns\n    df_pipeline_dataset['demo_urban_flag'] = df_pipeline_dataset.demo_sgname.str.lower().str.contains('urban').fillna(0).astype(int)\n    df_pipeline_dataset['demo_rural_flag'] = df_pipeline_dataset.demo_sgname.str.lower().str.contains('rural').fillna(0).astype(int)\n    df_pipeline_dataset['demo_family_flag'] = df_pipeline_dataset.demo_lsname.str.lower().str.contains('families').fillna(0).astype(int)\n\n    df_income_dummies = pd.get_dummies(df_pipeline_dataset[['demo_lsname']]) \n    df_income_dummies.columns = df_income_dummies.columns.str.replace('&', 'and')\n    df_income_dummies.columns = df_income_dummies.columns.str.replace(' ', '_')\n\n    df_pipeline_dataset.drop(columns=['demo_sgname', 'demo_lsname'], axis=1, inplace=True)\n\n    df_pipeline_dataset = df_pipeline_dataset.join(df_income_dummies)\n\n    df_join = df_pipeline_dataset.copy()\n\n    #column name clean-up\n    df_join.columns = df_join.columns.str.replace(' ', '_')\n    df_join.columns = df_join.columns.str.replace('-', '_')\n\n    #df_final\n    df_final = df_join.copy()\n    del df_join\n    gc.collect()\n    print('......df_final done')\n\n    for f in df_final.columns:\n        df_final[f] = list(df_final[f])\n\n    df_final.to_csv(save_data_path, index=True, compression='gzip') \n    del df_final\n    gc.collect()\n    print(f'......csv saved in {save_data_path}')\n    time.sleep(120)\n\n"
            ],
            "image": "northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/kfp-pycaret-slim:latest",
            "resources": {
              "cpuLimit": 8.0,
              "memoryLimit": 64.0
            }
          }
        },
        "exec-train-and-save-model": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "train_and_save_model"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.18' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef train_and_save_model(file_bucket: str\n                        , service_type: str\n                        , score_date_dash: str\n                        , score_date_val_dash: str\n                        , project_id: str\n                        , dataset_id: str\n                        , metrics: Output[Metrics]\n                        , metricsc: Output[ClassificationMetrics]\n                        , model: Output[Model]\n                        , token: str\n            )-> NamedTuple(\"output\", [(\"col_list\", list), (\"model_uri\", str)]):\n\n    import gc\n    import time\n    import pandas as pd\n    import numpy as np\n    import pickle\n    import xgboost as xgb\n    from datetime import datetime\n    from sklearn.metrics import roc_auc_score\n    from sklearn.preprocessing import normalize\n    from sklearn.model_selection import train_test_split\n    from google.cloud import storage\n    from google.cloud import bigquery\n\n    def get_lift(prob, y_test, q):\n        result = pd.DataFrame(columns=['Prob', 'Churn'])\n        result['Prob'] = prob\n        result['Churn'] = y_test\n        # result['Decile'] = pd.qcut(1-result['Prob'], 10, labels = False)\n        result['Decile'] = pd.qcut(result['Prob'], q, labels=[i for i in range(q, 0, -1)])\n        add = pd.DataFrame(result.groupby('Decile')['Churn'].mean()).reset_index()\n        add.columns = ['Decile', 'avg_real_churn_rate']\n        result = result.merge(add, on='Decile', how='left')\n        result.sort_values('Decile', ascending=True, inplace=True)\n        lg = pd.DataFrame(result.groupby('Decile')['Prob'].mean()).reset_index()\n        lg.columns = ['Decile', 'avg_model_pred_churn_rate']\n        lg.sort_values('Decile', ascending=False, inplace=True)\n        lg['avg_churn_rate_total'] = result['Churn'].mean()\n        lg['total_churn'] = result['Churn'].sum()\n        lg = lg.merge(add, on='Decile', how='left')\n        lg['lift'] = lg['avg_real_churn_rate'] / lg['avg_churn_rate_total']\n\n        return lg\n\n    df_train = pd.read_csv('gs://{}/{}/{}_train.csv.gz'.format(file_bucket, service_type, service_type),\n                           compression='gzip')  \n    df_test = pd.read_csv('gs://{}/{}/{}_validation.csv.gz'.format(file_bucket, service_type, service_type),  \n                          compression='gzip')\n\n    #### For wb\n    import google.oauth2.credentials\n    CREDENTIALS = google.oauth2.credentials.Credentials(token)\n\n    client = bigquery.Client(project=project_id, credentials=CREDENTIALS)\n    job_config = bigquery.QueryJobConfig()\n\n#     #### For prod \n#     client = bigquery.Client(project=project_id)\n#     job_config = bigquery.QueryJobConfig()\n\n    #set up df_train\n    sql_train = ''' SELECT * FROM `{}.{}.bq_churn_12_months_targets` '''.format(project_id, dataset_id) \n    df_target_train = client.query(sql_train).to_dataframe()\n    df_target_train = df_target_train.loc[\n        df_target_train['YEAR_MONTH'] == '-'.join(score_date_dash.split('-')[:2])]  # score_date_dash = '2022-08-31'\n    df_target_train['ban'] = df_target_train['ban'].astype('int64')\n    df_target_train = df_target_train.groupby('ban').tail(1)\n    df_train = df_train.merge(df_target_train[['ban', 'target_ind']], on='ban', how='left')\n    df_train.rename(columns={'target_ind': 'target'}, inplace=True)\n    df_train.dropna(subset=['target'], inplace=True)\n    df_train['target'] = df_train['target'].astype(int)\n    print(df_train.shape)\n\n    #set up df_test\n    sql_test = ''' SELECT * FROM `{}.{}.bq_churn_12_months_targets` '''.format(project_id, dataset_id) \n    df_target_test = client.query(sql_test).to_dataframe()\n    df_target_test = df_target_test.loc[\n        df_target_test['YEAR_MONTH'] == '-'.join(score_date_val_dash.split('-')[:2])]  # score_date_dash = '2022-09-30'\n    df_target_test['ban'] = df_target_test['ban'].astype('int64')\n    df_target_test = df_target_test.groupby('ban').tail(1)\n    df_test = df_test.merge(df_target_test[['ban', 'target_ind']], on='ban', how='left')\n    df_test.rename(columns={'target_ind': 'target'}, inplace=True)\n    df_test.dropna(subset=['target'], inplace=True)\n    df_test['target'] = df_test['target'].astype(int)\n    print(df_test.shape)\n\n    df_train.to_csv('gs://{}/{}/{}_train_monitoring.csv'.format(file_bucket, service_type, service_type))\n    df_test.to_csv('gs://{}/{}/{}_validation_monitoring.csv'.format(file_bucket, service_type, service_type))\n\n    #set up features (list)\n    cols_1 = df_train.columns.values\n    cols_2 = df_test.columns.values\n    cols = set(cols_1).intersection(set(cols_2))\n    features = [f for f in cols if f not in ['ban', 'target']]\n\n    #train test split\n    df_train, df_val = train_test_split(df_train, shuffle=True, test_size=0.25, random_state=42,\n                                        stratify=df_train['target']\n                                        )\n\n    create_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    df_train.to_csv('gs://{}/{}/backup/{}_train_{}.csv'.format(file_bucket, service_type, service_type, create_time))\n    df_val.to_csv('gs://{}/{}/backup/{}_train_{}.csv'.format(file_bucket, service_type, service_type, create_time))\n    df_test.to_csv('gs://{}/{}/backup/{}_train_{}.csv'.format(file_bucket, service_type, service_type, create_time))\n\n    ban_train = df_train['ban']\n    X_train = df_train[features]\n    y_train = np.squeeze(df_train['target'].values)\n\n    ban_val = df_val['ban']\n    X_val = df_val[features]\n    y_val = np.squeeze(df_val['target'].values)\n\n    ban_test = df_test['ban']\n    X_test = df_test[features]\n    y_test = np.squeeze(df_test['target'].values)\n\n    del df_train, df_val, df_test\n    gc.collect()\n\n    # build model and fit in training data\n    xgb_model = xgb.XGBClassifier(\n        learning_rate=0.01,\n        n_estimators=100,\n        max_depth=8,\n        min_child_weight=1,\n        gamma=0,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        objective='binary:logistic',\n        nthread=4,\n        scale_pos_weight=1\n        # seed=27\n    )\n\n    xgb_model.fit(X_train, y_train)\n    print('xgb training done')\n\n    #predictions on X_val\n    y_pred = xgb_model.predict_proba(X_val, ntree_limit=xgb_model.best_iteration)[:, 1]\n    y_pred_label = (y_pred > 0.5).astype(int)\n    auc = roc_auc_score(y_val, y_pred_label)\n    metrics.log_metric(\"AUC\", auc)\n\n    pred_prb = xgb_model.predict_proba(X_test, ntree_limit=xgb_model.best_iteration)[:, 1]\n    lg = get_lift(pred_prb, y_test, 10)\n\n    # save the model in GCS\n    models_dict = {}\n    create_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    models_dict['create_time'] = create_time\n    models_dict['model'] = xgb_model\n    models_dict['features'] = features\n    lg.to_csv('gs://{}/{}/lift_on_scoring_data_{}.csv'.format(file_bucket, service_type, create_time, index=False))\n\n    with open('model_dict.pkl', 'wb') as handle:\n        pickle.dump(models_dict, handle)\n    handle.close()\n\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(file_bucket)\n\n    MODEL_PATH = '{}/{}_xgb_models/'.format(service_type, service_type)\n    blob = bucket.blob(MODEL_PATH)\n    if not blob.exists(storage_client):\n        blob.upload_from_string('')\n\n    model_name_onbkt = '{}{}_models_xgb_{}'.format(MODEL_PATH, service_type, models_dict['create_time'])\n    blob = bucket.blob(model_name_onbkt)\n    blob.upload_from_filename('model_dict.pkl')\n\n    model.uri = f'gs://{file_bucket}/{model_name_onbkt}'\n\n    print(f\"....model loaded to GCS done at {str(create_time)}\")\n\n    col_list = features\n\n    return (col_list, model.uri)\n\n"
            ],
            "image": "northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/kfp-pycaret-slim:latest",
            "resources": {
              "cpuLimit": 4.0,
              "memoryLimit": 32.0
            }
          }
        },
        "exec-upload-model": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "upload_model"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.18' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef upload_model(\n    project_id: str,\n    model: Input[Model],\n    vertex_model: Output[Model],\n    region: str,\n    model_name: str,\n    prediction_image: str,\n    col_list: list, \n    model_uri: str\n    )-> NamedTuple(\"output\", [(\"model_uri\", str)]):\n    \"\"\"\n    Upload model to Vertex Model Registry.\n    Args:\n        project_id (str): project id for where this pipeline is being run\n        model (Input[Model]): model passed in from training component. Must have path specified in model.uri\n        region (str): region for where the query will be run\n        model_name (str): name of model to be stored\n        prediction_image (str): prediction image uri\n        col_list (str): string of list of columns in serving data\n    Returns:\n        vertex_model (Output[Model]): Model saved in Vertex AI\n    \"\"\"\n\n    from google.cloud import aiplatform\n    import os\n\n    aiplatform.init(project=project_id, location=region)\n\n    ## check if prediction image is custom or not\n    if prediction_image.startswith('northamerica-northeast1-docker'):\n        # custom: must set ports\n        health_route = \"/ping\"\n        predict_route = \"/predict\"\n        serving_container_ports = [7080]\n    else:\n        # Google pre-built\n        health_route = None\n        predict_route = None\n        serving_container_ports = None\n\n    ## check for existing models\n    # if model exists, update the version\n    try:\n        model_uid = aiplatform.Model.list(\n            filter=f'display_name={model_name}', \n            order_by=\"update_time\",\n            location=region)[-1].resource_name\n\n        uploaded_model = aiplatform.Model.upload(\n            display_name = model_name, \n            artifact_uri = os.path.dirname(model.uri),\n            serving_container_image_uri = prediction_image,\n            serving_container_predict_route=predict_route,\n            serving_container_health_route=health_route,\n            serving_container_ports=serving_container_ports,\n            serving_container_environment_variables =  {\"COL_LIST\":str(col_list), \"model_uri\": model_uri},\n            parent_model = model_uid,\n            is_default_version = True\n        )\n    # if model does not already exist, create a new model\n    except:\n        uploaded_model = aiplatform.Model.upload(\n            display_name = model_name,\n            artifact_uri = os.path.dirname(model.uri),\n            serving_container_image_uri=prediction_image,\n            serving_container_predict_route=predict_route,\n            serving_container_health_route=health_route,\n            serving_container_ports=serving_container_ports,\n            serving_container_environment_variables =  {\"COL_LIST\":str(col_list), \"model_uri\": model_uri},\n        )\n\n    vertex_model.uri = uploaded_model.resource_name\n\n    return (vertex_model.uri,)\n\n"
            ],
            "image": "northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/kfp-load-model-slim:1.0.0",
            "resources": {
              "cpuLimit": 4.0,
              "memoryLimit": 32.0
            }
          }
        }
      }
    },
    "pipelineInfo": {
      "name": "churn-12-months-train-pipeline"
    },
    "root": {
      "dag": {
        "outputs": {
          "artifacts": {
            "train-and-save-model-metrics": {
              "artifactSelectors": [
                {
                  "outputArtifactKey": "metrics",
                  "producerSubtask": "train-and-save-model"
                }
              ]
            },
            "train-and-save-model-metricsc": {
              "artifactSelectors": [
                {
                  "outputArtifactKey": "metricsc",
                  "producerSubtask": "train-and-save-model"
                }
              ]
            }
          }
        },
        "tasks": {
          "bq-create-dataset": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-bq-create-dataset"
            },
            "inputs": {
              "parameters": {
                "dataset_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "churn_12_months"
                    }
                  }
                },
                "environment": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "training"
                    }
                  }
                },
                "project_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-josh-pr-d1cc3a"
                    }
                  }
                },
                "region": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "northamerica-northeast1"
                    }
                  }
                },
                "score_date": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "2021-09-01"
                    }
                  }
                },
                "score_date_delta": {
                  "runtimeValue": {
                    "constantValue": {
                      "intValue": "0"
                    }
                  }
                },
                "token": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "ya29.a0AfB_byC_sxhfpekkmYO9-EcVrX2GW-dkeMF08Qz618BqYgXl5UGJilhAlWdyE9rszExfgm8JPF6-Kf6vuTarxOFM1EFLpQjoU0cuq9Wq4mfCgs4TR1_PqT51KYbyOieHsFctgcL1FWv5wqUziRoweotA-o9EMPdG07umdyqrujBCaCgYKAQ4SARISFQGOcNnCrk8MyxmUvm-FciDpNCAoWg0179"
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "bq-create-dataset"
            }
          },
          "bq-create-dataset-2": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-bq-create-dataset-2"
            },
            "dependentTasks": [
              "preprocess"
            ],
            "inputs": {
              "parameters": {
                "dataset_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "churn_12_months"
                    }
                  }
                },
                "environment": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "validation"
                    }
                  }
                },
                "project_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-josh-pr-d1cc3a"
                    }
                  }
                },
                "region": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "northamerica-northeast1"
                    }
                  }
                },
                "score_date": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "2022-09-01"
                    }
                  }
                },
                "score_date_delta": {
                  "runtimeValue": {
                    "constantValue": {
                      "intValue": "0"
                    }
                  }
                },
                "token": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "ya29.a0AfB_byC_sxhfpekkmYO9-EcVrX2GW-dkeMF08Qz618BqYgXl5UGJilhAlWdyE9rszExfgm8JPF6-Kf6vuTarxOFM1EFLpQjoU0cuq9Wq4mfCgs4TR1_PqT51KYbyOieHsFctgcL1FWv5wqUziRoweotA-o9EMPdG07umdyqrujBCaCgYKAQ4SARISFQGOcNnCrk8MyxmUvm-FciDpNCAoWg0179"
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "bq-create-dataset-2"
            }
          },
          "preprocess": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-preprocess"
            },
            "dependentTasks": [
              "bq-create-dataset"
            ],
            "inputs": {
              "parameters": {
                "dataset_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "churn_12_months"
                    }
                  }
                },
                "pipeline_dataset": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "bq_c12m_training_dataset"
                    }
                  }
                },
                "project_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-josh-pr-d1cc3a"
                    }
                  }
                },
                "save_data_path": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "gs://divg-josh-pr-d1cc3a-default/churn_12_months/churn_12_months_train.csv.gz"
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "preprocess"
            }
          },
          "preprocess-2": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-preprocess-2"
            },
            "dependentTasks": [
              "bq-create-dataset-2"
            ],
            "inputs": {
              "parameters": {
                "dataset_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "churn_12_months"
                    }
                  }
                },
                "pipeline_dataset": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "bq_c12m_validation_dataset"
                    }
                  }
                },
                "project_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-josh-pr-d1cc3a"
                    }
                  }
                },
                "save_data_path": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "gs://divg-josh-pr-d1cc3a-default/churn_12_months/churn_12_months_validation.csv.gz"
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "preprocess-2"
            }
          },
          "train-and-save-model": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-train-and-save-model"
            },
            "dependentTasks": [
              "preprocess-2"
            ],
            "inputs": {
              "parameters": {
                "dataset_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "churn_12_months"
                    }
                  }
                },
                "file_bucket": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-josh-pr-d1cc3a-default"
                    }
                  }
                },
                "project_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-josh-pr-d1cc3a"
                    }
                  }
                },
                "score_date_dash": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "2021-09-01"
                    }
                  }
                },
                "score_date_val_dash": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "2022-09-01"
                    }
                  }
                },
                "service_type": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "churn_12_months"
                    }
                  }
                },
                "token": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "ya29.a0AfB_byC_sxhfpekkmYO9-EcVrX2GW-dkeMF08Qz618BqYgXl5UGJilhAlWdyE9rszExfgm8JPF6-Kf6vuTarxOFM1EFLpQjoU0cuq9Wq4mfCgs4TR1_PqT51KYbyOieHsFctgcL1FWv5wqUziRoweotA-o9EMPdG07umdyqrujBCaCgYKAQ4SARISFQGOcNnCrk8MyxmUvm-FciDpNCAoWg0179"
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "train-and-save-model"
            }
          },
          "upload-model": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-upload-model"
            },
            "dependentTasks": [
              "train-and-save-model"
            ],
            "inputs": {
              "artifacts": {
                "model": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "model",
                    "producerTask": "train-and-save-model"
                  }
                }
              },
              "parameters": {
                "col_list": {
                  "taskOutputParameter": {
                    "outputParameterKey": "col_list",
                    "producerTask": "train-and-save-model"
                  }
                },
                "model_name": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "churn_12_months"
                    }
                  }
                },
                "model_uri": {
                  "taskOutputParameter": {
                    "outputParameterKey": "model_uri",
                    "producerTask": "train-and-save-model"
                  }
                },
                "prediction_image": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/kfp-pycaret-slim:latest"
                    }
                  }
                },
                "project_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "divg-josh-pr-d1cc3a"
                    }
                  }
                },
                "region": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "northamerica-northeast1"
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "upload-model"
            }
          }
        }
      },
      "inputDefinitions": {
        "parameters": {
          "file_bucket": {
            "type": "STRING"
          },
          "project_id": {
            "type": "STRING"
          },
          "region": {
            "type": "STRING"
          },
          "resource_bucket": {
            "type": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "train-and-save-model-metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          },
          "train-and-save-model-metricsc": {
            "artifactType": {
              "schemaTitle": "system.ClassificationMetrics",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    },
    "schemaVersion": "2.0.0",
    "sdkVersion": "kfp-1.8.18"
  },
  "runtimeConfig": {
    "parameters": {
      "file_bucket": {
        "stringValue": "divg-josh-pr-d1cc3a-default"
      },
      "project_id": {
        "stringValue": "divg-josh-pr-d1cc3a"
      },
      "region": {
        "stringValue": "northamerica-northeast1"
      },
      "resource_bucket": {
        "stringValue": "divg-josh-pr-d1cc3a-default"
      }
    }
  }
}