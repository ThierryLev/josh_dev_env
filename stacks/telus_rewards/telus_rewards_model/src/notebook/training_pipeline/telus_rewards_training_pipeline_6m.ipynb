{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c150da3-6e1e-4f02-a778-bb3b12af9054",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b559b7e6-fe51-49cc-8e9c-832380843832",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output, OutputPath, ClassificationMetrics,\n",
    "                        Metrics, component)\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import google\n",
    "from google.oauth2 import credentials\n",
    "from google.oauth2 import service_account\n",
    "from google.oauth2.service_account import Credentials\n",
    "from google.cloud import storage\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google_cloud_pipeline_components.v1.batch_predict_job import \\\n",
    "    ModelBatchPredictOp as batch_prediction_op\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558ff533-70c8-4421-813e-c567d6bc496b",
   "metadata": {},
   "source": [
    "### YAML Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f70d721-76e7-4c2e-8153-88b5bbe8ee47",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#tag cell with parameters\n",
    "PROJECT_ID =  ''\n",
    "BUCKET_NAME=''\n",
    "DATASET_ID = ''\n",
    "RESOURCE_BUCKET = ''\n",
    "FILE_BUCKET = ''\n",
    "REGION = ''\n",
    "MODEL_ID = '5065'\n",
    "MODEL_NAME = 'telus_rewards'\n",
    "PREDICTION_IMAGE = 'northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/pred-pycaret:1.1.0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb9c0d1-697a-4e34-92d5-125c106d6c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag cell with parameters\n",
    "PROJECT_ID =  'divg-josh-pr-d1cc3a'\n",
    "BUCKET_NAME='divg-josh-pr-d1cc3a-default'\n",
    "DATASET_ID = 'telus_rewards'\n",
    "RESOURCE_BUCKET = 'divg-josh-pr-d1cc3a-default'\n",
    "FILE_BUCKET = 'divg-josh-pr-d1cc3a-default'\n",
    "MODEL_ID = '5065'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eeed21-e12b-45ee-b1e4-d0c8f3843533",
   "metadata": {},
   "source": [
    "### Service Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65d1668-e1d8-4f58-958f-edc44d06a6b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SERVICE_TYPE = 'telus_rewards'\n",
    "SERVICE_TYPE_NAME = 'telus-rewards'\n",
    "TABLE_ID = 'telus_rwrd_redemption_targets'\n",
    "REGION = 'northamerica-northeast1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2229e5d1-1362-40ed-aa7d-cb22e41e4960",
   "metadata": {},
   "source": [
    "### Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d628b-e759-4c9c-aab5-568c54721aaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "STACK_NAME = 'telus_rewards'\n",
    "TRAIN_PIPELINE_NAME_PATH = 'telus_rewards/training_pipeline'\n",
    "PREDICT_PIPELINE_NAME_PATH = 'telus_rewards/predicting_pipeline'\n",
    "TRAIN_PIPELINE_NAME = 'telus-rewards-train-pipeline' # Same name as pulumi.yaml\n",
    "PREDICT_PIPELINE_NAME = 'telus-rewards-predict-pipeline' # Same name as pulumi.yaml\n",
    "TRAIN_PIPELINE_DESCRIPTION = 'telus-rewards-train-pipeline'\n",
    "PREDICT_PIPELINE_DESCRIPTION = 'telus-rewards-predict-pipeline'\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET_NAME}\"\n",
    "REGION = \"northamerica-northeast1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b1da86-930b-4529-b7ab-7c14716ea3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATASET_TABLE_NAME = 'bq_telus_rewards_pipeline_dataset'\n",
    "TRAINING_DATASET_SP_NAME = 'bq_sp_telus_rewards_pipeline_dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc0bd98-1d58-491e-bc5e-bbac378c7bbf",
   "metadata": {},
   "source": [
    "### Import Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f442904b-a40a-4ba8-b237-e9894ffd4f26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download required component files to local\n",
    "prefix = f'{STACK_NAME}/{TRAIN_PIPELINE_NAME_PATH}/components/'\n",
    "dl_dir = 'components/'\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(RESOURCE_BUCKET)\n",
    "blobs = bucket.list_blobs(prefix=prefix)  # Get list of files\n",
    "for blob in blobs: # download each file that starts with \"prefix\" into \"dl_dir\"\n",
    "    if blob.name.endswith(\"/\"):\n",
    "        continue\n",
    "    file_split = blob.name.split(prefix)\n",
    "    file_path = f\"{dl_dir}{file_split[-1]}\"\n",
    "    directory = \"/\".join(file_path.split(\"/\")[0:-1])\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    blob.download_to_filename(file_path) \n",
    "\n",
    "# import main pipeline components\n",
    "from components.bq_create_dataset import bq_create_dataset\n",
    "from components.preprocess import preprocess\n",
    "from components.train_and_save_model_12m import train_and_save_model\n",
    "from components.upload_model import upload_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dc65b3-f2f4-483e-a4b9-d207be85037d",
   "metadata": {},
   "source": [
    "### Import Pipeline Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96c7660-4b2e-4711-884a-ce4cd43d59cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download required component files to local\n",
    "prefix = f'{STACK_NAME}/{TRAIN_PIPELINE_NAME_PATH}/utils/'\n",
    "dl_dir = 'utils/'\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(RESOURCE_BUCKET)\n",
    "blobs = bucket.list_blobs(prefix=prefix)  # Get list of files\n",
    "for blob in blobs: # download each file that starts with \"prefix\" into \"dl_dir\"\n",
    "    if blob.name.endswith(\"/\"):\n",
    "        continue\n",
    "    file_split = blob.name.split(prefix)\n",
    "    file_path = f\"{dl_dir}{file_split[-1]}\"\n",
    "    directory = \"/\".join(file_path.split(\"/\")[0:-1])\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    blob.download_to_filename(file_path) \n",
    "\n",
    "from utils.monitoring import generate_data_stats\n",
    "from utils.monitoring import validate_stats \n",
    "from utils.monitoring import visualize_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c315d454-416f-4eed-b12d-1acd5aed1207",
   "metadata": {},
   "source": [
    "### Date Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfad310-a482-4285-aa31-6d9113008be6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scoringDate = date(2022, 8, 1)  # date.today() - relativedelta(days=2)- relativedelta(months=30)\n",
    "valScoringDate = date(2023, 8, 1)  # scoringDate - relativedelta(days=2)\n",
    "\n",
    "# training dates\n",
    "SCORE_DATE = scoringDate.strftime('%Y%m%d')  # date.today().strftime('%Y%m%d')\n",
    "SCORE_DATE_DASH = scoringDate.strftime('%Y-%m-%d')\n",
    "SCORE_DATE_MINUS_6_MOS_DASH = ((scoringDate - relativedelta(months=6)).replace(day=1)).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_LAST_MONTH_START_DASH = (scoringDate.replace(day=1) - timedelta(days=1)).replace(day=1).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_LAST_MONTH_END_DASH = ((scoringDate.replace(day=1)) - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "PROMO_EXPIRY_START = (scoringDate.replace(day=1) + relativedelta(months=3)).replace(day=1).strftime('%Y-%m-%d')\n",
    "PROMO_EXPIRY_END = (scoringDate.replace(day=1) + relativedelta(months=4)).replace(day=1).strftime('%Y-%m-%d')\n",
    "\n",
    "# validation dates\n",
    "SCORE_DATE_VAL = valScoringDate.strftime('%Y%m%d')\n",
    "SCORE_DATE_VAL_DASH = valScoringDate.strftime('%Y-%m-%d')\n",
    "SCORE_DATE_VAL_MINUS_6_MOS_DASH = ((valScoringDate - relativedelta(months=6)).replace(day=1)).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_VAL_LAST_MONTH_START_DASH = (valScoringDate.replace(day=1) - timedelta(days=1)).replace(day=1).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_VAL_LAST_MONTH_END_DASH = ((valScoringDate.replace(day=1)) - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "PROMO_EXPIRY_START_VAL = (valScoringDate.replace(day=1) + relativedelta(months=3)).replace(day=1).strftime('%Y-%m-%d')\n",
    "PROMO_EXPIRY_END_VAL = (valScoringDate.replace(day=1) + relativedelta(months=4)).replace(day=1).strftime('%Y-%m-%d')\n",
    "\n",
    "SCORE_DATE_DELTA = 0\n",
    "SCORE_DATE_VAL_DELTA = 0\n",
    "TICKET_DATE_WINDOW = 30  # Days of ticket data to be queried\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5e6dc5-8c12-40c7-876f-850e524a17a7",
   "metadata": {},
   "source": [
    "### Model Monitoring Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81551568-8f99-48eb-8a1d-f4ada57fccc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_MONITORING_STACK_NAME = 'util'\n",
    "MODEL_MONITORING_PATH = 'pipeline_utils'\n",
    "TRAINING_PIPELINE_NAME_PATH = 'telus_rewards_model/training_pipeline'\n",
    "SERVING_PIPELINE_NAME_PATH = 'telus_rewards_model/serving_pipeline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a041368e-6b3f-4bbc-893f-f788c1dd8660",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = date.today()\n",
    "\n",
    "# BQ table where training data is stored\n",
    "INPUT_TRAINING_DATA_TABLE_PATH = f\"{PROJECT_ID}.{DATASET_ID}.{TRAINING_DATASET_TABLE_NAME}\"\n",
    "INPUT_TRAINING_DATA_CSV_PATH = 'gs://{}/{}_train_monitoring.csv'.format(FILE_BUCKET, SERVICE_TYPE)\n",
    "INPUT_VALIDATION_DATA_CSV_PATH = 'gs://{}/{}_validation_monitoring.csv'.format(FILE_BUCKET, SERVICE_TYPE)\n",
    "\n",
    "# BQ dataset where monitoring stats are stored\n",
    "MODEL_MONITORING_DATASET = \"telus_rewards\"\n",
    "\n",
    "# Paths to statistics artifacts in GCS\n",
    "TRAINING_STATISTICS_PATH = f\"gs://{FILE_BUCKET}/{STACK_NAME}/{TRAINING_PIPELINE_NAME_PATH}/training_statistics/training_statistics_{today}\"\n",
    "TRAINING_STATS_PREFIX = f\"{STACK_NAME}/statistics/training_statistics\"\n",
    "TRAINING_STATISTICS_OUTPUT_PATH = f\"gs://{FILE_BUCKET}/{STACK_NAME}/statistics/training_statistics_{today}\" \n",
    "\n",
    "ANOMALIES_PATH = f\"gs://{FILE_BUCKET}/{STACK_NAME}/anomalies/anomalies_{today}\"\n",
    "PREDICTION_ANOMALIES_PATH = f\"gs://{FILE_BUCKET}/{STACK_NAME}/anomalies/prediction_anomalies_{today}\"\n",
    "PREDICTION_STATS_PATH = f\"gs://{FILE_BUCKET}/{STACK_NAME}/statistics/prediction_statistics_{today}\"\n",
    "PREDICTION_STATS_PREFIX = f\"{FILE_BUCKET}/statistics/prediction_statistics\"\n",
    "\n",
    "# Paths to schemas in GCS\n",
    "SCHEMA_PATH = f'gs://{FILE_BUCKET}/{MODEL_NAME}/schemas/training_stats_schema_{today}'\n",
    "# SATISTICS_PATH = f'gs://{FILE_BUCKET}/{MODEL_NAME}/schemas/training_statistics_{today}'\n",
    "# Thresholds for anomalies\n",
    "ANOMALY_THRESHOLDS_PATH = f\"{STACK_NAME}/{TRAINING_PIPELINE_NAME_PATH}/training_statistics/anomaly_thresholds.json\" #same path structure as utils reading from bucket\n",
    "\n",
    "# Filters for predictions monitoring\n",
    "DATE_COL = 'partition_date'\n",
    "DATE_FILTER = str(today)\n",
    "TABLE_BLOCK_SAMPLE = 1 # no sampling\n",
    "ROW_SAMPLE = 1 # no sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2eea99-a3f0-48ea-8962-115edcaf26d9",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5e82db-20c8-4480-bb48-b44d2ce005ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports\n",
    "from kfp.v2 import compiler\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "@dsl.pipeline(\n",
    "    name=TRAIN_PIPELINE_NAME, \n",
    "    description=TRAIN_PIPELINE_DESCRIPTION\n",
    "    )\n",
    "def pipeline(\n",
    "        project_id: str = PROJECT_ID,\n",
    "        region: str = REGION,\n",
    "        resource_bucket: str = RESOURCE_BUCKET, \n",
    "        file_bucket: str = FILE_BUCKET\n",
    "    ):\n",
    "    \n",
    "    import google.oauth2.credentials\n",
    "    token = !gcloud auth print-access-token\n",
    "    token_str = token[0]\n",
    "    \n",
    "    from datetime import datetime\n",
    "    update_ts = datetime.now()\n",
    "    update_ts_str = update_ts.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "#     # ----- create training set --------\n",
    "#     bq_create_training_dataset_op = bq_create_dataset(score_date=SCORE_DATE_DASH,\n",
    "#                           score_date_delta=SCORE_DATE_DELTA,\n",
    "#                           project_id=PROJECT_ID,\n",
    "#                           dataset_id=DATASET_ID,\n",
    "#                           region=REGION,\n",
    "#                           promo_expiry_start=PROMO_EXPIRY_START, \n",
    "#                           promo_expiry_end=PROMO_EXPIRY_END, \n",
    "#                           v_start_date=SCORE_DATE_MINUS_6_MOS_DASH,\n",
    "#                           v_end_date=SCORE_DATE_LAST_MONTH_END_DASH, \n",
    "#                           token = token_str)\n",
    "    \n",
    "#     bq_create_training_dataset_op.set_memory_limit('128G')\n",
    "#     bq_create_training_dataset_op.set_cpu_limit('16')\n",
    "    \n",
    "#     # ----- preprocessing train data --------\n",
    "#     preprocess_train_op = preprocess(\n",
    "#         pipeline_dataset=TRAINING_DATASET_TABLE_NAME, \n",
    "#         save_data_path='gs://{}/{}_train.csv.gz'.format(FILE_BUCKET, SERVICE_TYPE),\n",
    "#         project_id=PROJECT_ID,\n",
    "#         dataset_id=DATASET_ID, \n",
    "#         token = token_str\n",
    "#     )\n",
    "\n",
    "#     preprocess_train_op.set_memory_limit('128G')\n",
    "#     preprocess_train_op.set_cpu_limit('16')\n",
    "    \n",
    "    # ----- create validation set --------\n",
    "    bq_create_validation_dataset_op = bq_create_dataset(score_date=SCORE_DATE_VAL_DASH,\n",
    "                          score_date_delta=SCORE_DATE_VAL_DELTA,\n",
    "                          project_id=PROJECT_ID,\n",
    "                          dataset_id=DATASET_ID,\n",
    "                          region=REGION,\n",
    "                          promo_expiry_start=PROMO_EXPIRY_START_VAL, \n",
    "                          promo_expiry_end=PROMO_EXPIRY_END_VAL, \n",
    "                          v_start_date=SCORE_DATE_VAL_MINUS_6_MOS_DASH,\n",
    "                          v_end_date=SCORE_DATE_VAL_LAST_MONTH_END_DASH, \n",
    "                          token = token_str)\n",
    "    \n",
    "    bq_create_validation_dataset_op.set_memory_limit('128G')\n",
    "    bq_create_validation_dataset_op.set_cpu_limit('16')\n",
    "    \n",
    "    # ----- preprocessing validation data --------\n",
    "    preprocess_validation_op = preprocess(\n",
    "        pipeline_dataset=TRAINING_DATASET_TABLE_NAME, \n",
    "        save_data_path='gs://{}/{}_validation.csv.gz'.format(FILE_BUCKET, SERVICE_TYPE),\n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID, \n",
    "        token = token_str\n",
    "    )\n",
    "\n",
    "    preprocess_validation_op.set_memory_limit('128G')\n",
    "    preprocess_validation_op.set_cpu_limit('16')\n",
    "\n",
    "    train_and_save_model_op = train_and_save_model(file_bucket=FILE_BUCKET,\n",
    "                                                   service_type=SERVICE_TYPE,\n",
    "                                                   score_date_dash=SCORE_DATE_DASH,\n",
    "                                                   score_date_val_dash=SCORE_DATE_VAL_DASH,\n",
    "                                                   project_id=PROJECT_ID,\n",
    "                                                   dataset_id=DATASET_ID,\n",
    "                                                   )\n",
    "\n",
    "    train_and_save_model_op.set_memory_limit('128G')\n",
    "    train_and_save_model_op.set_cpu_limit('16')\n",
    "\n",
    "    # col_input_op = col_list = bq_create_dataset_op.outputs[\"col_list\"]\n",
    "    upload_model_op = upload_model(\n",
    "                        project_id = PROJECT_ID,\n",
    "                        region = REGION,\n",
    "                        model = train_and_save_model_op.outputs[\"model\"],\n",
    "                        model_name = MODEL_NAME,\n",
    "                        prediction_image = PREDICTION_IMAGE,\n",
    "                        col_list = train_and_save_model_op.outputs[\"col_list\"], \n",
    "                        model_uri = train_and_save_model_op.outputs[\"model_uri\"]\n",
    "                        )\n",
    "    \n",
    "    upload_model_op.set_memory_limit('32G')\n",
    "    upload_model_op.set_cpu_limit('4')\n",
    "        \n",
    "#     generate_training_data_stats_op = generate_data_stats(      \n",
    "#                                         project_id=PROJECT_ID, \n",
    "#                                         bucket_nm=FILE_BUCKET,\n",
    "#                                         data_type = 'csv',\n",
    "#                                         op_type = 'training',\n",
    "#                                         model_nm = MODEL_NAME,\n",
    "#                                         update_ts = update_ts_str,\n",
    "#                                         model_type  = 'supervised',\n",
    "#                                         dest_stats_gcs_path = TRAINING_STATISTICS_OUTPUT_PATH,\n",
    "#                                         src_csv_path = INPUT_TRAINING_DATA_CSV_PATH,\n",
    "#                                         in_bq_ind = True,\n",
    "#                                         dest_schema_path = SCHEMA_PATH,\n",
    "#                                         dest_stats_bq_dataset = MODEL_MONITORING_DATASET,\n",
    "#                                         pass_through_features = ['ban'],\n",
    "#                                         training_target_col = 'target',\n",
    "#                                         token = token_str\n",
    "#                                     ).set_display_name(\"generate-training-data-statistics\")    \n",
    "\n",
    "#     generate_training_data_stats_op.set_memory_limit('32G')\n",
    "#     generate_training_data_stats_op.set_cpu_limit('4')\n",
    "    \n",
    "#     # visualize serving statistics\n",
    "#     visualize_training_stats_op = visualize_stats(statistics = generate_training_data_stats_op.outputs[\"statistics\"], \n",
    "#                                                     stats_nm=f\"Training Data Statistics {update_ts_str}\",\n",
    "#                                                     op_type = 'training'\n",
    "#                                                 ).set_display_name(\"visualize-Training-data-statistics\")\n",
    "    \n",
    "#     # validate stats + find anomalies by comparing with training stats\n",
    "#     validate_serving_stats_op = validate_stats(\n",
    "#             project_id = PROJECT_ID,\n",
    "#             bucket_nm = FILE_BUCKET, # anomaly_thresholds.json is stored in resources bucket via github\n",
    "#             model_nm = MODEL_NAME,\n",
    "#             update_ts = update_ts_str,\n",
    "#             op_type = 'serving',\n",
    "#             validation_type = 'skew',\n",
    "#             dest_anomalies_bq_dataset = MODEL_MONITORING_DATASET, #Change this to dataset of model monitoring\n",
    "#             statistics = generate_serving_data_stats_op.outputs[\"statistics\"], \n",
    "#             base_stats_path = training_stats_path,\n",
    "#             src_schema_path = SCHEMA_PATH, # ok\n",
    "#             dest_anomalies_gcs_path = ANOMALIES_PATH, # ok \n",
    "#             src_anomaly_thresholds_path = ANOMALY_THRESHOLDS_PATH,\n",
    "#             in_bq_ind = True\n",
    "#         )\n",
    "    \n",
    "    # preprocess_train_op.after(bq_create_training_dataset_op)\n",
    "    # bq_create_validation_dataset_op.after(preprocess_train_op)\n",
    "    preprocess_validation_op.after(bq_create_validation_dataset_op)\n",
    "    train_and_save_model_op.after(preprocess_validation_op)\n",
    "    upload_model_op.after(train_and_save_model_op)\n",
    "    # generate_training_data_stats_op.after(upload_model_op)\n",
    "    # visualize_training_stats_op.after(generate_training_data_stats_op)\n",
    "    # validate_serving_stats_op.after(visualize_serving_stats_op)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2a51c8-4c37-4392-a21a-c4c3279d36d4",
   "metadata": {},
   "source": [
    "### Run the Pipeline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27465a63-5c3b-4e96-9c08-f9cc5dafde90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from kfp.v2 import compiler\n",
    "# from google.cloud.aiplatform import pipeline_jobs\n",
    "# import json\n",
    "\n",
    "# compiler.Compiler().compile(\n",
    "#    pipeline_func=pipeline, package_path=\"pipeline.json\"\n",
    "# )\n",
    "\n",
    "# # job = pipeline_jobs.PipelineJob(\n",
    "# #                                    display_name=TRAIN_PIPELINE_NAME,\n",
    "# #                                    template_path=\"pipeline.json\",\n",
    "# #                                    location=REGION,\n",
    "# #                                    enable_caching=False,\n",
    "# #                                    pipeline_root = PIPELINE_ROOT\n",
    "# #                                 )\n",
    "\n",
    "# token = os.popen('gcloud auth print-access-token').read()\n",
    "# token = re.sub(f'\\n$', '', token)\n",
    "# credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "# job = pipeline_jobs.PipelineJob(\n",
    "#     display_name=TRAIN_PIPELINE_NAME,\n",
    "#     template_path=\"pipeline.json\",\n",
    "#     pipeline_root=PIPELINE_ROOT,\n",
    "#     credentials=credentials,\n",
    "#     project='divg-josh-pr-d1cc3a',\n",
    "#     location=REGION,\n",
    "#     enable_caching=True, \n",
    "# )\n",
    "\n",
    "# job.run()\n",
    "\n",
    "# # job.run(service_account = \"notebook-service-account@divg-josh-pr-d1cc3a.iam.gserviceaccount.com\")\n",
    "# # job.run(service_account = f\"bilayer-sa@{PROJECT_ID}.iam.gserviceaccount.com\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4318de6a-09fd-47b0-8c6f-41847506133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.oauth2.credentials\n",
    "import json\n",
    "\n",
    "token = !gcloud auth print-access-token\n",
    "CREDENTIALS = google.oauth2.credentials.Credentials(token[0])\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "   pipeline_func=pipeline, package_path=\"pipeline.json\"\n",
    ")\n",
    "\n",
    "job = pipeline_jobs.PipelineJob(\n",
    "   display_name=TRAIN_PIPELINE_NAME,\n",
    "   template_path=\"pipeline.json\",\n",
    "   credentials = CREDENTIALS,\n",
    "   pipeline_root = PIPELINE_ROOT,\n",
    "   location=REGION,\n",
    "   enable_caching=False # I encourage you to enable caching when testing as it will reduce resource use\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b796ed29-6351-4521-aecd-238e9b7505fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974dbd82-895e-44eb-9f8e-92c613bef61d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae48d48-ae12-4348-9348-3a2bf4b25de5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb2d51e-84bc-4ad1-8d62-6e873b1974e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd26aa11-da13-4a14-a355-431e4ef3bd97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81375263-48c2-4ef1-bf8d-93a5191b9f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
