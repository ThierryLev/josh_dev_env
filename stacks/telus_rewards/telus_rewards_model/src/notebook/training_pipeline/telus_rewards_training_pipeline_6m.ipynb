{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c150da3-6e1e-4f02-a778-bb3b12af9054",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b559b7e6-fe51-49cc-8e9c-832380843832",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output, OutputPath, ClassificationMetrics,\n",
    "                        Metrics, component)\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import google\n",
    "from google.oauth2 import credentials\n",
    "from google.oauth2 import service_account\n",
    "from google.oauth2.service_account import Credentials\n",
    "from google.cloud import storage\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google_cloud_pipeline_components.v1.batch_predict_job import \\\n",
    "    ModelBatchPredictOp as batch_prediction_op\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558ff533-70c8-4421-813e-c567d6bc496b",
   "metadata": {},
   "source": [
    "### YAML Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f70d721-76e7-4c2e-8153-88b5bbe8ee47",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#tag cell with parameters\n",
    "PROJECT_ID =  ''\n",
    "BUCKET_NAME=''\n",
    "DATASET_ID = ''\n",
    "RESOURCE_BUCKET = ''\n",
    "FILE_BUCKET = ''\n",
    "REGION = ''\n",
    "MODEL_ID = '5070'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb9c0d1-697a-4e34-92d5-125c106d6c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag cell with parameters\n",
    "PROJECT_ID =  'divg-josh-pr-d1cc3a'\n",
    "BUCKET_NAME='divg-josh-pr-d1cc3a-default'\n",
    "DATASET_ID = 'telus_rewards'\n",
    "RESOURCE_BUCKET = 'divg-josh-pr-d1cc3a-default'\n",
    "FILE_BUCKET = 'divg-josh-pr-d1cc3a-default'\n",
    "MODEL_ID = '5070'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eeed21-e12b-45ee-b1e4-d0c8f3843533",
   "metadata": {},
   "source": [
    "### Service Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65d1668-e1d8-4f58-958f-edc44d06a6b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SERVICE_TYPE = 'telus_rewards'\n",
    "SERVICE_TYPE_NAME = 'telus-rewards'\n",
    "TABLE_ID = 'telus_rwrd_redemption_targets'\n",
    "REGION = 'northamerica-northeast1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2229e5d1-1362-40ed-aa7d-cb22e41e4960",
   "metadata": {},
   "source": [
    "### Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d628b-e759-4c9c-aab5-568c54721aaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "STACK_NAME = 'telus_rewards'\n",
    "TRAIN_PIPELINE_NAME_PATH = 'telus_rewards/training_pipeline'\n",
    "PREDICT_PIPELINE_NAME_PATH = 'telus_rewards/predicting_pipeline'\n",
    "TRAIN_PIPELINE_NAME = 'telus-rewards-train-pipeline' # Same name as pulumi.yaml\n",
    "PREDICT_PIPELINE_NAME = 'telus-rewards-predict-pipeline' # Same name as pulumi.yaml\n",
    "TRAIN_PIPELINE_DESCRIPTION = 'telus-rewards-train-pipeline'\n",
    "PREDICT_PIPELINE_DESCRIPTION = 'telus-rewards-predict-pipeline'\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET_NAME}\"\n",
    "REGION = \"northamerica-northeast1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b1da86-930b-4529-b7ab-7c14716ea3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATASET_TABLE_NAME = 'bq_telus_rewards_pipeline_dataset'\n",
    "TRAINING_DATASET_SP_NAME = 'bq_sp_telus_rewards_pipeline_dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc0bd98-1d58-491e-bc5e-bbac378c7bbf",
   "metadata": {},
   "source": [
    "### Import Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f442904b-a40a-4ba8-b237-e9894ffd4f26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download required component files to local\n",
    "prefix = f'{STACK_NAME}/{TRAIN_PIPELINE_NAME_PATH}/components/'\n",
    "dl_dir = 'components/'\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(RESOURCE_BUCKET)\n",
    "blobs = bucket.list_blobs(prefix=prefix)  # Get list of files\n",
    "for blob in blobs: # download each file that starts with \"prefix\" into \"dl_dir\"\n",
    "    if blob.name.endswith(\"/\"):\n",
    "        continue\n",
    "    file_split = blob.name.split(prefix)\n",
    "    file_path = f\"{dl_dir}{file_split[-1]}\"\n",
    "    directory = \"/\".join(file_path.split(\"/\")[0:-1])\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    blob.download_to_filename(file_path) \n",
    "\n",
    "# import main pipeline components\n",
    "from components.bq_create_dataset import bq_create_dataset\n",
    "from components.preprocess import preprocess\n",
    "from components.train_and_save_model import train_and_save_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c315d454-416f-4eed-b12d-1acd5aed1207",
   "metadata": {},
   "source": [
    "### Date Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfad310-a482-4285-aa31-6d9113008be6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scoringDate = date(2022, 7, 1)  # date.today() - relativedelta(days=2)- relativedelta(months=30)\n",
    "valScoringDate = date(2023, 1, 1)  # scoringDate - relativedelta(days=2)\n",
    "\n",
    "# training dates\n",
    "SCORE_DATE = scoringDate.strftime('%Y%m%d')  # date.today().strftime('%Y%m%d')\n",
    "SCORE_DATE_DASH = scoringDate.strftime('%Y-%m-%d')\n",
    "SCORE_DATE_MINUS_6_MOS_DASH = ((scoringDate - relativedelta(months=6)).replace(day=1)).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_LAST_MONTH_START_DASH = (scoringDate.replace(day=1) - timedelta(days=1)).replace(day=1).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_LAST_MONTH_END_DASH = ((scoringDate.replace(day=1)) - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "PROMO_EXPIRY_START = (scoringDate.replace(day=1) + relativedelta(months=3)).replace(day=1).strftime('%Y-%m-%d')\n",
    "PROMO_EXPIRY_END = (scoringDate.replace(day=1) + relativedelta(months=4)).replace(day=1).strftime('%Y-%m-%d')\n",
    "\n",
    "# validation dates\n",
    "SCORE_DATE_VAL = valScoringDate.strftime('%Y%m%d')\n",
    "SCORE_DATE_VAL_DASH = valScoringDate.strftime('%Y-%m-%d')\n",
    "SCORE_DATE_VAL_MINUS_6_MOS_DASH = ((valScoringDate - relativedelta(months=6)).replace(day=1)).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_VAL_LAST_MONTH_START_DASH = (valScoringDate.replace(day=1) - timedelta(days=1)).replace(day=1).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_VAL_LAST_MONTH_END_DASH = ((valScoringDate.replace(day=1)) - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "PROMO_EXPIRY_START_VAL = (valScoringDate.replace(day=1) + relativedelta(months=3)).replace(day=1).strftime('%Y-%m-%d')\n",
    "PROMO_EXPIRY_END_VAL = (valScoringDate.replace(day=1) + relativedelta(months=4)).replace(day=1).strftime('%Y-%m-%d')\n",
    "\n",
    "SCORE_DATE_DELTA = 0\n",
    "SCORE_DATE_VAL_DELTA = 0\n",
    "TICKET_DATE_WINDOW = 30  # Days of ticket data to be queried\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2eea99-a3f0-48ea-8962-115edcaf26d9",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5e82db-20c8-4480-bb48-b44d2ce005ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports\n",
    "from kfp.v2 import compiler\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "@dsl.pipeline(\n",
    "    name=TRAIN_PIPELINE_NAME, \n",
    "    description=TRAIN_PIPELINE_DESCRIPTION\n",
    "    )\n",
    "def pipeline(\n",
    "        project_id: str = PROJECT_ID,\n",
    "        region: str = REGION,\n",
    "        resource_bucket: str = RESOURCE_BUCKET, \n",
    "        file_bucket: str = FILE_BUCKET\n",
    "    ):\n",
    "    \n",
    "    import google.oauth2.credentials\n",
    "    token = !gcloud auth print-access-token\n",
    "    token_str = token[0]\n",
    "    \n",
    "    # ----- create training set --------\n",
    "    bq_create_training_dataset_op = bq_create_dataset(score_date=SCORE_DATE_DASH,\n",
    "                          score_date_delta=SCORE_DATE_DELTA,\n",
    "                          project_id=PROJECT_ID,\n",
    "                          dataset_id=DATASET_ID,\n",
    "                          region=REGION,\n",
    "                          promo_expiry_start=PROMO_EXPIRY_START, \n",
    "                          promo_expiry_end=PROMO_EXPIRY_END, \n",
    "                          v_start_date=SCORE_DATE_MINUS_6_MOS_DASH,\n",
    "                          v_end_date=SCORE_DATE_LAST_MONTH_END_DASH, \n",
    "                          token = token_str)\n",
    "    \n",
    "    bq_create_training_dataset_op.set_memory_limit('128G')\n",
    "    bq_create_training_dataset_op.set_cpu_limit('16')\n",
    "    \n",
    "    # ----- preprocessing train data --------\n",
    "    preprocess_train_op = preprocess(\n",
    "        pipeline_dataset=TRAINING_DATASET_TABLE_NAME, \n",
    "        save_data_path='gs://{}/{}_train.csv.gz'.format(FILE_BUCKET, SERVICE_TYPE),\n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID, \n",
    "        token = token_str\n",
    "    )\n",
    "\n",
    "    preprocess_train_op.set_memory_limit('128G')\n",
    "    preprocess_train_op.set_cpu_limit('16')\n",
    "    \n",
    "    # ----- create validation set --------\n",
    "    bq_create_validation_dataset_op = bq_create_dataset(score_date=SCORE_DATE_VAL_DASH,\n",
    "                          score_date_delta=SCORE_DATE_VAL_DELTA,\n",
    "                          project_id=PROJECT_ID,\n",
    "                          dataset_id=DATASET_ID,\n",
    "                          region=REGION,\n",
    "                          promo_expiry_start=PROMO_EXPIRY_START_VAL, \n",
    "                          promo_expiry_end=PROMO_EXPIRY_END_VAL, \n",
    "                          v_start_date=SCORE_DATE_VAL_MINUS_6_MOS_DASH,\n",
    "                          v_end_date=SCORE_DATE_VAL_LAST_MONTH_END_DASH, \n",
    "                          token = token_str)\n",
    "    \n",
    "\n",
    "    bq_create_validation_dataset_op.set_memory_limit('128G')\n",
    "    bq_create_validation_dataset_op.set_cpu_limit('16')\n",
    "\n",
    "    \n",
    "    # ----- preprocessing validation data --------\n",
    "    preprocess_validation_op = preprocess(\n",
    "        pipeline_dataset=TRAINING_DATASET_TABLE_NAME, \n",
    "        save_data_path='gs://{}/{}_validation.csv.gz'.format(FILE_BUCKET, SERVICE_TYPE),\n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID, \n",
    "        token = token_str\n",
    "    )\n",
    "\n",
    "    preprocess_validation_op.set_memory_limit('128G')\n",
    "    preprocess_validation_op.set_cpu_limit('16')\n",
    "\n",
    "    train_and_save_model_op = train_and_save_model(file_bucket=FILE_BUCKET,\n",
    "                                                   service_type=SERVICE_TYPE,\n",
    "                                                   score_date_dash=SCORE_DATE_DASH,\n",
    "                                                   score_date_val_dash=SCORE_DATE_VAL_DASH,\n",
    "                                                   project_id=PROJECT_ID,\n",
    "                                                   dataset_id=DATASET_ID,\n",
    "                                                   )\n",
    "\n",
    "    train_and_save_model_op.set_memory_limit('128G')\n",
    "    train_and_save_model_op.set_cpu_limit('16')\n",
    "\n",
    "    preprocess_train_op.after(bq_create_training_dataset_op)\n",
    "    bq_create_validation_dataset_op.after(preprocess_train_op)\n",
    "    preprocess_validation_op.after(bq_create_validation_dataset_op)\n",
    "    train_and_save_model_op.after(preprocess_validation_op)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0db708-27ad-4aa4-81b2-927db0f33ff5",
   "metadata": {},
   "source": [
    "### Reduced Pipeline - MODEL TRAINING ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240c4130-5eab-4d4b-8c03-f1621b60fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # library imports\n",
    "# from kfp.v2 import compiler\n",
    "# from google.cloud.aiplatform import pipeline_jobs\n",
    "# @dsl.pipeline(\n",
    "#     name=TRAIN_PIPELINE_NAME, \n",
    "#     description=TRAIN_PIPELINE_DESCRIPTION\n",
    "#     )\n",
    "# def pipeline(\n",
    "#         project_id: str = PROJECT_ID,\n",
    "#         region: str = REGION,\n",
    "#         resource_bucket: str = RESOURCE_BUCKET, \n",
    "#         file_bucket: str = FILE_BUCKET\n",
    "#     ):\n",
    "    \n",
    "#     import google.oauth2.credentials\n",
    "#     token = !gcloud auth print-access-token\n",
    "#     token_str = token[0]\n",
    "    \n",
    "#     train_and_save_model_op = train_and_save_model(file_bucket=FILE_BUCKET,\n",
    "#                                                    service_type=SERVICE_TYPE,\n",
    "#                                                    score_date_dash=SCORE_DATE_DASH,\n",
    "#                                                    score_date_val_dash=SCORE_DATE_VAL_DASH,\n",
    "#                                                    project_id=PROJECT_ID,\n",
    "#                                                    dataset_id=DATASET_ID,\n",
    "#                                                    )\n",
    "\n",
    "#     train_and_save_model_op.set_memory_limit('128G')\n",
    "#     train_and_save_model_op.set_cpu_limit('16')\n",
    "\n",
    "#     train_and_save_model_op\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2a51c8-4c37-4392-a21a-c4c3279d36d4",
   "metadata": {},
   "source": [
    "### Run the Pipeline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27465a63-5c3b-4e96-9c08-f9cc5dafde90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from kfp.v2 import compiler\n",
    "# from google.cloud.aiplatform import pipeline_jobs\n",
    "# import json\n",
    "\n",
    "# compiler.Compiler().compile(\n",
    "#    pipeline_func=pipeline, package_path=\"pipeline.json\"\n",
    "# )\n",
    "\n",
    "# # job = pipeline_jobs.PipelineJob(\n",
    "# #                                    display_name=TRAIN_PIPELINE_NAME,\n",
    "# #                                    template_path=\"pipeline.json\",\n",
    "# #                                    location=REGION,\n",
    "# #                                    enable_caching=False,\n",
    "# #                                    pipeline_root = PIPELINE_ROOT\n",
    "# #                                 )\n",
    "\n",
    "# token = os.popen('gcloud auth print-access-token').read()\n",
    "# token = re.sub(f'\\n$', '', token)\n",
    "# credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "# job = pipeline_jobs.PipelineJob(\n",
    "#     display_name=TRAIN_PIPELINE_NAME,\n",
    "#     template_path=\"pipeline.json\",\n",
    "#     pipeline_root=PIPELINE_ROOT,\n",
    "#     credentials=credentials,\n",
    "#     project='divg-josh-pr-d1cc3a',\n",
    "#     location=REGION,\n",
    "#     enable_caching=True, \n",
    "# )\n",
    "\n",
    "# job.run()\n",
    "\n",
    "# # job.run(service_account = \"notebook-service-account@divg-josh-pr-d1cc3a.iam.gserviceaccount.com\")\n",
    "# # job.run(service_account = f\"bilayer-sa@{PROJECT_ID}.iam.gserviceaccount.com\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4318de6a-09fd-47b0-8c6f-41847506133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.oauth2.credentials\n",
    "import json\n",
    "\n",
    "token = !gcloud auth print-access-token\n",
    "CREDENTIALS = google.oauth2.credentials.Credentials(token[0])\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "   pipeline_func=pipeline, package_path=\"pipeline.json\"\n",
    ")\n",
    "\n",
    "job = pipeline_jobs.PipelineJob(\n",
    "   display_name=TRAIN_PIPELINE_NAME,\n",
    "   template_path=\"pipeline.json\",\n",
    "   credentials = CREDENTIALS,\n",
    "   pipeline_root = PIPELINE_ROOT,\n",
    "   location=REGION,\n",
    "   enable_caching=False # I encourage you to enable caching when testing as it will reduce resource use\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b796ed29-6351-4521-aecd-238e9b7505fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974dbd82-895e-44eb-9f8e-92c613bef61d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae48d48-ae12-4348-9348-3a2bf4b25de5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb2d51e-84bc-4ad1-8d62-6e873b1974e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd26aa11-da13-4a14-a355-431e4ef3bd97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81375263-48c2-4ef1-bf8d-93a5191b9f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
