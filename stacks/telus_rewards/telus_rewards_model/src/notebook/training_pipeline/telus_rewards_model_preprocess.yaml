name: Preprocess
inputs:
- {name: pipeline_dataset, type: String}
- {name: save_data_path, type: String}
- {name: project_id, type: String}
- {name: dataset_id, type: String}
- {name: token, type: String}
implementation:
  container:
    image: northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/wb-platform/pipelines/kubeflow-pycaret:latest
    command:
    - sh
    - -c
    - |2

      if ! [ -x "$(command -v pip)" ]; then
          python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
      fi

      PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.11' && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp -d)
      printf "%s" "$0" > "$program_path/ephemeral_component.py"
      python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
    - "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing\
      \ import *\n\ndef preprocess(\n        pipeline_dataset: str, \n        save_data_path:\
      \ str,\n        project_id: str,\n        dataset_id: str, \n        token:\
      \ str\n):\n    import pandas as pd\n    import gc\n    import time\n    import\
      \ google\n    from google.cloud import bigquery\n    from datetime import datetime\n\
      \    import logging \n    from google.oauth2 import credentials\n\n    CREDENTIALS\
      \ = google.oauth2.credentials.Credentials(token) # get credentials from token\n\
      \n    client = bigquery.Client(project=project_id, credentials=CREDENTIALS)\n\
      \n    # pipeline_dataset \n    pipeline_dataset_name = f\"{project_id}.{dataset_id}.{pipeline_dataset}\"\
      \ \n    build_df_pipeline_dataset = f'SELECT * FROM `{pipeline_dataset_name}`'\n\
      \    df_pipeline_dataset = client.query(build_df_pipeline_dataset).to_dataframe()\n\
      \    df_pipeline_dataset = df_pipeline_dataset.set_index('ban') \n\n    # demo\
      \ columns\n    df_pipeline_dataset['demo_urban_flag'] = df_pipeline_dataset.demo_sgname.str.lower().str.contains('urban').fillna(0).astype(int)\n\
      \    df_pipeline_dataset['demo_rural_flag'] = df_pipeline_dataset.demo_sgname.str.lower().str.contains('rural').fillna(0).astype(int)\n\
      \    df_pipeline_dataset['demo_family_flag'] = df_pipeline_dataset.demo_lsname.str.lower().str.contains('families').fillna(0).astype(int)\n\
      \n    df_income_dummies = pd.get_dummies(df_pipeline_dataset[['demo_lsname']])\
      \ \n    df_income_dummies.columns = df_income_dummies.columns.str.replace('&',\
      \ 'and')\n    df_income_dummies.columns = df_income_dummies.columns.str.replace('\
      \ ', '_')\n\n    df_pipeline_dataset.drop(columns=['demo_sgname', 'demo_lsname'],\
      \ axis=1, inplace=True)\n\n    df_pipeline_dataset = df_pipeline_dataset.join(df_income_dummies)\n\
      \n    df_join = df_pipeline_dataset.copy()\n\n    #column name clean-up\n  \
      \  df_join.columns = df_join.columns.str.replace(' ', '_')\n    df_join.columns\
      \ = df_join.columns.str.replace('-', '_')\n\n    #df_final\n    df_final = df_join.copy()\n\
      \    del df_join\n    gc.collect()\n    print('......df_final done')\n\n   \
      \ for f in df_final.columns:\n        df_final[f] = list(df_final[f])\n\n  \
      \  df_final.to_csv(save_data_path, index=True, compression='gzip') \n    del\
      \ df_final\n    gc.collect()\n    print(f'......csv saved in {save_data_path}')\n\
      \    time.sleep(120)\n\n"
    args:
    - --executor_input
    - {executorInput: null}
    - --function_to_execute
    - preprocess
