{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c150da3-6e1e-4f02-a778-bb3b12af9054",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b559b7e6-fe51-49cc-8e9c-832380843832",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output, OutputPath, ClassificationMetrics,\n",
    "                        Metrics, component)\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import google\n",
    "from google.oauth2 import credentials\n",
    "from google.oauth2 import service_account\n",
    "from google.oauth2.service_account import Credentials\n",
    "from google.cloud import storage\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google_cloud_pipeline_components.v1.batch_predict_job import \\\n",
    "    ModelBatchPredictOp as batch_prediction_op\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558ff533-70c8-4421-813e-c567d6bc496b",
   "metadata": {},
   "source": [
    "### YAML Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f70d721-76e7-4c2e-8153-88b5bbe8ee47",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#tag cell with parameters\n",
    "PROJECT_ID =  ''\n",
    "BUCKET_NAME=''\n",
    "DATASET_ID = ''\n",
    "RESOURCE_BUCKET = ''\n",
    "FILE_BUCKET = ''\n",
    "REGION = ''\n",
    "MODEL_ID = '5070'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb9c0d1-697a-4e34-92d5-125c106d6c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag cell with parameters\n",
    "PROJECT_ID =  'divg-josh-pr-d1cc3a'\n",
    "BUCKET_NAME='divg-josh-pr-d1cc3a-default'\n",
    "DATASET_ID = 'telus_rewards'\n",
    "RESOURCE_BUCKET = 'divg-josh-pr-d1cc3a-default'\n",
    "FILE_BUCKET = 'divg-josh-pr-d1cc3a-default'\n",
    "MODEL_ID = '5070'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eeed21-e12b-45ee-b1e4-d0c8f3843533",
   "metadata": {},
   "source": [
    "### Service Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65d1668-e1d8-4f58-958f-edc44d06a6b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SERVICE_TYPE = 'telus_rewards'\n",
    "SERVICE_TYPE_NAME = 'telus-rewards'\n",
    "TABLE_ID = 'telus_rwrd_redemption_targets'\n",
    "REGION = 'northamerica-northeast1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2229e5d1-1362-40ed-aa7d-cb22e41e4960",
   "metadata": {},
   "source": [
    "### Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d628b-e759-4c9c-aab5-568c54721aaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "STACK_NAME = 'telus_rewards'\n",
    "TRAIN_PIPELINE_NAME_PATH = 'telus_rewards/training_pipeline'\n",
    "PREDICT_PIPELINE_NAME_PATH = 'telus_rewards/predicting_pipeline'\n",
    "TRAIN_PIPELINE_NAME = 'telus-rewards-train-pipeline' # Same name as pulumi.yaml\n",
    "PREDICT_PIPELINE_NAME = 'telus-rewards-predict-pipeline' # Same name as pulumi.yaml\n",
    "TRAIN_PIPELINE_DESCRIPTION = 'telus-rewards-train-pipeline'\n",
    "PREDICT_PIPELINE_DESCRIPTION = 'telus-rewards-predict-pipeline'\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET_NAME}\"\n",
    "REGION = \"northamerica-northeast1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b1da86-930b-4529-b7ab-7c14716ea3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATASET_TABLE_NAME = 'bq_telus_rewards_pipeline_dataset'\n",
    "TRAINING_DATASET_SP_NAME = 'bq_sp_telus_rewards_pipeline_dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc0bd98-1d58-491e-bc5e-bbac378c7bbf",
   "metadata": {},
   "source": [
    "### Import Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f442904b-a40a-4ba8-b237-e9894ffd4f26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download required component files to local\n",
    "prefix = f'{STACK_NAME}/{TRAIN_PIPELINE_NAME_PATH}/components/'\n",
    "dl_dir = 'components/'\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(RESOURCE_BUCKET)\n",
    "blobs = bucket.list_blobs(prefix=prefix)  # Get list of files\n",
    "for blob in blobs: # download each file that starts with \"prefix\" into \"dl_dir\"\n",
    "    if blob.name.endswith(\"/\"):\n",
    "        continue\n",
    "    file_split = blob.name.split(prefix)\n",
    "    file_path = f\"{dl_dir}{file_split[-1]}\"\n",
    "    directory = \"/\".join(file_path.split(\"/\")[0:-1])\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    blob.download_to_filename(file_path) \n",
    "\n",
    "# import main pipeline components\n",
    "from components.bq_create_dataset import bq_create_dataset\n",
    "from components.preprocess import preprocess\n",
    "from components.train_and_save_model_reg import train_and_save_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c315d454-416f-4eed-b12d-1acd5aed1207",
   "metadata": {},
   "source": [
    "### Date Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfad310-a482-4285-aa31-6d9113008be6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scoringDate = date(2022, 7, 1)  # date.today() - relativedelta(days=2)- relativedelta(months=30)\n",
    "valScoringDate = date(2023, 7, 1)  # scoringDate - relativedelta(days=2)\n",
    "\n",
    "# training dates\n",
    "SCORE_DATE = scoringDate.strftime('%Y%m%d')  # date.today().strftime('%Y%m%d')\n",
    "SCORE_DATE_DASH = scoringDate.strftime('%Y-%m-%d')\n",
    "SCORE_DATE_MINUS_6_MOS_DASH = ((scoringDate - relativedelta(months=6)).replace(day=1)).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_LAST_MONTH_START_DASH = (scoringDate.replace(day=1) - timedelta(days=1)).replace(day=1).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_LAST_MONTH_END_DASH = ((scoringDate.replace(day=1)) - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "PROMO_EXPIRY_START = (scoringDate.replace(day=1) + relativedelta(months=3)).replace(day=1).strftime('%Y-%m-%d')\n",
    "PROMO_EXPIRY_END = (scoringDate.replace(day=1) + relativedelta(months=4)).replace(day=1).strftime('%Y-%m-%d')\n",
    "\n",
    "# validation dates\n",
    "SCORE_DATE_VAL = valScoringDate.strftime('%Y%m%d')\n",
    "SCORE_DATE_VAL_DASH = valScoringDate.strftime('%Y-%m-%d')\n",
    "SCORE_DATE_VAL_MINUS_6_MOS_DASH = ((valScoringDate - relativedelta(months=6)).replace(day=1)).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_VAL_LAST_MONTH_START_DASH = (valScoringDate.replace(day=1) - timedelta(days=1)).replace(day=1).strftime('%Y-%m-%d')\n",
    "SCORE_DATE_VAL_LAST_MONTH_END_DASH = ((valScoringDate.replace(day=1)) - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "PROMO_EXPIRY_START_VAL = (valScoringDate.replace(day=1) + relativedelta(months=3)).replace(day=1).strftime('%Y-%m-%d')\n",
    "PROMO_EXPIRY_END_VAL = (valScoringDate.replace(day=1) + relativedelta(months=4)).replace(day=1).strftime('%Y-%m-%d')\n",
    "\n",
    "SCORE_DATE_DELTA = 0\n",
    "SCORE_DATE_VAL_DELTA = 0\n",
    "TICKET_DATE_WINDOW = 30  # Days of ticket data to be queried\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d665bed7-3c33-4eb8-81e6-4af7daea3688",
   "metadata": {},
   "source": [
    "### bq_created_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d57752-e477-4988-bc3e-4365e4ba8b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bq_create_dataset(score_date: str,\n",
    "                      score_date_delta: int,\n",
    "                      project_id: str,\n",
    "                      dataset_id: str,\n",
    "                      region: str,\n",
    "                      promo_expiry_start: str, \n",
    "                      promo_expiry_end: str, \n",
    "                      v_start_date: str,\n",
    "                      v_end_date: str, \n",
    "                      token: str) -> NamedTuple(\"output\", [(\"col_list\", list)]):\n",
    " \n",
    "    import google\n",
    "    from google.cloud import bigquery\n",
    "    from datetime import datetime\n",
    "    import logging \n",
    "    import os \n",
    "    import re \n",
    "    from google.oauth2 import credentials\n",
    "\n",
    "    CREDENTIALS = google.oauth2.credentials.Credentials(token) # get credentials from token\n",
    "    \n",
    "    client = bigquery.Client(project=project_id, credentials=CREDENTIALS)\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "\n",
    "    # Change dataset / table + sp table name to version in bi-layer\n",
    "    query =\\\n",
    "        f'''\n",
    "            DECLARE score_date DATE DEFAULT \"{score_date}\";\n",
    "            DECLARE promo_expiry_start DATE DEFAULT \"{promo_expiry_start}\";\n",
    "            DECLARE promo_expiry_end DATE DEFAULT \"{promo_expiry_end}\";\n",
    "            DECLARE start_date DATE DEFAULT \"{v_start_date}\";\n",
    "            DECLARE end_date DATE DEFAULT \"{v_end_date}\";\n",
    "        \n",
    "            -- Change dataset / sp name to the version in the bi_layer\n",
    "            CALL {dataset_id}.bq_sp_telus_rewards_pipeline_dataset(score_date, promo_expiry_start, promo_expiry_end, start_date, end_date);\n",
    "\n",
    "            SELECT\n",
    "                *\n",
    "            FROM {dataset_id}.INFORMATION_SCHEMA.PARTITIONS\n",
    "            WHERE table_name='bq_telus_rewards_pipeline_dataset'\n",
    "            \n",
    "        '''\n",
    "    \n",
    "    df = client.query(query, job_config=job_config).to_dataframe()\n",
    "    logging.info(df.to_string())\n",
    "    \n",
    "    logging.info(f\"Loaded {df.total_rows[0]} rows into \\\n",
    "             {df.table_catalog[0]}.{df.table_schema[0]}.{df.table_name[0]} on \\\n",
    "             {datetime.strftime((df.last_modified_time[0]), '%Y-%m-%d %H:%M:%S') } !\")\n",
    "    \n",
    "    ######################################## Save column list_##########################\n",
    "    query =\\\n",
    "        f'''\n",
    "           SELECT\n",
    "                *\n",
    "            FROM {dataset_id}.bq_telus_rewards_pipeline_dataset\n",
    "\n",
    "        '''\n",
    "    \n",
    "    df = client.query(query, job_config=job_config).to_dataframe()\n",
    "    \n",
    "    col_list = list([col for col in df.columns])\n",
    "    return (col_list,)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e515565d-6e9e-49db-99c4-0f0083d82f58",
   "metadata": {},
   "source": [
    "### preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc350e06-04e0-4433-b1ef-faa545a7ef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "        pipeline_dataset: str, \n",
    "        save_data_path: str,\n",
    "        project_id: str,\n",
    "        dataset_id: str, \n",
    "        token: str\n",
    "):\n",
    "    import pandas as pd\n",
    "    import gc\n",
    "    import time\n",
    "    import google\n",
    "    from google.cloud import bigquery\n",
    "    from datetime import datetime\n",
    "    import logging \n",
    "    from google.oauth2 import credentials\n",
    "\n",
    "    CREDENTIALS = google.oauth2.credentials.Credentials(token) # get credentials from token\n",
    "    \n",
    "    client = bigquery.Client(project=project_id, credentials=CREDENTIALS)\n",
    "\n",
    "    # pipeline_dataset \n",
    "    pipeline_dataset_name = f\"{project_id}.{dataset_id}.{pipeline_dataset}\" \n",
    "    build_df_pipeline_dataset = f'SELECT * FROM `{pipeline_dataset_name}`'\n",
    "    df_pipeline_dataset = client.query(build_df_pipeline_dataset).to_dataframe()\n",
    "    df_pipeline_dataset = df_pipeline_dataset.set_index('ban') \n",
    "\n",
    "    # demo columns\n",
    "    df_pipeline_dataset['demo_urban_flag'] = df_pipeline_dataset.demo_sgname.str.lower().str.contains('urban').fillna(0).astype(int)\n",
    "    df_pipeline_dataset['demo_rural_flag'] = df_pipeline_dataset.demo_sgname.str.lower().str.contains('rural').fillna(0).astype(int)\n",
    "    df_pipeline_dataset['demo_family_flag'] = df_pipeline_dataset.demo_lsname.str.lower().str.contains('families').fillna(0).astype(int)\n",
    "\n",
    "    df_income_dummies = pd.get_dummies(df_pipeline_dataset[['demo_lsname']]) \n",
    "    df_income_dummies.columns = df_income_dummies.columns.str.replace('&', 'and')\n",
    "    df_income_dummies.columns = df_income_dummies.columns.str.replace(' ', '_')\n",
    "\n",
    "    df_pipeline_dataset.drop(columns=['demo_sgname', 'demo_lsname'], axis=1, inplace=True)\n",
    "\n",
    "    df_pipeline_dataset = df_pipeline_dataset.join(df_income_dummies)\n",
    "\n",
    "    df_join = df_pipeline_dataset.copy()\n",
    "\n",
    "    #column name clean-up\n",
    "    df_join.columns = df_join.columns.str.replace(' ', '_')\n",
    "    df_join.columns = df_join.columns.str.replace('-', '_')\n",
    "\n",
    "    #df_final\n",
    "    df_final = df_join.copy()\n",
    "    del df_join\n",
    "    gc.collect()\n",
    "    print('......df_final done')\n",
    "\n",
    "    for f in df_final.columns:\n",
    "        df_final[f] = list(df_final[f])\n",
    "\n",
    "    df_final.to_csv(save_data_path, index=True, compression='gzip') \n",
    "    del df_final\n",
    "    gc.collect()\n",
    "    print(f'......csv saved in {save_data_path}')\n",
    "    time.sleep(120)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f769fc-1559-4b94-a00f-b05a05c99834",
   "metadata": {},
   "source": [
    "### train and save model - part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8b0d13-dd43-4247-b0d2-a6f88b7334c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_bucket=FILE_BUCKET\n",
    "service_type=SERVICE_TYPE\n",
    "score_date_dash=SCORE_DATE_DASH\n",
    "score_date_val_dash=SCORE_DATE_VAL_DASH\n",
    "project_id=PROJECT_ID\n",
    "dataset_id=DATASET_ID\n",
    "\n",
    "import gc\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_lift(prob, y_test, q):\n",
    "    result = pd.DataFrame(columns=['Prob', 'Redemption'])\n",
    "    result['Prob'] = prob\n",
    "    result['Redemption'] = y_test\n",
    "    result['Decile'] = pd.qcut(result['Prob'], q, labels=[i for i in range(q, 0, -1)])\n",
    "    add = pd.DataFrame(result.groupby('Decile')['Redemption'].mean()).reset_index()\n",
    "    add.columns = ['Decile', 'avg_real_redemption_rate']\n",
    "    result = result.merge(add, on='Decile', how='left')\n",
    "    result.sort_values('Decile', ascending=True, inplace=True)\n",
    "    lg = pd.DataFrame(result.groupby('Decile')['Prob'].mean()).reset_index()\n",
    "    lg.columns = ['Decile', 'avg_model_pred_redemption_rate']\n",
    "    lg.sort_values('Decile', ascending=False, inplace=True)\n",
    "    lg['avg_redemption_rate_total'] = result['Redemption'].mean()\n",
    "    lg = lg.merge(add, on='Decile', how='left')\n",
    "    lg['lift'] = lg['avg_real_redemption_rate'] / lg['avg_redemption_rate_total']\n",
    "\n",
    "    return lg    \n",
    "\n",
    "df_train = pd.read_csv('gs://{}/{}_train.csv.gz'.format(file_bucket, service_type),\n",
    "                       compression='gzip')  \n",
    "df_test = pd.read_csv('gs://{}/{}_validation.csv.gz'.format(file_bucket, service_type),  \n",
    "                      compression='gzip')\n",
    "\n",
    "#set up df_train\n",
    "client = bigquery.Client(project=project_id)\n",
    "sql_train = ''' SELECT * FROM `{}.{}.bq_telus_rwrd_redemption_targets_reg` '''.format(project_id, dataset_id) \n",
    "df_target_train = client.query(sql_train).to_dataframe()\n",
    "# df_target_train = df_target_train.loc[\n",
    "#     df_target_train['YEAR_MONTH'] == '-'.join(score_date_dash.split('-')[:2])]  # score_date_dash = '2022-08-31'\n",
    "df_target_train = df_target_train.loc[df_target_train['YEAR_MONTH'] == '2022-H1']  # score_date_dash = '2022-08-31'\n",
    "df_target_train['ban'] = df_target_train['ban'].astype('int64')\n",
    "df_target_train = df_target_train.groupby('ban').tail(1)\n",
    "df_train = df_train.merge(df_target_train[['ban', 'target_ind']], on='ban', how='left')\n",
    "df_train.rename(columns={'target_ind': 'target'}, inplace=True)\n",
    "# df_train.dropna(subset=['target'], inplace=True)\n",
    "df_train.fillna(0, inplace=True)\n",
    "df_train['target'] = df_train['target'].astype(int)\n",
    "print(df_train.shape)\n",
    "\n",
    "#set up df_test\n",
    "sql_test = ''' SELECT * FROM `{}.{}.bq_telus_rwrd_redemption_targets_reg` '''.format(project_id, dataset_id) \n",
    "df_target_test = client.query(sql_test).to_dataframe()\n",
    "# df_target_test = df_target_test.loc[\n",
    "#     df_target_test['YEAR_MONTH'] == '-'.join(score_date_val_dash.split('-')[:2])]  # score_date_dash = '2022-09-30'\n",
    "df_target_test = df_target_test.loc[df_target_test['YEAR_MONTH'] == '2023-H1']  # score_date_dash = '2022-08-31'\n",
    "df_target_test['ban'] = df_target_test['ban'].astype('int64')\n",
    "df_target_test = df_target_test.groupby('ban').tail(1)\n",
    "df_test = df_test.merge(df_target_test[['ban', 'target_ind']], on='ban', how='left')\n",
    "df_test.rename(columns={'target_ind': 'target'}, inplace=True)\n",
    "# df_test.dropna(subset=['target'], inplace=True)\n",
    "df_test.fillna(0, inplace=True) \n",
    "df_test['target'] = df_test['target'].astype(int)\n",
    "print(df_test.shape)\n",
    "\n",
    "#set up features (list)\n",
    "cols_1 = df_train.columns.values\n",
    "cols_2 = df_test.columns.values\n",
    "cols = set(cols_1).intersection(set(cols_2))\n",
    "features = [f for f in cols if f not in ['ban', 'target']]\n",
    "\n",
    "#train test split\n",
    "df_train, df_val = train_test_split(df_train, shuffle=True, test_size=0.3, random_state=42,\n",
    "                                    stratify=df_train['target']\n",
    "                                    )\n",
    "\n",
    "ban_train = df_train['ban']\n",
    "X_train = df_train[features]\n",
    "y_train = np.squeeze(df_train['target'].values)\n",
    "\n",
    "ban_val = df_val['ban']\n",
    "X_val = df_val[features]\n",
    "y_val = np.squeeze(df_val['target'].values)\n",
    "\n",
    "ban_test = df_test['ban']\n",
    "X_test = df_test[features]\n",
    "y_test = np.squeeze(df_test['target'].values)\n",
    "\n",
    "del df_train, df_val, df_test\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92be1fd9-678a-44cf-b472-a0b758746974",
   "metadata": {},
   "source": [
    "### hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ec558d-d88e-4918-86c1-b49de666ff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid: gbm_param_grid\n",
    "gbm_param_grid = {\n",
    "    'learning_rate': [0.02, 0.05, 0.1, 0.2, 0.3], \n",
    "    'colsample_bytree': [0.8],\n",
    "    'subsample': [0.8],\n",
    "    'n_estimators': [50, 100, 200, 500, 1000],\n",
    "    'max_depth': [5, 6, 8, 10, 12], \n",
    "    'objective': ['reg:squarederror'] \n",
    "}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor()\n",
    "\n",
    "# Perform grid search: grid_mse\n",
    "grid_mse = GridSearchCV(estimator=gbm, param_grid=gbm_param_grid, scoring=\"neg_mean_squared_error\", cv=5, verbose=1)\n",
    "\n",
    "# Fit grid_mse to the data\n",
    "grid_mse.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", grid_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c527ee-7831-499b-acf2-dd05273f5a57",
   "metadata": {},
   "source": [
    "### train and save model - part 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561d124e-452d-4fb7-9eb7-48bff5a157a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# build model and fit in training data\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    learning_rate=0.02,\n",
    "    n_estimators=1000,\n",
    "    max_depth=8,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='binary:logistic',\n",
    "    nthread=4,\n",
    "    scale_pos_weight=1\n",
    "    # seed=27\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "print('xgb training done')\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "#predictions on X_val\n",
    "y_pred = xgb_model.predict_proba(X_val, ntree_limit=xgb_model.best_iteration)[:, 1]\n",
    "y_pred_label = (y_pred > 0.5).astype(int)\n",
    "auc = roc_auc_score(y_val, y_pred_label)\n",
    "metrics.log_metric(\"AUC\", auc)\n",
    "\n",
    "pred_prb = xgb_model.predict_proba(X_test, ntree_limit=xgb_model.best_iteration)[:, 1]\n",
    "lg = get_lift(pred_prb, y_test, 10)\n",
    "\n",
    "# save the model in GCS\n",
    "from datetime import datetime\n",
    "models_dict = {}\n",
    "create_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "models_dict['create_time'] = create_time\n",
    "models_dict['model'] = xgb_model\n",
    "models_dict['features'] = features\n",
    "lg.to_csv('gs://{}/lift_on_scoring_data_{}.csv'.format(file_bucket, create_time, index=False))\n",
    "\n",
    "with open('model_dict.pkl', 'wb') as handle:\n",
    "    pickle.dump(models_dict, handle)\n",
    "handle.close()\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(file_bucket)\n",
    "\n",
    "MODEL_PATH = '{}_xgb_models/'.format(service_type)\n",
    "blob = bucket.blob(MODEL_PATH)\n",
    "if not blob.exists(storage_client):\n",
    "    blob.upload_from_string('')\n",
    "\n",
    "model_name_onbkt = '{}{}_models_xgb_{}'.format(MODEL_PATH, service_type, models_dict['create_time'])\n",
    "blob = bucket.blob(model_name_onbkt)\n",
    "blob.upload_from_filename('model_dict.pkl')\n",
    "\n",
    "print(f\"....model loaded to GCS done at {str(create_time)}\")\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "#predictions on X_test\n",
    "pred_prb = xgb_model.predict_proba(X_test, ntree_limit=xgb_model.best_iteration)[:, 1]\n",
    "# pred_prb = np.array(normalize([pred_prb]))[0]\n",
    "\n",
    "#join ban_test, X_test, y_test and pred_prb and print to csv\n",
    "#CHECK THE SIZE OF EACH COMPONENT BEFORE JOINING\n",
    "q=10\n",
    "df_ban_test = ban_test.to_frame()\n",
    "df_test_exp = df_ban_test.join(X_test) \n",
    "df_test_exp['y_test'] = y_test\n",
    "df_test_exp['y_pred_proba'] = pred_prb\n",
    "df_test_exp['y_pred'] = (df_test_exp['y_pred_proba'] > 0.5).astype(int)\n",
    "df_test_exp['decile'] = pd.qcut(df_test_exp['y_pred_proba'], q, labels=[i for i in range(q, 0, -1)])\n",
    "\n",
    "lg = get_lift(pred_prb, y_test, q)\n",
    "\n",
    "df_test_exp.to_csv('gs://{}/df_test_exp.csv'.format(file_bucket, index=True))\n",
    "print(\"....df_test_exp done\")\n",
    "\n",
    "lg.to_csv('gs://{}/lift_on_scoring_data.csv'.format(file_bucket, index=False))\n",
    "print(\"....lift_to_csv done\")\n",
    "\n",
    "time.sleep(120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc950a6-f7ac-45c1-a0f0-61cfafadcb03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4844e15d-c54c-4adc-b4fe-e7a068530fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e2eea99-a3f0-48ea-8962-115edcaf26d9",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5e82db-20c8-4480-bb48-b44d2ce005ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports\n",
    "from kfp.v2 import compiler\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "@dsl.pipeline(\n",
    "    name=TRAIN_PIPELINE_NAME, \n",
    "    description=TRAIN_PIPELINE_DESCRIPTION\n",
    "    )\n",
    "def pipeline(\n",
    "        project_id: str = PROJECT_ID,\n",
    "        region: str = REGION,\n",
    "        resource_bucket: str = RESOURCE_BUCKET, \n",
    "        file_bucket: str = FILE_BUCKET\n",
    "    ):\n",
    "    \n",
    "    import google.oauth2.credentials\n",
    "    token = !gcloud auth print-access-token\n",
    "    token_str = token[0]\n",
    "    \n",
    "    # ----- create training set --------\n",
    "    bq_create_training_dataset_op = bq_create_dataset(score_date=SCORE_DATE_DASH,\n",
    "                          score_date_delta=SCORE_DATE_DELTA,\n",
    "                          project_id=PROJECT_ID,\n",
    "                          dataset_id=DATASET_ID,\n",
    "                          region=REGION,\n",
    "                          promo_expiry_start=PROMO_EXPIRY_START, \n",
    "                          promo_expiry_end=PROMO_EXPIRY_END, \n",
    "                          v_start_date=SCORE_DATE_MINUS_6_MOS_DASH,\n",
    "                          v_end_date=SCORE_DATE_LAST_MONTH_END_DASH, \n",
    "                          token = token_str)\n",
    "    \n",
    "    bq_create_training_dataset_op.set_memory_limit('128G')\n",
    "    bq_create_training_dataset_op.set_cpu_limit('16')\n",
    "    \n",
    "    # ----- preprocessing train data --------\n",
    "    preprocess_train_op = preprocess(\n",
    "        pipeline_dataset=TRAINING_DATASET_TABLE_NAME, \n",
    "        save_data_path='gs://{}/{}_train.csv.gz'.format(FILE_BUCKET, SERVICE_TYPE),\n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID, \n",
    "        token = token_str\n",
    "    )\n",
    "\n",
    "    preprocess_train_op.set_memory_limit('128G')\n",
    "    preprocess_train_op.set_cpu_limit('16')\n",
    "    \n",
    "    # ----- create validation set --------\n",
    "    bq_create_validation_dataset_op = bq_create_dataset(score_date=SCORE_DATE_VAL_DASH,\n",
    "                          score_date_delta=SCORE_DATE_VAL_DELTA,\n",
    "                          project_id=PROJECT_ID,\n",
    "                          dataset_id=DATASET_ID,\n",
    "                          region=REGION,\n",
    "                          promo_expiry_start=PROMO_EXPIRY_START_VAL, \n",
    "                          promo_expiry_end=PROMO_EXPIRY_END_VAL, \n",
    "                          v_start_date=SCORE_DATE_VAL_MINUS_6_MOS_DASH,\n",
    "                          v_end_date=SCORE_DATE_VAL_LAST_MONTH_END_DASH, \n",
    "                          token = token_str)\n",
    "    \n",
    "\n",
    "    bq_create_validation_dataset_op.set_memory_limit('128G')\n",
    "    bq_create_validation_dataset_op.set_cpu_limit('16')\n",
    "\n",
    "    \n",
    "    # ----- preprocessing validation data --------\n",
    "    preprocess_validation_op = preprocess(\n",
    "        pipeline_dataset=TRAINING_DATASET_TABLE_NAME, \n",
    "        save_data_path='gs://{}/{}_validation.csv.gz'.format(FILE_BUCKET, SERVICE_TYPE),\n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID, \n",
    "        token = token_str\n",
    "    )\n",
    "\n",
    "    preprocess_validation_op.set_memory_limit('128G')\n",
    "    preprocess_validation_op.set_cpu_limit('16')\n",
    "\n",
    "    train_and_save_model_op = train_and_save_model(file_bucket=FILE_BUCKET,\n",
    "                                                   service_type=SERVICE_TYPE,\n",
    "                                                   score_date_dash=SCORE_DATE_DASH,\n",
    "                                                   score_date_val_dash=SCORE_DATE_VAL_DASH,\n",
    "                                                   project_id=PROJECT_ID,\n",
    "                                                   dataset_id=DATASET_ID,\n",
    "                                                   )\n",
    "\n",
    "    train_and_save_model_op.set_memory_limit('128G')\n",
    "    train_and_save_model_op.set_cpu_limit('16')\n",
    "\n",
    "    preprocess_train_op.after(bq_create_training_dataset_op)\n",
    "    bq_create_validation_dataset_op.after(preprocess_train_op)\n",
    "    preprocess_validation_op.after(bq_create_validation_dataset_op)\n",
    "    train_and_save_model_op.after(preprocess_validation_op)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0db708-27ad-4aa4-81b2-927db0f33ff5",
   "metadata": {},
   "source": [
    "### Reduced Pipeline - MODEL TRAINING ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240c4130-5eab-4d4b-8c03-f1621b60fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # library imports\n",
    "# from kfp.v2 import compiler\n",
    "# from google.cloud.aiplatform import pipeline_jobs\n",
    "# @dsl.pipeline(\n",
    "#     name=TRAIN_PIPELINE_NAME, \n",
    "#     description=TRAIN_PIPELINE_DESCRIPTION\n",
    "#     )\n",
    "# def pipeline(\n",
    "#         project_id: str = PROJECT_ID,\n",
    "#         region: str = REGION,\n",
    "#         resource_bucket: str = RESOURCE_BUCKET, \n",
    "#         file_bucket: str = FILE_BUCKET\n",
    "#     ):\n",
    "    \n",
    "#     import google.oauth2.credentials\n",
    "#     token = !gcloud auth print-access-token\n",
    "#     token_str = token[0]\n",
    "    \n",
    "#     train_and_save_model_op = train_and_save_model(file_bucket=FILE_BUCKET,\n",
    "#                                                    service_type=SERVICE_TYPE,\n",
    "#                                                    score_date_dash=SCORE_DATE_DASH,\n",
    "#                                                    score_date_val_dash=SCORE_DATE_VAL_DASH,\n",
    "#                                                    project_id=PROJECT_ID,\n",
    "#                                                    dataset_id=DATASET_ID,\n",
    "#                                                    )\n",
    "\n",
    "#     train_and_save_model_op.set_memory_limit('128G')\n",
    "#     train_and_save_model_op.set_cpu_limit('16')\n",
    "\n",
    "#     train_and_save_model_op\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2a51c8-4c37-4392-a21a-c4c3279d36d4",
   "metadata": {},
   "source": [
    "### Run the Pipeline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27465a63-5c3b-4e96-9c08-f9cc5dafde90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from kfp.v2 import compiler\n",
    "# from google.cloud.aiplatform import pipeline_jobs\n",
    "# import json\n",
    "\n",
    "# compiler.Compiler().compile(\n",
    "#    pipeline_func=pipeline, package_path=\"pipeline.json\"\n",
    "# )\n",
    "\n",
    "# # job = pipeline_jobs.PipelineJob(\n",
    "# #                                    display_name=TRAIN_PIPELINE_NAME,\n",
    "# #                                    template_path=\"pipeline.json\",\n",
    "# #                                    location=REGION,\n",
    "# #                                    enable_caching=False,\n",
    "# #                                    pipeline_root = PIPELINE_ROOT\n",
    "# #                                 )\n",
    "\n",
    "# token = os.popen('gcloud auth print-access-token').read()\n",
    "# token = re.sub(f'\\n$', '', token)\n",
    "# credentials = google.oauth2.credentials.Credentials(token)\n",
    "\n",
    "# job = pipeline_jobs.PipelineJob(\n",
    "#     display_name=TRAIN_PIPELINE_NAME,\n",
    "#     template_path=\"pipeline.json\",\n",
    "#     pipeline_root=PIPELINE_ROOT,\n",
    "#     credentials=credentials,\n",
    "#     project='divg-josh-pr-d1cc3a',\n",
    "#     location=REGION,\n",
    "#     enable_caching=True, \n",
    "# )\n",
    "\n",
    "# job.run()\n",
    "\n",
    "# # job.run(service_account = \"notebook-service-account@divg-josh-pr-d1cc3a.iam.gserviceaccount.com\")\n",
    "# # job.run(service_account = f\"bilayer-sa@{PROJECT_ID}.iam.gserviceaccount.com\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4318de6a-09fd-47b0-8c6f-41847506133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.oauth2.credentials\n",
    "import json\n",
    "\n",
    "token = !gcloud auth print-access-token\n",
    "CREDENTIALS = google.oauth2.credentials.Credentials(token[0])\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "   pipeline_func=pipeline, package_path=\"pipeline.json\"\n",
    ")\n",
    "\n",
    "job = pipeline_jobs.PipelineJob(\n",
    "   display_name=TRAIN_PIPELINE_NAME,\n",
    "   template_path=\"pipeline.json\",\n",
    "   credentials = CREDENTIALS,\n",
    "   pipeline_root = PIPELINE_ROOT,\n",
    "   location=REGION,\n",
    "   enable_caching=False # I encourage you to enable caching when testing as it will reduce resource use\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b796ed29-6351-4521-aecd-238e9b7505fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
